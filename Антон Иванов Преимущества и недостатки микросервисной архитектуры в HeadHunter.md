**Здесь переводится видео в статью Антона Иванова из HeadHunter "Преимущества и недостатки микросервисной архитектуры в HeadHunter"**

Ссылка на видеодоклад https://www.youtube.com/watch?v=7WT_Rl6m2DU

**Текст сделан из субтитров. Буду очень благодарен в форматировании текста, его вычитки. Для этого достаточно знать русский язык.
Присылайте свои pull request или присылайте текст на почту patsev.anton[собака]gmail.com**



я работаю в конторе с этим видом перед тем как я закончил рассказывать кто есть там в центре я хочу немножко спойлеров про свой доклад чтобы вы понимали о чем я буду рассказывать и чего от него ожидать первый тезис это что микро сервис упростят вам жизнь в одном но серьезно усложняет вам жизнь в чем то другом причем микро сервиса затронут всю компанию не только отдел разработки на отдела тестирования делал эксплуатации и тут важный смотрите на эффект для компании в целом я в санта-кларе попытаюсь рассказать о том с какими проблемами столкнулись решение будет не очень много хотя они тоже будут мне интересно рассказать о том чтобы вы понимали с какими болячками с какими трудностями вам придется сталкиваться для того чтобы переходить на красивую архитектуру может быть вы подумаете над чем стоит ли вам это делать сейчас или это сделать потом или вот не делать в офисе ну вот обо мне я два года в центре работал простым разработчикам и в течение этого времени был в общем ярым сторонником на красивую архитектуру мне хотелось распылить абсолютно все мне казалось что наступит светлое будущее потом я перешел работать в команду который нас называю занимается отказоустойчивостью сайта и тут мои представления о том насколько классный микро сервиса и стали более там зверским я не говорю о том что это плохая архитектур я говорю что у нее есть вполне себе ограничения и вы можете извлечь преимущества и недостатки у меня clicker плохо работает с этим можно что-то сделать ok чтобы вы представляли у нас был монолит вполне себе в 2006 году который был написан на java на таких фреймворк ах как struts дсп торг который мы в общем там и половина из этих технологий мы уже поменяли монолит до сих пор у нас каком ты виде остается тогда ходил вам кажется базу данных тогда мы использовали еще mais quel я говоря тогда мы использовали это а по словам не это да еще в центре не было но тем не менее вот и потом мы стали от него отпиливать куски выдели отдельно сервис которые занимаются идентификации пользователя хайди имя фамилия логина и пароля авторизации в соцсетях и так далее все хранилось там и у него была отдельная база мы выделили отделена сервис поиска который собственно отвечал за то что по поисковому запросу вернуть список иди шик резюме или вакансий мы выбили сервис баннеров которого тоже своя база была и сейчас он разрабатывается вообще отдельный команду просто отдельной команды отдельным разработчикам который мне штата как hunter находится это секс история это было здорово сейчас мы пришли к следующей архитектуре у нас появилась h3 фронтонных сервиса они написаны на питоне с использованием торнадо это сервис api который отвечает за то чтобы собственно представляете установить нашего сайта мобильным приложением или чтобы работодатель интегрировались нашим сайтом это собственно сервис который отрисовывает основной сайт к хору и и сервис который отрисовывает мобильный сайт за него у нас отвечает за мобильный сайт отдельная команда хотя все они примерно одних и тех же технологий их написано потом торнадо плюс наш фреймворк фантик у нас выдалась много микро сервисов гораздо больше чем помещается на этот слайд среди них там сервис откликов приглашений от клип сервис машинного обучения рекомендации и так далее и тому подобное я бы попытался какие-то стрелки нарисовать на самом деле это далеко не все стрелки взаимодействие между сервисами какой-то момент мы поняли что у нас появляются очень много интеграционной логике ну то есть простой пример надо отобразить эту страничку результатов поиска вакансий на сходить сервис поиска поискать одесский а потом сходить в сервис вакансий и наложить на это все данные вакансиях название описание зарплаты и так далее у нас появляется сервис интеграционной логики в которые могут ходить фронтовые сервисы фронтовые сервиса могут ходить и напрямую в бэг-энд если им не нужна никакая интеграционная логика но вот много там крутится такая трехуровневая архитектура получилось я этот слайд потом покажу еще раз в конце мы с вами на него посмотрим что в этом слайде есть хорошего что в нем есть плохого с какие решения были хорошо приняты такие решения были приняты не очень здорово давайте с вами начали посмотрим что то что посмотрим много всего написано есть много докладов о преимуществе мега сервисной архитектуры я хочу рассказать о том что мы чувствуем от накрась разной архитектуры чего нам в не нравится первое это конечно декомпозиция монолита понятно что там простые сервисы по ним обильнее чем один большой монстр монолит в котором не понятно все как взаимодействуют а тут по крайней мере есть понятная api и понятные связи между сервисами микро сервис и прочие тестировать то есть разработчику достаточно там установить какие-то стопы внешних походов и все вот скоб тестирование он такой маленький в отличие от монолит который там еще кучи текстура надо развернуть для того чтобы протестировать и не сфокусируешься просто на на своем коде независимые релиза это кстати очень интересная история особенно для нас актуально было независимость релизов когда у нас наши монолитное приложение релизе 1 раз в неделю от релизов каждый день но команда получала допуск к релизу вот монолит там раз в неделю эти времена уже давно в прошлом и мы видимся каждый день но до сих пор команда отвечает что когда у меня есть собственный сервис за которые отвечаю я могу вы среди дня там собрать релиз протестировать его и там выложить в порода с вашим монолитом не до конца дня придется ждать вот ну вот до сих пор наша команда отмечает такое преимущество что еще круто независимая деградация ну понятно если лежит сервиса и он не ошибся там ведь сайта эффекты только не большую функциональность то пользователь это может даже не заметить независимо и масштабирование у нас есть например сервис поиска которому требуется много много оперативной памяти естественно мы закупаем поднимал специфичное оборудование под базы данных закупаем специфическое оборудование а под другие сервисы можем закупать другое оборудование независимо масштабировать те или иные сервисы возможность пробовать новые технологии ну да просто не возьмешь не перепишешь есть монолитно новую технологию а тут удобно переписал сервис на какую технологию понравилось не понравилось распространяем дальше этот успех или выпиливаем переписываем обратно и наконец есть такая менеджер ска и преимущество его называю что у сервисы есть команда владелец есть кому обратиться кого попросить чего-то исправить более того они сами даже заинтересованы в развитии этого сервиса в поддержке разбор инцидентов и так далее это хорошая история чего не круто чем пришлось сталкиваться ну первое это когда у вас монолит у вас в принципе контекст запроса понять что вы смотрите влоги греппа эти там по какому-то идентификатору получаете лог когда у вас много сервисов вы будете крепость смотреть и не понимать что же происходит что же происходило с пользовательским запросам во всем этом придется разбираться эта проблема решаема вы присваиваете каждому пользовательскому запрос идентификатор какой то вы поэтому этот которого ищите и данте индексирует все логе ищите например мы пробовали внедрять грейлок он там не ластик сечь построен и тут надо понимать что у нас достаточно много логов относительно конечно вот там больше терабайта в день и на тех машинах которые были выделены под грибок это не были какие-то там talkie непонятно это были железные мощную машину у них было сто процентов юн успевал индексировать просто physiology и разработчики шутили что он скорее иногда находят правило не находит никаких никаких данных по запросам мы могли потратить время для того чтобы там его настроить это все не rapids мы решили что мы перепишем сделаем собств в общем за работу которая основана о том что мы prequest айди запрос присваивания бы какой засовываем туда time spent начала выполнения запроса дальше можно увидеть что физиологии упорядоченное в общем-то алгоритм поиска он бинарные простаты каемся в любое место логу смотрим time stamp этой строчке больше или меньше и так далее очень быстро находим нужное окно которое нужно просканировать для того чтобы все логи собрать ну и так по каждому блоку тыр каемся и что интересно у нас страной сервис сервер на котором сливает на который сливается все логе вот и там просто стоит несколько вот таких простых демонов которые собственно и делают и вот эта система она не требует никаких особенных ресурсов все логе запрос показывает но опять же мы в нее вложились написали и над ней там трудились пара человек слайд которая долго думал вставлять не вставлять потому что здесь есть benchmark за benchmark всегда можно покритиковать даже там устроен неправильно вот но тем не менее я встречал разработчиков которые в общем читают что удаленный вызов это очень просто почти ничем не отличается от локального вызова иван по этому поводу очень классно доклад прочитал очень советую его пересмотреть вот я от себя лишь добавлю что ну какой путь проходит запросам там пишется в буфер сокета стерилизованные до данные запросы пишутся в буфер сок это в бустер сетевой карты по цепочке сетевых устройств продаются на может быть выделены и железные балансировщик и может быть нет там раскрывается запрос крутится на конкретный сервер и дальше и так далее и тому подобное это мы только дослали запрос до удаленной стороны удаленная сторона еще сделает полезную работу какую-то а дальше начнет обратно стерилизовать ответа передачи света обратно по сети вот интуитивно кажется что если у вас сервис выполняет очень мало работа ему там сложение двух чисел например наверное не стоит этот удаленный поход делать посети да с два числа гораздо проще сложить локально вот и поэтому вот я сейчас приведу график того насколько быстрее медленные удаленные и локальный вызов метода а метод у нас он просто делает какую-то работу по горизонтальной оси как раз отложено работу которой он делает там 1 2 3 10 миллисекунд график синяя линия этого то сколько мы запросов в секунду умудряемся делать вызываем этот напрямую красная линия сколько запросов умудряемся делать типу доделаю удаленный вызов на самом деле не совсем удаленный вызов я просто этапа прослойку на локальный компьютер поставил вот но оживления собственно показывает наш overhead но видно что этот overhead конечно чем дольше вас запрос всем тем он меньше относительно собственно полезная работа которая делается вот но все равно на там когда у вас 1 2 3 миллисекунду занимают этот эффект будет достигать по камере там 20 процентов вот идут интересно что многие разработчики опять же в колл-центр и очень часто думают наши запросы занимают там по 50 100 миллисекунд и в общем сетевой вверх и тон никакой вот на самом деле я там много рассмотрю вижу перцентиле скорее мы ближе к началу этого графика 1 2 3 мы 1 до 3 миллисекунду нас многие запросы снимают вот поэтому но даже есть там много выделенных серверов которые только занимаются тем что роутер балансируют запросы до настолько настолько много у нас сетевых вызов и тут мне возражает но антон это все лирика железо-то дешевая ну да дешевое в обслуживании то есть над этим же работает там служба закупок и железо нужно чтобы кто-то его закупил чтобы это железо поставили настроили продолжали обслуживать а это все уже люся люди что стоит они так дешево да вот поэтому вот есть такой баланс ну интуитивно кажется опять же что если у вас мало сетевых походов того как бы можно ли график не смотреть еще о чем хочется сказать это что рпц в принципе сложнее вызова метода что вот для того чтобы сделать удаленный вызов у нас есть целое семейство стр акции например есть там поисковый клиенты к сервису поиска он построен на базе нашего хахашного хоть этапы клиента который свою очередь базируется на базе клиента за там открыты от открытого щенках этапа клиент running который в свою очередь между прочим не все знают используют найти под собой и так далее и тому подобное и все это достаточно много не простого кода и хорошо когда разработчик от этого все абстрагировано может туда не смотреть но вот у нас например команды среду до заглядывает периодически и находит там много нового всякие полу соединение по улыбает буферов event лупы хотите ли вы об этом знать или не хотите до вопрос к вам и второе о чем здесь тоже надо сказать это то что рпц удаленный вызов он сложнее в том смысле что вам придётся думать над тем от чего делать когда сервис не ответил и как он не ответил реагировали на пятисотку также как реагировать на рид тайм-аут вопрос иногда открытый у нас всякие баги в прозе случались связанные с тем что разработчики написали код так что считаю что сервер не ответил или ответил пустым ответом это одно и то же нет ни одно и то же там разные бизнес-логика зависимы зависимости от этого должно было быть распределенность я так просто скажу что вот когда вы выделяете сервис и вы тут же падает на все проблемы распределенных систем и отказы тайм-аут и ретро и дубли и так далее будьте к этому готовы я вам приведу просто простой пример того как мы в сарае упариваем ся по отстройке все его цепочки вызовов сервисов очень простой пример смотрите у нас есть балансировщик который научит запрос на сервиса на один сервер или на другой сервер под сервисом а и сервис б перед которым тоже есть балансировщик там из сервиса и так далее ну вот запрос для того чтобы быть обслужим он проходит вот такую цепочку и допустим вы хотите уложиться в той малость 2 секунды ну там пользователю сказать через две секунды извини не смог и дальше не хотите напрягать вашу инфраструктуру этим запросам ну и кажется что вообще-то тайм-аут от 1 балансировщика до сервиса должен быть меньше чем 2 секунды правильно ну потому что мы обнаружим что сервис недоступен на мужчина то время перед травиться для того чтобы другой сер его обслуживал мы так недолго думая выставляем одну секунду тут конечно можно порассуждать на тем что это может быть где-то в районе от одной секунды до 2 секунд до потому что мы долго ждем края сервис не ответил понимаем что он лег а потом быстро рид роимся из и собственно ответ получаем но там предположим одну секунду дальше это одну секунду мы разбиваем на пол секунды перед сервисом b и на там 250 миллисекунд перед сервисом c дальше начинаются интересные вопросы в селе запросы сервиса укладывается в 250 миллисекунд причем это не должна быть там какая-то 999 представители вас реально могут существовать url и которые просто тяжелые если у вас запас вы же не хотите запускать в вашей системе лавину retrieve только потому что у вас чего ты там притупила в сердце или в базе отдельные тайм-аут и для тяжелых запросов вы хотите прописывать отдельно в конфиге или вот что вы с ними будете делать как не завалить себя ретро им во время проблем с базой services у вас база сервиса тупит и у вас там первый сервер мне ответил пошла лавина retrieve и второй сервис тоже не ответил потому что пита базы под ним вот и ваша система все завалено ретро я тут опять же необходимо настраивать цепочку retrieve так чтобы вы понимали что произошел летрай и ретро ретро уже не нужно хоть вы-то вкладываться что делать если сервиса хочет ходить в сервис c напрямую как там тайм-аут и отстроить что делать если мы хотим ни один ретро и а там не а там два три и больше такое тоже бывает необходимо ну вот просто вести русского вопросов вам придется продумывать даже если вы захотите использовать какое-то решение вы должны будете понимать что работать именно так как вам нужно есть некоторые церемония связанные с новым сервисом да я как считал что я сейчас вот выберу фреймворк на котором я буду писать допишу его под себя все и он будет вроде упаковываем демонизируем дакаре зиру им настраиваем процедуру выкладки настраиваем ротацию заливку логов настраиваем мониторинг триггеры подключаем систему мониторинга низкочастотных ошибок потому что мониторинге вы увидите высокочастотные ошибки а вот все случается на pointer exception а выход за пределы массива вы тоже хотите смотреть на баги в вашем коде и так далее и нет каждой из этих проблем она очень там никакого руки ценз не требуют для решения и там например нас есть система которая автоматизирует процесс сборки любого приложения просто вопрос добавления сервиса про добавление конфига в эту систему в и так далее но тем не менее решительно себя хотите ли вы вот все ввязываются или нет поддержка в актуальном состоянии я опять же на примерах вот мы вы согрешили за мониторить походы в мам кэш нам мониторинга того который самом к же предоставляет не хватало мы хотели посмотреть а что же с этим происходит на стороне клиента на самом деле очень важная часть любой системой распределенные то именно клиенты вот ну чё мы выделяем библиотеку ххмм кэш клиент идем по сто пятьсот сервисом обновляемых так чтобы в общем там за использовать эту библиотеку дальше мы решили откидывать запросы если сервис перегрузом перегружен опять же там зажимаем очереди отстраиваем пороги срабатывания и все запросы которые приходят на сервис если он понимает что он перегружен он просто отдает сотку или вообще живет соединение таким образом не заваливая себя еще больше ok выделяем библиотеку ха-ха-ха ттп where you tell us проходимся по сто пятьсот сервисом обновляем здесь нет никакого рокет сайенс опять же ничего в этом сложно это даже можно автоматизировать потом просто еще придется пройти по сто пятьсот сервисом и поднастроить тайм-аут которые были неправильно настроен и в самом начале и в общем мы конечно так никогда не делаем мы выбираем 35 главных сервиса от которых реально у нас болит а все остальное отдаем на откуп man тренером сервисов ребята когда вам приспичит обновляйте они естественным не приспичит до последнего момента и в последний момент они начинают обновляться долго и мучительно перепрыгивая через несколько версий и спрашивай на х что уже в там понаделали вот в той там предыдущей версии и это тоже проблема решается там административно документация или там график не знаю обновлений вот есть есть такая боль транзакции ну иногда хочется снова нечасто вот так что в общем то это небольшая боль и и тут когда вот я вот набросал дату и более с которым мы строку столкнулись очень интересно посмотреть на те преимущества которые мы извлекли ли мы их точно или не всегда то есть завить за записью например вот у нас был монолит и мы хотели его декомпозировать и вот он устроен как то непонятно есть пакет в котором дал лежит закон седова режиме давай юзер дома которые занимались с базой пакет сервис в котором лежит то же самое только там с префиксом вакансии резюме юзер пакет ресурс который абстрагируюсь нас там от нюансов типа походов реализуют данные и так далее ну естественно вот в этой структуре нам совершенно не видно никакая структуру приложения то есть каждый из этих папок на самом деле это свалка из классов на ничего более вот можно выделить пакеты папки которые будут соответствовать не технологическим слоям а бизнес fi чем конечно в предыдущем докладе сергей правильно говорил что вообще вот эти барьеры а не от его тяжело то есть инкапсулируется в том смысле что придет какой-нибудь разработчик поставит public у того метода у того класса которые хотели инкапсулировать и все и ваше ваша странная система инкапсуляции разрушена вот но иногда бывают другие контраргумента против этого аргумента говорят но и тот же разработчик начнут писать api потом и нет ничего хуже чем разработчик с хорошую способность он вам о яппи переписок ты перепишет вполне себе быстро вот модуля вы можете выделить модулю до коллекцию классов съемным зависимость это тоже пока мере на какое-то время вам структурирует ваш монолит вот то есть есть способ декомпозировать монолит будет ли он работать не будет зависит от вам решать ну да потом если вы хотя бы по папкам разложите потом хоть проще сервис выделять да они они из каждые там пакета дергать отдельный класс проще тестировать ну да заставил протестировал своей microserver служил в прато вперед но иногда хочется протестировать всю систему в целом есть конечно подхода когда мы не тестируем и выкладываем сразу в продакшен это нормальный подход между прочим вот но есть все таки есть адепта более там консервативные которые считают что систему нужно протестировать целом и дальше вы нарываетесь на сложные стенды в которых нужно развернуть все сервисы от корректировать настройки у нас вот кантри прям берется кусок продакшена копируется его с помощью тех же play буков 100 с помощью то манси бля на тестовые стенды просто корректируются какие-то настройки при затираются для того чтобы структура теста стенда она была абсолютно такая же как в пройдено за исключением того что в пробе там 5 инстансов сервиса здесь один ну а это все на самом деле это достаточно большая работа чтобы чтобы создать такой стенд и поддерживать независимое релизе выше я упоминал что отлично у нас каждый сервис можно лечить отдельно из-за этого как каждая команда который его поддерживает работает причем если мы выпускали монолит но представьте себе что вы хотите изменить api сервиса в том для этого нужно сделать ну и зарелизить services совместимым api там будет и старые новые какое-то время он будет работать со старым с новым и сервис б тоже потом зарелизить сервиса переключив его на использование нового api мы потом подальше дальше идем по сервисам бисера функции выпиливаем старой api вроде как были независимы релиза ну вот в этом случае независимой тут вопрос насколько часто колбасит ваш api насколько часто вам приходится ради реализации бизнес фичи переписывать api хорошо ли она написана или она может быть написано хорошо просто там фичу вас такие что меняют вашу систему достаточно частые там сервиса бы тестирования много об это стирание проводится и так далее это все открытые вопросы тут нету прямо каких-то рецептов независимая деградация да очень круто когда лежит как это сервис и пользователи этого не замечают но представьте себе что у вас лежит сервис поиска сайт по поиску работы и поиск сотрудников в него лежит сервис поиска чуть за сайт нас пол сайта читаете лежит вот сервис откликов от приглашений до работодатель не могут пригласить соискателя на работу соискатель не может откликнуться читаем что такой сайта работает нет а если у вас лежит сервис сессии который собственно отвечает за то чтобы узнать пользователя до его там информацию имя фамилия отчество email так далее передать во все бэг-энда то мы считаем что вообще сайт не работает никто с ним работать не может то есть это вопрос к тому насколько насколько важна ваша национальность которую вы хотите деградировать если она настолько важно что ваш сайт перестанет работать то как бы пользу от того что у вас независимо деградация вы особо не извлечете отказоустойчивость многие считают что монолиту ужасно подучи там что происходит затрагивают сразу весь монолиты все падает на практике я видел что скорее большую часть проблем вызываю вызываю сетевые походы они падают с большей вероятностью чем что-то происходит внутри монолита более того вот массандре до сих пор монолит сохраняется и честно говоря я нас он там своей практике очень редко видел чтобы реально там какая-то проблема в одном месте затронула все вот нас было бывали downtime и сдалась ваш там memcache клиент плохо работал за этого монолит подумал но это не были проблемы там скажем в какой-то бизнес функциональность это были чисто технологические проблемы и вот еще вопрос а бардан баз данных вашего сервиса микро сервис отдельная об этом говорили тут я хочу подчеркнуть что должна быть отдельная железка потому что вот ну там до башни ки чтобы сэкономить на обслуживании там вообще на инфраструктуре вполне себе могут установить ваши базы данных на одну и туже леску и вот у нас в центре периодически происходит маленькие down тайники противные когда один сервис завалил жесткие диски базы и в общем это привело к падению других сервисов про масштабирование до у вас есть возможность независимо от масштабировать ваши микро сервисы или макро сервис и не важно что происходило в случае с монолитом вы брали просто ставили еще один сервер стояли свое жирное приложение туда правда очень простое масштабирование правда интересно и когда пригонял этот доклад мне возразили но подожди твои монолитное приложение может разрастись до таких размеров что мы просто не влезет на сервера и курьезом оказалось то что это рассказывал на команда пью и которая разработала тестового стенда на которых строительстве head hunter только мини копия всего had хантера со всеми микро сервисами что же туда же все влезает в какой момент монолит станет настолько монолитом что он не влезет на современное железо я не представляю что вы делаете с микро сервисами ну да вы берете microserver ставите эту железку желательно в трех вариантах для того чтобы обеспечить отказоустойчивость ну или в 2 или в 4 это там от вашей компании зависит но ваш сервер они загружены до поэтому вы добавляете туда еще приложение на пике войти и ввязывайтесь в историю с виртуалками докером там регистрации и так далее у нас раньше были виртуалке у нас от них болела большой вверх от мы переводим сейчас продакшна докер у нас многие сервисов продакшн и в докере крутится и мне кажется что не далек недалеко от это светлое будущее когда у нас будет оркестрация куда-нить если еще что-нибудь в этом роде вот на самом деле пользовать независимо масштабирования вы извлекаете тогда когда вам действительно нужно независимо от масштабироваться а все остальное на пищевые вот сервисов в железке это все там ускорение при независимо и масштабирования а про за загрузку оборудования возможность пробовать новые технологии очень классно когда у вас за каждым сервисом закреплена команда некоторые бизнес свеча вход хантер и требует того чтобы изменить в разных сервисах что-то и за это у нас обычно отвечает то есть у нас выделяются команда под бизнес фичу они под сервиса иногда из-за этого болит команда приходит править сервис чужой виде что там spring вместо just a или чувственность и спринга им приходится с этим разбираться в этом нет ничего страшного на самом деле потому что если понимать подходы то в общем конкретный фреймворк не так важен там начинается зоопарк подходов одна команда написала в одном сервисе так unit-тест другая по другому если у вас нету четкого разделения команд по бизнес сущностям и соответствующие выделение микро сервиса под бизнес сущностью то вы попали потому что вам будет ходить команды по сервисам и прописывать сквозь эти сервисы свой бизнес свечу гибко но не единообразно команда владелец в некоторые сервисы пишут все обычно это указывают на то что сервиса плохо выделены мы это понимаем вот есть другая штука что и сырые например у нас отдельно есть команда которая занимается до отказоустойчивостью реагирует на проблему быстрее чем бизнес команда владелец этого сервиса потому что у них свои проблемы у них там деноминации из беларуси надо срочно переписать весь биллинг вот там что что антон у вас там сайт падает это уж там как-нибудь сами разберитесь вот его серые просто чисто больше технологических знаний и нам физически просто работать с монолитом потому что мы там что там что то там исправим это сразу все работает вот обойтись по всем командам еще донести до них те знания которые мы получили от непростая задачка мы регулярно вход хантами проводим этапы дымке там и так далее тому подобное [музыка] вообще-то в проблема микро сервисов не требует рокет сайенс для решения и зависит от вас хотите ли вы в это ввязываться или нет как хан 40 раз на мой взгляд находится в такой брак в расколбаса у нас очень много адептов меграхи разной архитектуры которые говорят да антон давай уже допилим все вот то есть люди которые говорят да ладно поживем еще у нас там скорость роста просто технологических от hunter не такая большая чтобы там вкладываться в микро стираются настолько чтобы мы стали из этого пользу извлекать вот и тут интересно посмотреть на то что я вам рассказывал и большую часть вы увидели что я рассказывал не бы преимущество продам скажем недостатки на самом деле чины доставки очень легко нивелировать если плетни микро сервиса правильно как пилить правильно вот у вас есть монолит в который приходит запрос он их обслуживают отдает обратно ответы и вы можете его поделить на слои появляется слой фронтэнда как у нас есть вход hunters который там занимаются обход там походом по бокам дом наложение на все это дело там шаблонизация и так далее и у вас есть слой бэг-энда который собственно отвечает за взаимодействие с базы данных слоев может быть много да у нас еще интеграционный слой в этом в этом пироге вот что здесь плохо с каждый слой от но он с точки зрения бизнеса нет сам достаточно в том смысле что вы не можете выпустить бизнес свечу в одном сервисе вам приходится прописывать ваши бизнес фичу сразу через много слоев у вас достаточно много сетевых походов он на каждый запрос пользовательские от слове один наверняка будет один или несколько походов слой 2 надо больше у вас появляется максимум более от всего это архитектуру потому что у вас походов много сервис ни за кем не закреплён независимые разработки независимых релизов вы не получаете пробовать новые технологии не можете а зато у вас походов много и система распределенные вот и в общем минимум профита 100 если вы распилить его шмонали таким образом чтобы там были достаточно хорошо выделено вертикальные составляющие бизнес сущностей на чтобы вас какую-то специальность на сайте можно было показать просто сходив в один сервис в пределе да тогда вы получаете в общем вот все сервисом достаточно его разрабатывает команда у вас мало сетевых походов желательно чтобы их было мало конечно совсем их не станет вы извлекаете максимум профит и минимум более это конечно предельные случаи for hunters гибрид и вот например у нас есть секс история когда выделен сервис api у него есть отдельная команда она его поддерживает до это не самодостаточное слой что пили команде api необходимо ходить в бэг-энде там иногда там что-то правильно в принципе очень хорошо история что он выделен как и с сервисом мобильной версии сайта интеграционный условия на мой взгляд наша боль потому что через него пропитывается абсолютно вся функциональность это фактически еще один монолит или может быть просто отпочковался там от изначального монолита еще один сервис который там микро сервисом ни в коем случае не является просто там инкапсулирует интеграционную логику должна ли она быть отдельно или и можно было ставить монолите это еще там большой вопрос у нас я рассказывал есть разборки сервис поиска который тоже неплохо выделено да они там банок вообще самодостаточная а сервис поиска не самодостаточна в том смысле что он дает а и диски кому нужна одышки хочется посмотреть на вакансию в целом да но тем не менее это там у нас целая команда которая занимается там в том числе машинным обучением а официант сам который the service пишет а мы на давай из-за сарая приходим и говорим о ребят вот здесь надо поправить потому что у вас там технологически неправильно все сделано ну вот кажется я рассказал почти все что хотел если какой-то делать вывод из моей презентации наверно он будет таким что сервисной архитектура и командный архитектура должна соответствовать прежде всего бизнес вещам они представления на красивой архитектуре у меня это все спасибо сейчас нас будут вопросы мне сразу маленькой техническое замечание у нас есть у всех точнее у вас у нас у всех есть приложение мобильная и всем докладчиком будет очень приятно если вы в мобильном приложении будете там ставить 5 звездочек то есть им конечно и за 4 звездочкой будет хорошо и за три можно котики да и все отеки а в буа бой тени в мобильном приложении будете тоже можно оценить докладчиков я забыл вам сказать ногти были кто был на предыдущих докладах пожалуйста оцениваете докладчиков спасибо за доклад за такое практическое изложение проблемы достоинств мне такой вопрос вот как раз последний слайды были значит мы нарезаем приложение горизонтально и вертикально у меня клики перестал работать я не могу хорошо там не обязательно вот в общем нас есть монолитный сервис у него есть монолитная админка вот которая имеет авторизацию как а ну как и в обычном монолитных в приложениях 1 1 логин пароль и весь функционал причем вся админка как построена на базе какого-то фреймворка то есть построена средствами framework то есть бэкон формирует html и css выбрасывает то есть кнопочки крут все сделано да вот и когда мы будем распиливать все это дело на микро сервисы как быть админке то есть если для бизнес фич мы вопи можем выбросить только 510 бизнес методов которые необязательно круто только е ты там и ничего больше да тут админка как правило требует много большего функционала и крут по полной на на там параметр атрибуты и вот как быть оставлять это написано на framework ах чудо все до или же через api этим управлять но там фронт-энд писать фронте не рама backend нубы киндеры как ты уже там это взаимодействовать то есть как это устроено у вас и там соответственно если какие если разная часть админке проблемы с авторизацией вот такой длинный вопрос то у нас на самом деле устроено и так и так дело в том что у нас есть сервис и который находится под одной админкой есть команда которые пишут совсем свои сервисы они решили свой собственный админку замутить вот и и им от нравится у нас есть сервис который занимаюсь с узнаванием пользователя прям когда фронтовой запрос приходит нашу систему пользователь узнается и дальше все сведения об это пользователя его правах так далее пробрасываю ца прямо в заголовке х ттп во все бэг-энда в этом смысле на статус бэг-энде вот и поэтому эта лодка в полностью можно в баках посмотреть она что мы за сессия прошла у него таки это право может сделать это не может сделать это и все и те кто пишут сервис они в принципе не думают об админке думают только о правах которые предоставлены или не предоставлена если у вас такая проблема что одному человеку нужно сходить и туда и туда и туда и как эта с с он то есть чтобы ни разу не надо было ему авторизироваться много раз и какой у нас есть единый сервис тематический стек может сейчас быстро сказать основании чего взял технологический сток наших брендов нет вот этого сосуда есть на чем это все сделали руками на коленке самописная хороший код понятно спасибо здоров антон у нас тут интимный вопрос такой тоже про авторизация вот вы подняли вопрос независимо деградации а вот имя он тоже очень сильно интересовал мы хотим ими красивой с авторизации сделать как обойти вот проблемы с тем то что нас авторизации ложится и больше никто ничего сделать не может потому что ну как бы проблему понятно проблему принципе достаточно очевидно но как бы какие-то готовы классных решений я до сих пор не слышал про него я не буду говорить как сделать вам я расскажу как сделано у нас да я думаю что мы тут только поймем вот когда у нас лежит сервис авторизации то мы отдаем анонимную сессию просто она не не анонимного пользователя и пользователя который приходит наш сайт имеет возможность работать с сайтом но сайт его не узнают то есть это вот такая контролируемая деградация она в этом заключу естественно все это есть триггер естественно мы считаем downtime а если мы отдаем много анонимных сессий но вот мы поступили так а вопрос а что-нибудь знаете вот не знаю кто-то так делал может быть сами думали закажите условно все права и по крайней мере для старых сессий сохранить вот все эти заголовки который опрокидываться дальше на бэг-энда но и надеется то что этого хватит хотя по какому-то процент пользователей ну такой проблемы не стояло честно говоря там сервис узнаванию нас падает достаточно редко чтобы мы думали тем что дальше как-то прокидывать права пользователей frontend из японцев контента нам свои права прислал да еще нужно думать о сик юностью чтобы он там их не подделал ну и так далее просто такой задача не стояла мы с этими проблемами еще не столкнулись хорошо пойдем так мы задаём вопросы у нас есть рука поднимаем руки раньше тогда мы сможем задавать вопросы более оперативно не слышно но я хотел бы узнать по поводу для каждого сервиса своя база данных вот насколько это распространяет то есть предыдущем докладом я понял то что там может быть база данных именно совершенно различные то есть и например кто-то собирает ли аналитики часто базу данных да и работать именно по аналитике то там ну для своей бизнес-логики вот и меня такой вопрос то есть именно насколько она роста страниц насколько у вас она баз данных разных то есть я жену обычно жить играет какая-то большая база данных ну да о котором ну все и например сервис может работать какой то какой то этой частью как у вас этот вопрос решен то есть вы реально делайте сыр именно слепы к баз данных и обновляете его там или какая-то идет репликация насколько этот вопрос отдельно базу данных для каждого сервиса на насколько как как он вообще выглядит как он выглядит но ротацию или в тестировании в эксплуатации то есть как какими примеры очень просто у нас есть основная база данных которые лежат основные бизнес сущности вакансию резюме отчасти пользователя и так дале тому подобное и она высоконагруженные у нее есть мастер реплику несколько реплик с которых можно только читать данные на которой данную долетают и либо кэндо устроена таким образом что стараются брать данные с реплик который легко отмасштабировать допрос доставив нового железа и стараются можно меньше ходить именно в мастер для изменения запросов вот так же вот основная база у нас есть много всяких отдельных бас которые для которых просто по степени критичности выделяются разные железки есть там средне критичные базы для них отдельный железка есть там совсем не критичная база они там на другую железки крутится они тоже имеют возможность масштабироваться в том смысле что мы можем добавлять реплики бэг-энде про эти реплики знают опять же автоматически переключаются между нужной репликой если одна из них упала у нас есть кассандра которая в которой мастер не имеет единого мастера имеет каждый моды является мастером фактически вот у нас есть даже два кластер кассандры один отвечает опять же за критический функционал без которого мы жить не можем 2 замуж за какую-то данную помойку где мы храним много данных и так далее но обычно все-таки сервисы каждый сервис и свой собственный кластер кассандра не разворачивают вот используют обычно общий класс не какой-то я спасибо спасибо спасибо за доклад у меня вопрос как раз по поводу баз данных и реагирование на скажем так инцидент то есть как у вас происходит система ну вот что-то пошло не так где-то что-то коротнуло как у вас идет система ролл беков восстановления после какой-то тепло и неудачной фичи если таковой бывает на моей памятью что я работаю вход hunters краб туда нос у нас еще не было мы бэкап и регулярно делаем и даже службы эксплуатации регулярно проводят учения по восстановлению с бэкапов но честно говоря инцидентов связаны с кратностью данных у нас не было поэтому вот вот так и можно еще один вопрос распределенных то есть ну наверняка же вас а сам контент который ну-ка файлы какие-то еще как-то то есть это же определенное хранилище то есть на чем у вас это работает цех что именно ну контент распределены ну не знаю там картинки видео что у вас там еще есть то есть вы же где-то хранить распределенные то ну да у нас там есть какая-то система из вообще у нас эксплуатацию свое время написал кассандру на джинсах фактических вот они сделали что несколько серверов на каждом установлен индекс которая даёт статику и определенным образом определенными скриптами между этими инстансами данные реплицируется кто приходит запрос у меня есть этот шарф нет уж адапаленом пойду от себя его возьму пойду с другого сервиса возьму это система работы до сих пор единственно что есть кэширование перед ней на уровне прямо фронтовых серверов ну вот как-то так это работает довольно прикольно ну да ну как самописная там с вами не так много кода индекса дает данные какой то скрипт их реплицирует статику отдавать прямо актуально в общем никакого резона не ну не нужно точно то есть какая разница он поедет вот вот и все здравстуйте большое спасибо за доклад вот вопрос такой объемный но постараюсь разбить что было понятно и мне и может быть кому то кто тоже задался таким вопросом всегда ли под микро сервисами стоит понимать какую-то одну большую железку на которой разложено либо докер контейнеры либо виртуальной машины то есть или же нужно держать для каждого сервиса какую-то одну единицу там сервер железной там или тут вопрос чего вы хотите да то есть если вы хотите железобетонно отказываетесь так далее наверно у вас будет микро сервиса просто сервисы и каждый будет стоять на своей железки с другой стороны если вы понимаете что это экономически неэффективно зачем под каждый сервис держать железку которой еще нет загружены требует обслуживания то вы наверное пожертвовать отказоустойчивость там и так далее в пользу чего-то еще вообще но там не знаю вся архитектура это про систему балансов вот можно выиграть в одном прыгать в другом микро сервис это про это ж спасибо очень даже понятно и собственно вопрос который вытекает из вот этого у вас на слайде было видно что у вас как раз таки вот на одном сервере виртуалке и контейнеры что вы делаете в случае отказа железного сервера гипервизора хост машины они как-то мигрируют либо просто за дублирована другом сервере там нет мы эту проблему решаем тем что у нас есть 100 запас по мощности примерно 30 процентов мы закладываем на то чтобы запас по мощности был мы регулярно нагружаем сайт прямо нагрузочным тестом прям сайт по живому чтобы убедиться что он при возросших нагрузка на груз как работает вот и пока угревую живем без мигрировали я просто упал к это докер железками черт бы с ней большое спасибо здравствуйте спасибо за доклад хотел бы задать вопросы по вашей менее копии так называемого продакшена то есть вы туда выкатывает сервисы которые сконфигурированный прямо для продакшена или вы хотите сделать отдельные сборку для вот этой тестовой схемы и выкатывать туда мы стараемся использовать все те же скрипты и это обычно они были скрипты которые есть в эксплуатацию для того чтобы развернуть сервис мы его разворачиваем в докер контейнеры используя эти скрипты но конфиге подкладываем собственные даже не столько собственной конфиге мы в некоторых участках конфигов пропиливаем placeholder и что вот здесь используют значения для продал здесь используют значения для тестовой инфраструктура вот как-то так получается но это те же сервиса абсолютно из той же детки даже ну то есть докер имидж он один и тот же там и там то есть ну замены там там я наверно точными расскажу об этом гораздо лучше расскажет наш клей вот мы дело в том что мы сейчас в процессе перехода на docker да то есть нас какие-то сервисы уже в продакшене крутится в докере вот какие-то сервисы до сих пор debian пакетами раскладываются вот не хочу не знаю не хочу быть некомпетентным не отвечу хорошо вам спасибо этот образ [музыка] здравствуйте спасибо за доклад вот у меня такой вопрос водная обрезана у вас было диаграмма там были указаны сервиса там a b c и вот с таймингами которые пробрасываю ца как вы вот-вот иди тайминг пробрасывать если у вас появились новые сервисы и вот что вы делаете какая стратегия ничто не можем продолжать делить тайминги игнорировать бреду я мы кадр конечно это я провел предельный случай мы конечно так не паримся более того у нас в некоторых сервисах мы намерены отключили троя мы сказали от них больше более чем от чем польза потому что этом во время инцидента фраза заваливают других эти ретро и настроили сервисы которые не очень критичны для продам и особенно там выставили вы ставили какие-то дефолтные тайм-аут и соображение того что но не должен запрос дольше двух секунд то там в этом сервисе жить стервецы который гораздо более критично ко времени ответу мы там действительных проанализировали выставили другие тайм-аут и то есть все зависит от того какую выгоду мы хотим извлечь настраиваю все эти тайм-аут вот все еще один вопрос по поводу сервис интегратор вот вы говорили о ну там большая боль с ним вот хотелось бы по подробнее что именно вот вот у нас есть сервис у нас там есть какие-то скрипты и ну или что-то или что что это такое вот который знают о других сервисов которые ну как-то или бы вы наказываете это лазер вы имеете ввиду сервис discovery какой то нет но сервис discovery понятно я имею ввиду вот у вас есть сервис он map интеграционный что вы не можете его просто перейди плавать или в чем почему проблемы сервис интеграционную ну проблем интеграционного сервиса вытекают из того что на самом деле он не делает ни какую полезную сущности именно для бизнеса да то есть он с одной стороны принимает запросы которые впитал впитывает от фронтэнда особенно с ними ничего не делает кроме того что прокси рует на бэг-энда и в зависимости от ответы от брендов идет на другие бэг-энда и так далее да вот то есть фактически получился никто такое монолитное приложение которое добавляет сетевого вверх это добавляет этот храм насели за цию стерилизацию но при этом [музыка] там бизнес логики никакой нету что ли там бизнес лойко есть она просто минимально вопрос нужно ли для этой бизнес-логики выделять именно отдельные diplo июнь от или может быть этот небольшой кусок бизнес лодки сходить туда когда ответит возьмите данные сходить туда да держать вместе с другой бизнес лойко это открытый вопрос на самом деле у нас опять же есть сторонники оставления этого интеграционного слова дантона все-таки он как-то диком позирует монолит дано есть сторонники которые говорят что нет давайте обратно интеграционный слой куда-нибудь сервис который отвечает закон функциональность до вас я как-то построил просто диаграмму сетевого взаимодействия казалось что 30 процентов запросов у нас всех вот countries лежать все все запросы до 30 процентов это будет между интеграционным слоем и слоем скоро функциональностью вопрос вы хотите держать интеграционный слой отдельно или может его засунуть слой скоро функциональностью хули они так сильно связаны вот ну а если у нас есть вот это последовательность действий и скрипта из началом этот сервис мгц а потом нужно их сторнировать если что-то пошло не так нам все равно нужен этот интеграционный слой не а почему эту логику нельзя написать в любом другом месте почему для этого нужно выделять отдельные типологию нет отдельной репозитории и так далее тому подобное это жилой как можно написать в любом сервисе хорошо спасибо здравствуйте меня зовут дмитрий меня такой вопрос по поводу было ли было ли у вас практика прямо в продакшене например изменить именно в репрессированные sbd изменить структуру я просто опишу проблему которую на свое время возникла есть на по интернет магазин и есть некоторые карты лояльности которые привязываются к клиентам и в качестве там primary key мы используем как раз номер карты вдруг через полгода через год возникает потребность ввести нового вендора и получается что мы не можем использовать это первичный ключ и нам продакшене таким хирургическим путем приходилось перри привязывать первичным тары и коричные ключи вот была ли у вас такая практика проблема особенно когда мы говорим ориентированных sbd там tt еще проблематично и все сделать схема базу данных мы регулярно меняем в том числе заменяем одни ключ на другие так далее это бывает необходимо иногда это приводит к да он поймал в том смысле что эти данные прилетают на реплику то молочную таблицу там и так далее я честно говоря не большой специалист в области под прессом и на погрузчики вот тебе и город душ об этом расскажет такое бывало вот ну как с этим живем ну я просто немножко как бы уточню когда просто добавить одно поле это легко когда надо отвязать перри привязать с десяток таблицы вот эти вот первичный и вторичный ключи но это пароль для все застрелиться нам приходилось со времен скрипты просто писать на питоне который автоматически это делать перед этим 10 раз локально протестировать развернуть кластер там протестировать на форки продакшене и уже после вот этого убедив что ничего это него лице вот мы стараемся делать все инкрементальный в том смысле что выпусти какой то скрипт на прот посмотреть работают выпусти другой на пробу смотреть работает и постепенно шишками приходим к той схеме который нам нужно бывает для этого требуется несколько релизов да то есть у вас принципе тоже возникают такие довольно сильно добрый день у меня простой вопрос чем отличается сервисный orchid у от микро сервисной тут как-то вообще в катится хоть какая-то сейчас есть или она потерялась окончательно отличный вопрос я например считаю что не крась сервисной архитектуру это там предельный случай сервисной архитектура как монолит тоже предельный случай сервисной архитектуры в котором всего один сервис да вот я не знаю мне честно говоря никогда не приходило в голову для какие-то четкие критерии просто их в бизнесе особенно не по используешь назовешь ты своим сервисом микро сервисом или просто сервиса на учет изменится но постер пьера понимаю разницу не знаю в том же амазоне мы их дома за мля мда которая такая четки микро сервис и здесь сервис уже не сделать и там нормальным каким-нибудь со страшным сервером который какой-то сервис реализует вот а при этом как понимаю почти все что у нас происходит вот в нашем энтерпрайзе там в общем это нормальный сервис а никто не пишет сервис в десять строчек отдельных никто реально это все-таки нормальные хорошие она ценные большие тяжелые окей все давайте еще раз поблагодарим докладчика [аплодисменты]
