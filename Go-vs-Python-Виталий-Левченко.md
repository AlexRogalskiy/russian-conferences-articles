**Предлагаю ознакомиться с расшифровкой доклада Виталия Левченко Go vs Python**

Go — волшебное слово, решение всех проблем продакшна разом и одновременно негодная технология без эксепшнов. Истина посередине, поэтому поговорим о конкретных примерах:

- asyncio vs горутины;
- производительность узких мест;
- лаконичность vs простота кода;
- порог входа;
- тулинг и паттерны поиска проблем и оптимизации производительности;
- обслуживание в продакшне.

<oembed>https://www.youtube.com/watch?v=a8oEETIRpCk</oembed>

<cut />

![](https://habrastorage.org/webt/4d/wr/vf/4dwrvfvdawsbws2m8mf2qusiaqc.png)

Я буду рассказывать про Go, потому что я на Go пишу примерно с 2012 года, а на Python я уже даже перестал писать. Я думаю, что я акцентирую внимание на том, что не очевидно. Моя риторика по поводу Go обычно довольно простая, что-то в духе «бросайте свой язык программирования и пишите на Go». Тут я надеюсь выжить до конца конференции, поэтому буду чуть помягче. 

Собственно, почему я это всё делаю? Потому что я продвигаю Go с 2012 года, делаю Go MeetUp’ы с 2014, делаю кучу сервисов на достаточно большие нагрузки.



![](https://habrastorage.org/webt/gq/za/_n/gqza_n8d5n-gbqbyvjo9tj601s4.png)

То, с чего вообще начинаю сравнение языков – это бенчмарки. 

С бенчмарков вообще принято сравнивать. Если начинать холивар, то говорят: «Вот у нас есть мега бенчмарк, дергаем http, собственно, одно быстрее, другое медленнее». И вот так устроена половина холиваров в интернете уж точно. Мы не будем исключением, возьмём http. На самом деле возьмем просто бенчмарк, который сделали прекрасные люди из techempower.com/benchmarks/. У них http и довольно простенький формат в духе веб-приложения, достать 16 текстов из базы данных, отрендерить и выдать по http.

![](https://habrastorage.org/webt/ua/kl/_q/uakl_q8xgt4w3u-_snzndnps94m.png)

Они тестируют на довольно неплохом железе, на 14 ядрах, с гипертрейдингом. 

![](https://habrastorage.org/webt/nw/ex/ke/nwexketyzcovi-0dgfe4mg_cuu0.png)

И выдают следующие цифры. Собственно, 329 000 запросов в секунду переживает Go со специальным модулем для http, без специального модуля на нативном http он переживает 180 000 запросов в секунду. Это больше, чем перемалывают обычные сетевые карты.

![](https://habrastorage.org/webt/gk/al/rl/gkalrlbrneqjdw5e4kpvuzr_jiu.png)

С Python все гораздо грустнее. 65 – 70 тысяч запросов это специализированные фреймворки, для того, что отвечать быстро.

![](https://habrastorage.org/webt/l6/vc/1o/l6vc1ohv1ahyqn8zbsze2hawrye.png)

А если берем django, всё ещё грустнее. Мы видим разницу больше, чем на порядок, то есть в 5 раз разница идёт со специализированным фреймворков, в 5-6 раз идёт разница с http, и django гораздо медленнее, ну и все остальные Twisted и прочее.

![](https://habrastorage.org/webt/_a/ct/8p/_act8prbq1debonqcs6uddsycsk.png)

На самом деле имеет смысл сравнить latency, потому что это часто более важно. Я работал в рекламе, в стриминге. Там было важно. Если задержка больше 100 миллисекунд, то пользователи уже начинают жаловаться, что подтормаживает.

![](https://habrastorage.org/webt/jx/k-/6r/jxk-6rk_vj-vjv4xmixaugvdbtk.png)

Фишка в том, что Python оказывается вполне с адекватными цифрами с latency, т.е. 7 миллисекунд на ответ на технагрузку это прямо хорошо. Но больше всего меня удивил django. 

![](https://habrastorage.org/webt/pn/ho/jb/pnhojb4qzwxg-x1m6niznjqqmty.png)

Latency у django получилось меньше, чем у Go. Если возьмёте в руки калькулятор и просто разделите одну цифру на другую, то есть вот эти 14 запросов в секунду, 28 ядер и 2 миллисекунды, оно поделится ровно.  Django отвечает без накладных расходов. Я к тому, что на самом деле все не так плохо, как показалось с самого начала.

![](https://habrastorage.org/webt/i4/hv/18/i4hv18ij1dvbwkxim1m5i-46bdq.png)

Следующий момент, который сравнивается в бенчмарках, те люди, которые более-менее прошаренные, как работают приложения, это аллокации. Собственно, в чём суть? Нам периодически из кэша надо доставать кучу данных, например из redis или базы данных. Скажем, мы достаем 10 000 объектов по 100 полей, такие обычные ситуации, то есть, скажем, 10 раз по 1000 объектов достали и как-то их пережевали. В json это какие-то смешные 20 мегабайт, а то и 10 мегабайт. 

![](https://habrastorage.org/webt/a7/oc/dj/a7ocdjf4afbtufkaecefuzvfzoa.png)



Мы достаем из redis данные за долю миллисекунды, сеть их приняла за долю миллисекунды, а после этого Python их переживал ещё 200 миллисекунд. Осознайте эту дельту в цифрах. Больше того, после этого обработать эти данные будет стоить 10 миллисекунд в худшем случае. Локация – это реально дорого. Но фишка в том, что Go не сильно быстрее, но в Go есть другая штука, называется «кэш в памяти», мы можем просто этот кэш засунуть в память, не дергать его из базы, не дергать его из редиса, а просто брать и использовать. 



![](https://habrastorage.org/webt/6x/v6/qu/6xv6qutrc0dokc-1wbj6k1v5cc4.png)

Но мы можем попробовать использовать multiprocessing в Python.  Дальше увидеть то, что обновление этого объекта в памяти, это 2 секунды, в течении которых приложение не может доставать из кэша ничего. То есть каждое обновление элемента кэша — это фриз приложения на 2 секунды. И вот это вот, это не в тупую запись, это просто перезапись объекта, то есть dict-update или что-то в таком духе.

![](https://habrastorage.org/webt/pv/cv/r-/pvcvr-hei8ozixa4ykjuc_5chsc.png)

Поэтому мы можем память не шарить, а просто держать ее в памяти локального приложения. Но дальше мы получаем очень грустную историю, если нам объектов нужно хранить много, скажем, 10 Гб в целом приложении, то мы получаем грустную историю, нам на 28 ядер нужно 280 Гб памяти. И это нерешабельная проблема в Python. Если вам по каким-то причинам нужно перемалывать хоть сколько-то значимое количество объектов, то Python просто не про вас. 

![](https://habrastorage.org/webt/xf/e2/q0/xfe2q0aslngig_8dbi1hd2zgkka.png)

И еще один важный момент: если вы всё-таки делаете 28 приложений, то есть вы запустили multiprocessing и крутите. Вы хотите сделать общие счётчики, например, метрики считать, просто какие-то счётчики. Выясняется, что сохранять в redis будет стоить 0,2 миллисекунды на каждый запрос. В случае Manager.Value плюс-минус такие же цифры, т.е. если мы в общую память сохраняем. Плагин Prometheus для счётчиков используют mmap и merged их на живую. Я не до конца понимаю, почему это работает и не падает по дороге. То есть он просто берёт на каждый процесс mmap файл и после этого без всяких mutex просто берет и merged. Это выглядит страшно, но по виду работает.

![](https://habrastorage.org/webt/sz/cl/ev/szclevm6eam5yqa7ogq3plm0yvk.png)

В Python в 2018 году появилась нормальная асинхронность Async/Await.

![](https://habrastorage.org/webt/g0/fr/ff/g0frffiyzcipmlhnjbg-c_02qqs.png)

Логика вот какая. Я веду мысль к тому, что есть кейс, с которого я начинал знакомство с Go. Он выглядел примерно следующим образом, у нас были просто очереди в TopFace, они перемалывают какие-то лайки, запросы, сообщения, обработку подарочков. Технически это просто очереди, которые перемалывают демоны. Они упираются в кучу запросов в базу, в какую-то арифметику на этих данных и ЦПУ. 

Представим, что у нас на машинке те самые 14 ядер. Как вы думаете, сколько воркеров надо запустить, чтобы эффективно утилизировать ЦПУ? 

На самом деле, действительно, зависит от ситуации. Реальная цифра порядка 100-150-200, в зависимости от того, насколько быстро базы отвечают, и это в случае, если мы не используем нормальные goroutine и асинхронность. Больше того, если у нас база начинает отвечать медленнее, у нас ЦПУ не догружена, если база отвечает быстрее, у нас процесс оказывается перегружен на 200 потоках у нас 200 и машинка перестает отвечать хоть как-то. Поэтому нужны goroutine, поэтому нужно эффективное использование каждого ядра в таком формате.

Почему это важны goroutine в этом контексте? Мы можем запустить 200 потоков, которые просто что-то делают, а можем запустить 100 потоков, а можем 14, и по-простому что они делают во время запроса в базу? Если у нас goroutine, во время запроса в базу, она обрабатывает другие потоки, если у нас не goroutine, то просто thread залочен и все. Поэтому в случае goroutine мы хоть как-то это утилизируем. Этим Go крут. Потому что мы просто запускаем приложение. Оно просто скушает все ЦПУ. Поэтому переходите на 3.7, если у вас многопоточное приложение и вы упираетесь в ЦПУ. 

![](https://habrastorage.org/webt/jo/kz/yj/jokzyje0uaq0tqmel9io3q4nh54.png)

В принципе по асинхронности самое важное, что есть в Go, это обработка в scheduler. Она сделана хитро. То есть просто syscall при каждом вызове отдают goroutine в scheduler с пометкой, что она не занимается ничем полезным, ивозвращает из scheduler, когда syscall заканчивается. У нас в контексте 14 ядер, будет запущено ровно 14 goroutine, которые не в syscall, ну или меньше, если ядер физически меньше. Это работает, причем работает магический круто, т.е. вы просто запихнули какие-то goroutine, они что-то делают, они включаются в базу, на диск, еще что-то, ЦПУ утилизруется нормально, и ничего не блокируется, потому что все блокировки, они в первом пункте.

С mutex аналогично, но поскольку они в нативной библиотеке, стандартной, все проработано и точно так же работает.

![](https://habrastorage.org/webt/av/n3/-o/avn3-o32pgvs4nob5go6ssoq_mu.png)

На самом деле в Asyncio всё похоже. Точно также, если идёт запрос в сеть, он просто передается в Scheduler, который разруливает ее с какими-нибудь kqueue, epoll. Это красиво работает, это относительно синхронно на текущий момент выглядит, не так, как у Go. Но с Async/Await это хоть как-то хорошо. 

![](https://habrastorage.org/webt/y3/h6/fl/y3h6flzsfuu6bwspz4aq61zmgba.png)

Беда в том, что не все функции не блокирующие. Это приводит к грустной истории. Вот мы описали честный поток Async/Await. А потом внутри случайно вызвали функцию, которая что-то блокирует. Чем больше приложений, тем больше вероятность, что мы такую функцию вызовем. У нас весь runtime встал целиком, пока функция не разблокируется. 

![](https://habrastorage.org/webt/gj/xb/fj/gjxbfj9wc5jgooobf4-dbhzeqj8.png)

Поэтому нужно использовать внешние библиотеки, которые Asyncio точно поддерживает. Мы открываем aio-libs, которые пишут 11 баз данных. У них, правда, есть всякие интересные проблемы, например оно падает на каких-то хитрых ответах или при попытке сделать Close, но оно делает вид что работает. Там нет важных для production баз типа Aerospike, и на самом деле там не хватает ряда полезных таймаутов. Это проблема. Если у вас есть какой-то таймаут базы, который вы не можете поставить, и этот таймаут сработает, то у вас корутина сдохнет на время таймаута, и вам нужно либо снаружи обрабатывать каким-то странным образом.

![](https://habrastorage.org/webt/3j/dr/fr/3jdrfrgbuizhpdj3k3jablpdb9s.png)

Но интересно другое, интересно то, что в Python нет нормального способа коммуникации между тредами, а в Go это есть из коробки, и вот за это реально надо любить Go. То есть в данном случае мы можем честно взять и уведомить все goroutine, в которых происходят какая-то обработка и сказать им: «Надо закончиться», например, мы делаем ShutDown, то есть мы шлем сигнал, все аккуратненько заканчивается и после этого мы останавливаем приложение. В случае Python нужны либо поддержки соответствующих обработчиков, то есть поддержка внутри http, внутри net’а и ещё чего-то, shield’а который нормально не работает, ещё чего-нибудь в таком духе, и всё равно это неудобно, вы всё равно пробрасываете огромное количество каких-то вещей, и у вас нет гарантий, что он завершится, а здесь прям простенько, из коробки, вы просто делаете канал, в который вы шлете сигнал о том, что приложение завершается и обработчик заканчивается. Вот как-то так. 

![](https://habrastorage.org/webt/t8/q7/bg/t8q7bgl5ad2vkzetusxeadbgifq.png)

![](https://habrastorage.org/webt/eb/-h/9f/eb-h9fz8twvdj8vtcgjxlpd0hrw.png)



Больше того, select еще крут тем, что мы хотим читать сразу из 5 разных потоков. вот у нас есть 5 разных продюсеров, они пикируют 5 разных видов данных, их надо обрабатывать по-разному, мы их можем просто взять и обрабатывать всех по-разному. В случае Python нам их нужно объединять в единый кью, то есть сперва объединить с потерей типов, потом обратно разъединить с нахождением типов, потом узнать, что нужно читать в двух разных местах и всё сломается. В Go такой фигни нет, и это круто.

Теперь перейдем к самой, наверное, холиворной вещи в контексте Python и Go. Люди говорят: «Я пишу в DataScience, я всякие модельки рассчитываю, умножаю их и так далее». Кто, кстати, из вас DataScience занимается или Машерингом? Оу, неплохо. На самом деле DataScience, это здорово, и за него хорошо платят, поэтому заниматься DataScience, это здорово. Но в Go на самом деле все не так уж и плохо, то есть более-меня все популярные библиотеки, всякие Boosting, деревья, вот это всё, оно сделано и сделано нормально. Больше того, если вы в какой-то момент начинаете упираться в ваши библиотеки, например, у вас обучение работает слишком медленно или оно вам нужно в других параметрах, или вам нужно онлайн-обучение, сразу выясняется, что ничего готового нет, надо делать самим. И в этот момент выясняется, что то, что вы сделаете, может получиться в разы быстрее, чем в Python'овской либе, причём вы не сильно напрягались, ну или напряглись, а получилось сильно быстрее. То есть вот есть пример, есть в SiKit, расчёт рекомендаций, т.е. это штука, то, что называется коллаборативной фильтрацией, когда у нас есть пользователь, который, не знаю, фильмы лайкает, мы хотим найти, собственно, похожих пользователей, и собственно, исходя из этого оценить для пользователя, как он оценит текущий фильм. Есть SiKit, у нас есть библиотека там, которая SVD умеет. Там есть 2 алгоритма, SVD, который обсчитывает модельку 2 минуты и SVD++, который обсчитывает эту модельку, конкретную, за 3 часа. Как вы думаете, за сколько Go считает SVD++? Кто ставит, что час? Окей, 2 минуты? Ну ладно, похоже вы не знаете, но на самом деле правда 2 минуты. Эта цифра забавная, то есть логика в том, что если вы берете библиотеку, которая хорошая, enterprise’ная и т.д., она много где используется, вы можете в ряде случаев увеличить производительность, грубо говоря, на два порядка, и больше того, даже если вам это не так нужно, то есть просто вам хочется улучшить текущую производительность там, где нормально оптимизировано, вы можете просто взять как-то аккуратненько дооптимизировать, скажем, UX взять и получить прирост ещё на 20%, как видите, для обычного SVD. Это прям стоит того, чтобы использовать. У меня есть знакомая, которая, собственно, использует Go именно для конкретного обучения, просто взяли и выкинули из парка, переписали нужную генерацию на Go, это стоило им в разработке что-то типа пары недель, что вполне адекватно на мой вкус.

И тут мы подходим к тому, ради чего, в принципе, с Go хорошо делать, когда мы говорим про производительность, это всяческая оптимизация. В чём фишка? Фишка в том, что мы берём Go, пишем как-то, ну реально как-то, как-то вот мы нафигачили проектики, функции повызывали, он работает быстро, если мы хотим, чтобы оно стало работать еще быстрее, мы просто вызываем профилирование, и оно начинает работать в 2 раза быстрее или в 10 раз быстрее, если мы что-то не так сделали. Фишка в том, что профайлинг делается в Go в одну строчку, просто импортить pprof, и он выкидывает наружу по http, если вы анонсировали следующий заголовок, эту ссылочку можно открывать прямо в браузере или, скажем, открывает через pprof утилитку нативную. И она делает магию, она показывает, что в конкретных либах, в том числе системных, в том числе какие-то syscall тормозят, то есть она говорит, скажем: «Тормозит Garbage Collector» или «Тормозит конкретный мьютекс», или «Тормозит конкретная ваша логика», и это офигенно круто. Прямо из этого pprof’а можно дернуть листфанг, и он прямо ещё построчно покажет, на каких строчках она сколько времени проводит, дальше мы можем вызвать disasm этой функции и ещё увидеть, на каких строчках оно так себя ведет. Это выглядит примерно вот так, на самом деле оно, это общий профайлинг это плюс-минус одинаково для всех языков, с которыми я работал, но это просто визуализация времени вызова. Есть FrameGraf, который внезапно в Python появился раньше, и вот то, что я говорил, можно построчно показать, как это по миллисекундам работает. И, собственно, то, что здорово, это то, что мы можем взять этот самый gcflag-S и, собственно, получить исполняемый ассемблерный код прямо на живую, или можем поставить gcflag-m и прям… ну это при компиляции. После этого нам с pprof будет выдавать информацию по Inline функциям локации на хипе и очень прекрасные вещи. Мы можем взять и переписать функцию на C Ассемблере и после этого просто понимая, что происходит, вот мы видим какую-нибудь функцию, которая работает 8% времени, мы просто берем и переписываем аккуратненько и 8% времени превращается в 10% времени. И можем после этого еще сравнить 2 профайлинга и, собственно, крутость Go в том, что на этом основана разработка Go как такового. То есть вот разработчики реально каждый раз приходят и говорят: «У нас раньше было http c такой скоростью работала, вот в этой версии с такой скоростью», прямо циферки показывают. И это каждый раз, это уникально из того, что я видел по языкам, это именно когда разработчики всерьёз заморочены и отчитываться по оптимизации. Тулинг, который есть, собственно, через онлайн pprof умеет всё, умеет спамить, умеет блокировки, умеет мьютекс, умеет стектрейсинг, умеет трейсинг прямо живой, то есть вы запустили и прямо на живую можете посмотреть, как это работало последние 5 секунд, и можете даже отслеживать, что вы из собственного треда запускаете, если вы по каким-то причинам запускаете много, это можно отловить. Есть магические вещи, например, есть build-race, это штука, которая актуальна, как только вы начинаете работать во много потоков, например, вы просто пытаетесь в одну и ту же переменную писать параллельно, или писать и считать, или ещё что-нибудь в таком духе. Build-race соберет приложение с поиском айдишенов, он будет работать в 4 раза медленнее, но при этом он вам отчитается, что вы работаете с блокировками не так, ну или просто работаете с памятью не так. Большего того, можно еще тестировать память, можно тестировать сколько локаций происходит и так далее, это полезно, когда вы начинаете работать с ГЦ. Но это все фигня. На самой деле самое интересное, это разработка. Крутость Go в том виде, в котором я с ним работал, это то, что мы апгрейдим Go, вот реально с 1.0 на 1.1, с 1.1 на 1.2, с 1.2 на 1.3 и так далее, с 1.11 на 1.12, и у вас он просто работает, то есть работает ваш код, работают все библиотеки, без единой адаптации, и это было у меня с 2012 года, то есть последние 7 лет. И вторая вещь, которая крутая, то есть она, конечно, спорная, потому что долгое время не было нормальной работы с зависимостями, но за счёт этого, ну то есть то, что мы делаем в Go и достается библиотека из мастера, прямо вот свежий мастер и он летит сразу всем пользователям. Это, конечно, понятный минус, но в связи с этим мэйнтенеры библиотек дисциплинированы, я не знаю, что если они что-то поменяют обратно не совместимо, но существенная доля пользователей будет сильно недовольна, причём очень сильно. Если первые годы это было неожиданно, то сейчас это стало привычкой, и если меняется, делается обратная несовместимость, это делают другой, просто другую ссылку на скачивание пакета, и оно так работает, оно так прижилось, и реально обратную совместимость не портят, за очень редким исключением. На моем пути, вот прям в живом соседнем проекте, взяли Python, обновили с 3.6 на 3.7, он не скомпилировался, сказал, что какие-то кьютовские библиотеки нормально не работают. Берём Python’овские либы и выясняем, что мажорная версия ломает обратную совместимость, причем ломает так, что это чинить неоправданно дорого. Возможно у вас другой опыт, но я это просто на живую видел в соседних проектах, меня это каждый раз расстраивало. В Go с зависимостям всё даже повеселее, чем в Python’е, потому что не принято зависеть от системных либ, принято, если вы что-то используете в системных либах, принято это писать самостоятельно. Я вижу скептические лица, но на самом деле это хорошая идея, потому что после этого оно переносимо, после этого если библиотека, которые там … , собственно, заявила, что она поддерживается на MAC, винде, Linux и еще на каком-то телефоне, то она там действительно поддерживается и не надо доставлять 5 либ и непонятных версий, причём, если эта версия случайно не 3.16 или 3.17, то всё развалится. Если она находиться по другому пути, все развалится, такой фигни просто нет. И в 1.12 появился Go Mod, именно как дефолтная штука работы с зависимостями, и всё стало здорово, то есть мы просто вызываем фактически Install, тут Go Install в данном случае, и он просто Go Mod файл заполняет сам прямо при вызове, собственно, Go Get или Go Get Update, прям сам, не спрашивая, то есть вам не нужно делать этот самый PipFreeze и прочее, не забыть сделать PipFreeze, вы вроде протестировали на одной версии, реально работает на другой, всей этой фигни нет, она просто хорошо работает. Больше того, зависимости byDefault не хранятся вместе с пакетом, то есть нет папочки virtual env, в которой лежит половина интернета, как в случае с какими-нибудь none modules. В Python этого, конечно, поменьше, но всё равно такие существенные папочки получаются, если много чего используете, еще в некоторых случаях Python складывается, ещё что-нибудь для надежности и прямо совсем хорошо получается, когда у вас 10 проектов. Здесь такой фигни нет, он просто все складывает в общий кэш общий на пользователе, и собственно, там версианирует, подписывает с ключиками и следит за тем, чтобы сохранялись, можно не бояться того, что … возьмет и перепишет что-нибудь, потому что это будет видно по контрольным суммам. И из прикольных вещей в Go библиотеки не пытаются что-то делать с рантентом, ГЦ паузы перенастраивать, так, чтобы ГЦ вызвался или, наоборот, ГЦ не вызывался, или пропатчить какие-то функции системные или ещё что-нибудь, этой фигни просто нет, то есть вы импортите либу, и она не влияет на то, на что не должна влиять.

Дальше, своя библиотека в Go. В чём фишка? Вы просто берете директорию, хоть в вашем проекте, просто копируете в соседнюю, там её инициализируете, заносите прямо в гитхаб и она сразу работает, больше того она заноситься в DocOrg и она будет документации к вашему проекту прямо из коробки. Больше того, если вы, например, патчите чужую библиотеку, у вас … приняли, и вы сразу можете пользоваться результатами, прям нативно, через тот самый Pip Install, вам смежили по request минуту назад, вы уже через Pip Install получаете пропатченную зависимость, без костылей. Больше того, если вам по каким-то причинам это не нравится, то есть мершат медленно или ещё что-то, вы просто берете и используете свой форк, что делается реально в одну команду, и это прикольно.

Раз уж мы заговорили про удобства, то прикольно в Go, это то, что есть документация, то есть вызываете Godoc http и у вас прям вся документация по Go и всем пакетам прямо в ноутбуке и работает в режиме самолета. Я реально в самолёте писал код, и это реально было прикольно, потому что я не всегда помню, как это реально работает. Система документации реально одна, то есть нет 10 способов, как это делать через какой-нибудь пайдок или ещё что-нибудь или финкс или три разных варианта, или ещё что-то. У нас GoDoc, он стандартный, он везде работает.

Теперь идём к самому интересному. Кто из вас слышал, что в Go нет Excepton’ов? Так вот это неправда, они там есть и называются паники, вот, есть прекрасный кейс от Никиты Соболева, о том, что просто берем функцию, которая не знаю, дергает интернет, спрашивает пользователя и пытается вернуть этого самого пользователя наружу. Как вы думаете, в скольких местах это может сломаться с Exception’ом? Ну типа того, действительно во многих. И очень хорошо будет Excepton’ы обработать, потому что они разные. Ну на самом деле их нужно наверх пробросить, потому что они с разными кодами и так далее и в идеале ещё и как-то перемолоть по-разному. На Go это просто делается выводом ошибок, т.е. у нас функции, которые могут вывести ошибку они последним параметром выводят ошибку, и мы можем их обработать, вот как здесь видно, просто берем http get, если вернулась ошибка, то возвращаем ошибку наверх, с какой-нибудь оберткой. Видно? Понятно? Ага. Следующий момент, это обработка статус квот, который внезапно не таймаут, т.е. нам сервер ответил корректно, просто 500, это меня каждый раз удивляет в http’шных либах, просто реально 500 ответ считается корректным, а 400 это точно корректный, корректнее некуда. Ну и собственно дальше, декодировать и обратно вывести. На самом деле это такой же говнокод как и предыдущий, только вместо неявного Excepton’а у нас появились явные ошибки, по которым мы хотя бы видим, что сломалось, в каком месте, и что он точно сломался в этом месте, а не в соседнем. Но здесь же можно внимательно подумать и обработать это, то есть, например, не вернуть ошибку и сказать, что знаете, если внешний серв лежит, то у нас нет пользователя. Почему нет? Кодировки. Это забавная штука, в Go это UTF-8, а в Python с 3.7 тоже сразу UTF-8, но на практике не все tools об этом догадываются. 

Митя меня ускоряет, поэтому перейдём к выводам. На самом деле все вот это вот фигня, реально если вы опытный бэкендер, вам без разницы на чём писать, на Python, Ruben, XSire, Go, Java, ещё на чём-то, если это не C++, то точно на чём угодно. Есть гораздо более важный показатель – зарплата. Да, мы берём статистику моего круга за вторую половину прошлого года, они говорят, что 75 процентиль зарплаты питониста 150 000, 90% процентиль – 185 000, Get It, у них есть бот в телеграме, они говорят, что senior питонист получает 175 – 200 тысяч. В принципе это соответствует тем цифрам, которые я знаю, но понятно, что есть отклонение вверх неопределённое, но в целом так. OverFlow, который тоже собирал статистику по 100 000 пользователей по всему миру, они говорят 98000 баксов за Python, с Go все куда веселее. Собственно, на мой круг дают 180 – 225 тысяч, Get It дает - 205 – 250, OverFlow 110 тысяч долларов в год, вот в целом инкамп + 20%. Просто то что вы пишете на Go и вот это важно.

Подытоживаю. Python лучше, если же вам на самом деле нужен быстрое приложение. То есть вы взяли накатили, хренак django up, вот это все, взять бустинг и если вас он устраивает, тот работает, в режиме на коленке собрали сложное приложение за недельку. Если вы делаете в Data science и вас устраивает то, что получается, тоже здорово. И самое важное, если вы тот прекрасный человек, которому действительно нравится, как устроена AsyncAwait, у вас есть Python mod и они вас очень ждут. Вот, если вы по каким-то причинам начинается упираться в скорость ответов, в latency ответов, в память, которую надо хранить или скажем вам нужно с диском как-то аккуратно работать. Внезапно выяснится, что Go существенно лучше, например, когда придётся как-то подпиливать то, что происходит или, когда выяснится, что она просто медленная и есть конечно кейс мозилы, которые говорят, что PayPay медленнее всего в 2-3 раза, но мы же понимаем, что он медленнее в 2-3 раза и по памяти всё так же плох. Это хорошо, когда вам важна простота operation, вы просто бросили бинарник в production и забыли про него, вы просто обработали все ошибки и поэтому вы хотя бы знаете какие ошибки есть, у вас приложение внезапно не падает в непонятном месте и вот этого всего. Вот, но и по последнему, конечно, если вы хотите уходить, то есть почему зарплаты в Go больше? потому что задача более сложная, потому что вы не формочки пилите, а вот с сетью работаете, там рекламу делаете, ещё что-нибудь такое там, не знаю, видео декодируйте. Вот, это более интересная задача и за них дают больше денег, и собственно в этом на самом деле вся разница, а не в том, что Go какой-то магический. Как-то так,

Вопросы? Давайте похлопаем.

Тема действительно предельно холиварная, при этом важная, потому что многие питонисты, как вы знаете уходят в Go или частично уходят. Давайте обсудим, у нас есть время на пару вопросов. Кто поднимал руки? Здравствуйте, спасибо за доклад, скажите пожалуйста, какой интерпретатор питона вы использовали в бенчмарках?

Си Python.

Спасибо.

На самом деле вот по первому бенчмарку, который http’ный, PayPay ипользовался, он не сильно лучше, но то есть там где-то хуже, где-то лучше, но вот самый первый бенчмарк, там неважно.

Вопрос относительно скорости разработки на Python и Go, потому что всё-таки Python считается более высокоуровневый и вот были какие-то исследование по данной теме?

Я специально искал такие исследования, я их не нашёл. Я могу поделиться своим опытом по этому вопросу. Внезапно скорость разработки, она ограничивается тем, с какой скоростью мы можем воспринимать текст, который написан, то есть вот скажем 90%, 80% времени мы читаем код, 10% пишем, ну или что-то патчим. И здесь более критично не количество букв, а сложность кода, то есть вот, если вы наворотили лямбду лямбду в лямбде, это читать тяжело и вот вы просто сидите и втыкаете в код 5 минут, чтобы понять, что в этих 5 строчках написано, ну у меня такой богатый опыт с клажурой был. Вот, код Go тупой как валенок, больше того его тяжело сделать по-разному и это замечательно, тут вот Python’овский формат должен быть один хороший путь сделать, он в Go действительно реализован и поскольку он реализован, вы с первого раза считываете паттерны, а потом просто читаете код с листа быстро, исходя из этого скорость разработки сравнимая. Я делаю Megafon TV, это вот прям магазин-магазин, и там продажи всяких фильмом, сериальчиков и прочего, как бы команда разработки и скорость плюс-минус такая же, что как бы с рельсами.

Холиварный вопрос, почему еще не все на Go?

Почему все на Go?

Ещё не все почему на Go?

Ну, люди консервативные, компании еще более консервативные, то есть вот, если взять рынок, который действительно конкурентный, т.е., например, OpenSource, то мы видим что все информационные утилиты пишут на Go, уже с года 2014, все эти Промитеи, Инфлюксы, Докеры, Кубернейтисы половина, и больше того все сервисы, которые начинает упираться в такие задачи, типа какого-нибудь не знаю, DropBox или CloudFleer, они просто берут и пишут на Go. То есть вот, есть OpenSource и в OpenSource информационные утилиты исключительно на Go пишут, даже базы данных как Rouch на Go пишу.

Спасибо за доклад, у меня такой вопрос, ты сказал….

У вас, по-моему, микрофон выключен, нет? Попробуйте…

Нет, включен. Ты сказал, что зарплаты самое важное, но если пойти на тот же Мой круг, окажется, что на Скала и на Элексире платят больше, это значит, что мне надо пойти туда, а не в Go?

Фишка в том, что именно в прошлом году Go стала более дорогим и в этом фишка.

Не к ночи был упомянут Элексир, у нас следующий докладчик как раз с этой темой.

Больше того, есть американский обзор по зарплатам, который говорит, что вот именно из скиллов, по языкам, Go самый дорогой. 

Можно?

Да, давайте последний вопрос.

У меня вопрос, смотри. Ты упомянул две вещи, которые мне кажется немножечко взаимоисключающими, ты сказал про читаемость кода, а как мы, ну я думаю, что вряд ли ты будешь с этим спорить, но любая обработка ошибок эксплицитная, она, как правило, замыливает глаз и ты глядя например на хороший написанный код на С, ты понимаешь что состоит примерно из 90% обработки ошибок потенциальных, а сама мекотка она размазана, потому что каждая строчка там была написана кровью и корками. При этом ты сказал о том, что, например, ты пишешь программу на Go? проверяешь в ней все ошибки, и видимо ты как раз, исходя из слайда это всё вручную, каждая ошибка явно эксплицитно проверяется и дальше пробрасывается. И при этом ты говоришь про чистоту и простоту написания кода, при этом когда мы говорим, про то, что мы пишем код на Python, да, ну мы пишем какой-то код, в котором мало обработки ошибок, как бы, чёрт с ним, дальше соберём стектрейс, соберём с утра в центре, посмотрим всё что было, отберем то, что критичное и с этим будем разбираться, при этом код остаётся гораздо меньше, потому что, ну и вот я не буду говорить, чем кода меньше, тем он читаем - это неверно, но чем меньше в нем явно ненужной специфной церемониальной обработки ошибок, тем, он тогда будет читаем. И как это совмещается? Эти вещи, про которые ты говорил.

Это хороший вопрос. На самом деле это моя личная боль. Ну прям, если бы была оценка лучшего вопроса, вот это он. В мякотку. Это больно, то есть эта ситуация сейчас решается, она в 1.13, сделают нормальную обертку для ошибок и там в 1.14 и 1.15 сделают возможность именно на уровне языка исключения, вообще есть несколько паттернов, которые позволяют избежать обработки ошибок, ну то есть заменить на паники, фактически аналогов Exception’ов.

Или нарост переписать.

Это решение не для каждого. Да нет, … просто сильно более низкоуровневый, поэтому не туда. Есть способы как это обработать, обратность обработки ошибок внутри функции, то есть сделать так, что функция в самом начале делают проверку в структуре, формат есть ошибка, нет ошибки, как бы, если есть, то даже не обрабатываются и за счет этого проглатывается существенная часть. 

Попытаюсь как-то подытожить, это сводится к сноровке, опять же, обращения с языком? Верно я тебя понял?

Ну, типа того, то есть, ну какой сноровке, просто приходит человек, он видит, как это написано, и пишет Си’шный код по аналогии. Вот, если код изначально был написан нормально, то, он пишет нормально, если он вдохновляется …, то код будет нормальный.

Хорошо. Так, ну, получается лучший вопрос, как ты сказал, мы вручаем Максиму. Давайте похлопаем.

![](https://habrastorage.org/webt/jj/6y/b7/jj6yb7hkh6ykmlsmmuj_fxo5jb4.png)

![](https://habrastorage.org/webt/wb/z_/g7/wbz_g7k8ursqnu1nzjwqu06xkc0.png)

![](https://habrastorage.org/webt/ah/rt/es/ahrtesbwrsazyvgqjtk-slh7pxc.png)

![](https://habrastorage.org/webt/j9/cq/40/j9cq40ufpt5gricwkea7vxg2l1i.png)

![](https://habrastorage.org/webt/so/vs/rd/sovsrdvh5hubuxkfzjs9e4bjaja.png)

![](https://habrastorage.org/webt/-x/dp/1-/-xdp1-6la0jwnmun9oqa2fxh80c.png)

![](https://habrastorage.org/webt/zd/zx/ab/zdzxab4c0xddy5qdoy5ocpnr8ci.png)

![](https://habrastorage.org/webt/br/jo/do/brjodozgarx54bmld8_fm8l5jvu.png)

![](https://habrastorage.org/webt/w1/ej/9i/w1ej9ip-vxf0dqauzccywy1oeea.png)

![](https://habrastorage.org/webt/6v/j5/b-/6vj5b-p4ph2_edt0tyekskgwf4k.png)

![](https://habrastorage.org/webt/-q/ll/jp/-qlljp7i1ojm2vt1zajlig27tqk.png)

![](https://habrastorage.org/webt/l2/ks/kj/l2kskjyrjqzkcqxgnh_egnzrrwq.png)

![](https://habrastorage.org/webt/ms/5k/jo/ms5kjoeb8wbp-l7cfsfjbpzkajw.png)

![](https://habrastorage.org/webt/t5/lt/py/t5ltpyoziej_er1woiay-uizemy.png)

![](https://habrastorage.org/webt/-r/1r/ps/-r1rps3agpkuuo_wdyee5slw0ma.png)

![](https://habrastorage.org/webt/a3/kv/ku/a3kvkuj1eo6jnhsojogvnpjevoc.png)

![](https://habrastorage.org/webt/wi/jt/md/wijtmdt397tcpmwyuqpnndxypfm.png)

![](https://habrastorage.org/webt/ls/jo/to/lsjotog-fryeoamx8uo76cdhgvo.png)

![](https://habrastorage.org/webt/fx/jm/ta/fxjmtavyrgqpe6ozfciqtw3dkxm.png)

![](https://habrastorage.org/webt/hm/84/wy/hm84wydhfesfwhqd0xh3kefh8by.png)

![](https://habrastorage.org/webt/vi/3o/lc/vi3olco0-w0fswml2usxke4y6zq.png)

![](https://habrastorage.org/webt/_n/md/b8/_nmdb8qezji1fqh4ufojdzpqsfg.png)

![](https://habrastorage.org/webt/e6/b4/25/e6b425ra3vvo-1sy4fejjksk8ea.png)

![](https://habrastorage.org/webt/3t/ry/cm/3trycmsib-mcex-capipxjk7oc4.png)

![](https://habrastorage.org/webt/ap/b-/qd/apb-qd-af1zjlcdqt2osracluou.png)

![](https://habrastorage.org/webt/ut/jq/l3/utjql3e7_c43er0mr9cy-rvqcme.png)

![](https://habrastorage.org/webt/ck/s9/2e/cks92elx4ceaugt5-mlrfx5g8vk.png)

![](https://habrastorage.org/webt/ia/ag/4t/iaag4t7qzdj3aunxqrxhvkurhoe.png)

![](https://habrastorage.org/webt/fm/zo/ff/fmzoffxgld-ukvhr19lh5o1_cty.png)

![](https://habrastorage.org/webt/30/yr/v4/30yrv4maqjycsnkxrfrhdwjtxsy.png)

![](https://habrastorage.org/webt/a0/vb/uy/a0vbuysdum17zxuev1s8co1clcy.png)

![](https://habrastorage.org/webt/cv/jw/3e/cvjw3elt-jfjwutzkn9jgvdgkke.png)