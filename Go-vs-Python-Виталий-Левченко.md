Всем привет. Я пока всё равно вступаю. Во-первых, спасибо, что дождались, я прям редкий форс-мажор, водитель такси, к которому я сел, сперва мне сказал о том, что у меня что-то не так с какой-то чакрой и что-то где-то светится, а потом сказал, что он лучше знает дорогу. В общем, поехал он в другую сторону, это оказалось немножко не вовремя. В общем внимательно следите за водителем, возможно они под грибами. В любом случае я вам благодарен, что вы тут собрались, набили полный зал практически и даже не разбежались за 10 минут, вы молодцы. Я буду рассказывать про Go, потому что я на Go пишу примерно с 2012 года, а на Python я уже даже перестал писать. Я думаю, что я акцентирую внимание на том, что не очевидно. То есть я полагаю, что… наверное, важный вопрос, господа, а кто из вас знаком с тем, что такое Go, видел Go 'шный код и примерно представляет, чем он от Python’а отличается? Слушайте, неполный зал. Окей, а кто не представляет, чем они отличаются? Оу, слушайте, десятая часть зала не представляет. Неожиданно, ладно, тогда будет проще. В любом случае моя риторика по поводу Go обычно довольно простая, что-то в духе «бросайте свой язык программирования и пишите на Go». Тут я надеюсь выжить до конца конференции, поэтому буду чуть помягче. 

Собственно, почему я это всё делаю? Потому что я продвигаю Go с 2012 года, делаю Go MeetUp’ы с 2014, как Митя говорил, делаю кучу сервисов, достаточно большие нагрузки, и в целом какой-то там менеджер, делаю что-то такое полезное в компании, правда которую не могу называть. То, с чего вообще начинаю сравнение языков – это бенчмарки. Скажите, кто считает, что не с бенчмарков надо начинать сравнение языков? А кто считает, что с бенчмарков? А кто считает, что всё равно? Ага, а кто считает, что с бенчмарков? Ага, а кто считает, что все равно? Ага, а всем остальным просто уже всё равно стало. Окей. С бенчмарков вообще принято сравнивать, если начинать холивар, то говорят: «Вот у нас есть мега бенчмарк, дергаем http, собственно, одно быстрее, другое медленнее». И вот так устроена половина холиваров в интернете уж точно. Мы не будем исключением, возьмём http, на самом деле возьмем просто бенчмарков, который сделали прекрасные люди из StationPowers. У них http и довольно простенький формат в духе веб- приложения, достаточно текстов из базы данных, отрендерить и выдать по http, очень-очень хитро. Они тестируют на довольно неплохом железе, на 14 ядрах, с гипертрейдингом и все такое, и дают следующие циферки. Собственно, 329 000 запросов в секунду переживает Go со специальным модулем для http, без специального модуля на нативном http он переживает 180 000 запросов в секунду, что как бы дофига, то есть это больше, чем перемалывают сетевые карты обычные. С Python все гораздо грустнее, примерно так, да. 65 – 70 тысяч запросов это специализированные фреймворки, для того, что отвечать быстро. А если берем Jango, всё ещё грустнее. Мы видим разницу больше, чем на порядок, то есть в 5 раз разница идёт со специализированным фреймворков, в 5-6 раз идёт разница с http, и Jango гораздо медленнее, ну и все остальные Twisted и прочее. В этот момент, на самом деле, можно уже уходить и переставать делать доклад, потому что и так всё понятно, но мы продолжим. На самом деле имеет смысл сравнить latency, потому что это часто более важно. Наверное, вот какой вопрос: господа, а кому из вас вообще важно, с какой скоростью отвечает ваше приложение? То есть это является значимым? Что, всем остальным неважно? Реально? Ну правда? То есть если ваше приложение отвечает пользователю через 2 секунды, это для вас не проблема? Ну ладно. На самом деле я просто работал в рекламе, я работал, собственно, в стриминге, и там было довольно важно, что больше 100 миллисекунд пользователи уже начинают жаловаться, что подтормаживает, я не привык к каким-то другим вещам. Фишка в том, что Python оказывается вполне с адекватными цифрами с latency, т.е. 7 миллисекунд на ответ на технагрузку это прямо хорошо. Но больше всего меня удивил Jango. Как вы думаете, на фоне этих циферок, какой latency у Jango? Кто ставит, что больше 50 миллисекунд? Окей, а кто ставит, что больше 20? А больше 10? Ага, а меньше, чем у Go? Я серьёзно, оно меньше, чем у Go получилось. Если возьмёте в руки калькулятор и просто разделите одну цифру на другую, то есть вот эти 14 запросов в секунду, 28 ядер и 2 миллисекунды, оно поделится ровно, то есть это ровно-ровно, Jango отвечает без накладных расходов, это внезапно. Я к тому, что на самом деле все не так плохо, как показалось с самого начала. Следующий момент, который сравнивается в бенчмарках, те люди, которые более-менее прошаренные, как работают приложения, это локация. Собственно, в чём суть? Нам периодически из кэша надо доставать кучу данных, не знаю, редисы, стучимся в него, в базу данных стучимся, достаем кучу данных. Скажем, мы достаем 10 000 объектов по 100 полей, такие обычные ситуации, то есть, скажем, 10 раз по 1000 объектов достали и как-то их пережевали. В JSon’е это какие-то смешные 20 мегабайт, а то и 10 мегабайт, какие-то копейки. Как вы думаете, сколько времени JSon Load будет работать на этот самый миллион объектов? Кто ставит, что порядка 10 миллисекунд? Окей, а порядка 100 миллисекунд? Ага, а порядка 200 миллисекунд? Ну вот вы правы. Собственно, мы просто достаем из редиса данные, то есть редис их выплюнул за долю миллисекунды, сеть их приняла за долю миллисекунды, а после этого Python их переживал ещё 200 миллисекунд, осознайте эту дельту в цифрах. Больше того, после этого обработать эти данные будет стоить 10 миллисекунд в худшем случае, в лучшем даже меньше. Локация – это реально дорого. Но фишка в том, что Go не сильно быстрее, но в Go есть другая штука, называется «кэш в памяти», мы можем просто этот кэш засунуть в память, не дергать его из базы, не дергать его из редиса, а просто брать и использовать. Но мы можем попробовать его взять и использовать в Python’е, взять multiprocessing и засунуть его, и дальше увидеть то, что обновление этого объекта в памяти, это 2 секунды, в течении которых приложение не может доставать из кэша ничего, и мы радуемся. То есть каждое обновление элемента кэша — это фриз приложения на 2 секунды. Офигительно, правда? То есть это даже не 200 миллисекунд, это 2 секунды, я не ожидал этих цифр. И вот это вот, это не в тупую запись, это просто перезапись объекта, то есть дикт-апдейт или что-то в таком духе. Поэтому мы можем память не шарить, а просто держать ее в памяти локального приложения. Но дальше мы получаем очень грустную историю, если нам объектов нужно хранить много, скажем, 10 Гб в целом приложении, то мы получаем грустную историю, нам на 28 ядер нужно 280 Гб памяти, в общем печаль и катастрофа. И это нерешабельная проблема в Python’е. То есть если вам по каким-то причинам нужно перемалывать хоть сколько-то значимое количество объектов, то Python просто не про вас. И насколько я понимаю, это в 3.8 может решиться, но там надо мерить по тестам. И еще один важный момент: если вы всё-таки делаете 28 приложений, то есть вот сейчас вы запустили multiprocessing enginiring cornel и крутите. Вы, скажем, хотите сделать общие счётчики, то есть, не знаю, ярики не считать, еще что-нибудь такое, просто какие-то счётчики на приложения. Выясняется, что в редисах сохранять будет стоить 0,2 миллисекунды на каждый запрос, причем в живую ЦПУ. В случае Manager Value плюс-минус такие же цифры, т.е. если мы в общую память сохраняем. Плагин промитея… Кстати, кто из вас слышал про Monitoring Prometheus? Остальные не слышали? Ок. в общем, это гораздо более важная штука, чем Go, бросайте то, чем вы мониторите и используйте Prometheus. Короче, его модуль для счётчиков используют Map и межет на живую. Я не до конца понимаю, почему это работает и не падает по дороге. То есть он просто берёт на каждый процесс map файл и после этого без всяких мьюдексов просто берет и межет. Это выглядит страшно, но по виду работает. 

Давайте вернемся к тому, ради чего мы вообще собрались. В Python’е год назад появилась нормальная асинхронность AsyncAwait. Вот тут мне реально интересно, а кто из вас в production уже использует 3.7 и AsyncAwait? O, даже Митя поднял и с ним четверть зала. Окей, а остальные не используют? Что? 3.6? Окей, а у кого 3.6 или меньше в production? Ну ползала практически. Ну четверть, ладно. Окей, а у остальных 3.7, или у остальных не Python? Ладно, ну короче примерно поровну поделилась. Логика вот какая. Я веду мысль к тому, что есть кейс, с которого я начинал знакомство с Go. Он выглядел примерно следующим образом, у нас были просто очереди в TopFace, они перемалывают какие-то лайки, запросы, сообщения, обработку подарочков и всякой прочей дряни, технически это просто очереди, которые перемалывают демоны, они упираются в кучу запросов в базу, в какую-то арифметику на этих данных и ЦПУ. Представим, что у нас на машинке те самые 14 ядер. Как вы думаете, сколько воркеров надо запустить, чтобы эффективно утилизировать ЦПУ? Кто думает, что 14? Ок, а кто думает, что 50? А кто думает, что 100? Ок, а кто думает, что хрен его знает? Подавляющее большинство. Отлично. На самом деле, действительно, хрен его знает, реальная цифра порядка 100-150-200, в зависимости от того, насколько быстро базы отвечают, и это в случае, если мы не используем нормальные корутины и асинхронности. Больше того, если у нас база начинает отвечать медленнее, у нас ЦПУ не догружена, если база отвечает быстрее, у нас процесс оказывается перегружен на 200 потоках у нас 200 и машинка перестает отвечать хоть как-то, радуемся. Поэтому нужны корутины, поэтому нужно эффективное использование каждого ядра в таком формате. Наверное, стоит спросить, а кто из вас не понял, что я имею в виду под корутинами, и почему это важно в этом контексте? Окей, всем понятно, почему, собственно… Ну ладно, попробую объяснить. В чём логика? Мы имеем ситуацию, что у нас… мы можем запустить 200 потоков, которые просто что-то делают, а можем запустить 100 потоков, а можем 14, и по-простому что они делают во время запроса в базу? Если у нас корутины, во время запроса в базу, она обрабатывает другие потоки, если у нас не корутины, то просто трейд заблочен и все. Поэтому в случае корутин мы хоть как-то это утилизируем. Этим Go крут, потому что просто запускаем приложение, оно просто скушает все ЦПУ и не мяукает – замечательно. Поэтому переходите на 3.7, если у вас хоть какой-то ЦПУ запросов. 

Пойдем дальше. В принципе по асинхронности самое важное, что есть в Go, это обработка в scheduler, она сделана хитро, то есть просто сисколлы при каждом вызове отдают, собственно, корутину в scheduler с памяткой, что она не занимается ничем полезным, и собственно, возвращает из scheduler, когда сисколл заканчивается. То есть получается прекрасная. У нас в контексте 14 ядер, будет запущено ровно 14 корутин, которые не в сисколлах, ну или меньше, если ядер физически меньше. Это работает, причем работает магический круто, т.е. вы просто запихнули какие-то корутины, они что-то делают, они включаются в базу, на диск, еще что-то, ЦПУ утилизруется нормально, и ничего не блокируется, потому что все блокировки, они в первом пункте. С мьютексами аналогично, но поскольку они в нативной библиотеке, стандартной, все проработано и точно так же работает. На самом деле в Asyncio всё похоже, то есть точно также, если идёт запрос в сеть, он просто передается в Scheduler, который разруливает ее с какими-нибудь какхиу, еполом и прочей дрянью. Это красиво работает, это относительно синхронно на текущий момент выглядит, не так, как у Go с AsyncAwait это хоть как-то хорошо. И замечательно, беда в том, что не все функции блокирующие, не блокирующие. Это приводит к грустной истории. Вот мы описали частный поток AsyncAwait, радуемся, а потом внутри случайно вызвали функцию, которая что-то блокирует, и чем больше приложений, тем больше вероятность, что мы такую функцию вызовем, и собственно, у нас весь ANTN встал целиком, пока функция не разблокируется. Поэтому нужно использовать внешние библиотеки, которые Asyncio точно поддерживает. Мы открываем aio-libs, которые пишут целых 11 баз данных, это на самом деле немало. У них, правда, есть всякие интересные проблемы в духе оно падает на каких-то хитрых ответах или при попытке сделать Close, или еще на чем-нибудь таком, но оно делает вид, что работает. Там нет важных для production баз типа Aerospike, и на самом деле там не хватает ряда полезных таймаутов, это проблема. Если у вас есть какой-то таймаут базы, который вы не можете поставить, этот таймаут сработает, и у вас приложение, конечно не сдохнет, у вас корутина сдохнет на время таймаута, и вам нужно либо снаружи обрабатывать каким-то странным образом, wait for поставить или ещё что-то, либо страдать. Но интересно другое, интересно то, что в Python нет нормального способа коммуникации между тредами, а в Go это есть из коробки, и вот за это реально надо любить Go. То есть в данном случае мы можем честно взять и уведомить все корутины, в которых происходят какая-то обработка и сказать им: «Надо закончиться», например, мы делаем ShutDown, то есть мы шлем сигнал, все аккуратненько заканчивается и после этого мы останавливаем приложение. В случае Python нужны либо поддержки соответствующих обработчиков, то есть поддержка внутри http, внутри net’а и ещё чего-то, shield’а который нормально не работает, ещё чего-нибудь в таком духе, и всё равно это неудобно, вы всё равно пробрасываете огромное количество каких-то вещей, и у вас нет гарантий, что он завершится, а здесь прям простенько, из коробки, вы просто делаете канал, в который вы шлете сигнал о том, что приложение завершается и обработчик заканчивается. Вот как-то так. Больше того, select еще крут тем, что мы хотим читать сразу из 5 разных потоков. вот у нас есть 5 разных продюсеров, они пикируют 5 разных видов данных, их надо обрабатывать по-разному, мы их можем просто взять и обрабатывать всех по-разному. В случае Python нам их нужно объединять в единый кью, то есть сперва объединить с потерей типов, потом обратно разъединить с нахождением типов, потом узнать, что нужно читать в двух разных местах и всё сломается. В Go такой фигни нет, и это круто.

Теперь перейдем к самой, наверное, холиворной вещи в контексте Python и Go. Люди говорят: «Я пишу в DataScience, я всякие модельки рассчитываю, умножаю их и так далее». Кто, кстати, из вас DataScience занимается или Машерингом? Оу, неплохо. На самом деле DataScience, это здорово, и за него хорошо платят, поэтому заниматься DataScience, это здорово. Но в Go на самом деле все не так уж и плохо, то есть более-меня все популярные библиотеки, всякие Boosting, деревья, вот это всё, оно сделано и сделано нормально. Больше того, если вы в какой-то момент начинаете упираться в ваши библиотеки, например, у вас обучение работает слишком медленно или оно вам нужно в других параметрах, или вам нужно онлайн-обучение, сразу выясняется, что ничего готового нет, надо делать самим. И в этот момент выясняется, что то, что вы сделаете, может получиться в разы быстрее, чем в Python'овской либе, причём вы не сильно напрягались, ну или напряглись, а получилось сильно быстрее. То есть вот есть пример, есть в SiKit, расчёт рекомендаций, т.е. это штука, то, что называется коллаборативной фильтрацией, когда у нас есть пользователь, который, не знаю, фильмы лайкает, мы хотим найти, собственно, похожих пользователей, и собственно, исходя из этого оценить для пользователя, как он оценит текущий фильм. Есть SiKit, у нас есть библиотека там, которая SVD умеет. Там есть 2 алгоритма, SVD, который обсчитывает модельку 2 минуты и SVD++, который обсчитывает эту модельку, конкретную, за 3 часа. Как вы думаете, за сколько Go считает SVD++? Кто ставит, что час? Окей, 2 минуты? Ну ладно, похоже вы не знаете, но на самом деле правда 2 минуты. Эта цифра забавная, то есть логика в том, что если вы берете библиотеку, которая хорошая, enterprise’ная и т.д., она много где используется, вы можете в ряде случаев увеличить производительность, грубо говоря, на два порядка, и больше того, даже если вам это не так нужно, то есть просто вам хочется улучшить текущую производительность там, где нормально оптимизировано, вы можете просто взять как-то аккуратненько дооптимизировать, скажем, UX взять и получить прирост ещё на 20%, как видите, для обычного SVD. Это прям стоит того, чтобы использовать. У меня есть знакомая, которая, собственно, использует Go именно для конкретного обучения, просто взяли и выкинули из парка, переписали нужную генерацию на Go, это стоило им в разработке что-то типа пары недель, что вполне адекватно на мой вкус.

И тут мы подходим к тому, ради чего, в принципе, с Go хорошо делать, когда мы говорим про производительность, это всяческая оптимизация. В чём фишка? Фишка в том, что мы берём Go, пишем как-то, ну реально как-то, как-то вот мы нафигачили проектики, функции повызывали, он работает быстро, если мы хотим, чтобы оно стало работать еще быстрее, мы просто вызываем профилирование, и оно начинает работать в 2 раза быстрее или в 10 раз быстрее, если мы что-то не так сделали. Фишка в том, что профайлинг делается в Go в одну строчку, просто импортить pprof, и он выкидывает наружу по http, если вы анонсировали следующий заголовок, эту ссылочку можно открывать прямо в браузере или, скажем, открывает через pprof утилитку нативную. И она делает магию, она показывает, что в конкретных либах, в том числе системных, в том числе какие-то сисколлы тормозят, то есть она говорит, скажем: «Тормозит Garbage Collector» или «Тормозит конкретный мьютекс», или «Тормозит конкретная ваша логика», и это офигенно круто. Прямо из этого pprof’а можно дернуть листфанг, и он прямо ещё построчно покажет, на каких строчках она сколько времени проводит, дальше мы можем вызвать disasm этой функции и ещё увидеть, на каких строчках оно так себя ведет. Это выглядит примерно вот так, на самом деле оно, это общий профайлинг это плюс-минус одинаково для всех языков, с которыми я работал, но это просто визуализация времени вызова. Есть FrameGraf, который внезапно в Python появился раньше, и вот то, что я говорил, можно построчно показать, как это по миллисекундам работает. И, собственно, то, что здорово, это то, что мы можем взять этот самый gcflag-S и, собственно, получить исполняемый ассемблерный код прямо на живую, или можем поставить gcflag-m и прям… ну это при компиляции. После этого нам с pprof будет выдавать информацию по Inline функциям локации на хипе и очень прекрасные вещи. Мы можем взять и переписать функцию на C Ассемблере и после этого просто понимая, что происходит, вот мы видим какую-нибудь функцию, которая работает 8% времени, мы просто берем и переписываем аккуратненько и 8% времени превращается в 10% времени. И можем после этого еще сравнить 2 профайлинга и, собственно, крутость Go в том, что на этом основана разработка Go как такового. То есть вот разработчики реально каждый раз приходят и говорят: «У нас раньше было http c такой скоростью работала, вот в этой версии с такой скоростью», прямо циферки показывают. И это каждый раз, это уникально из того, что я видел по языкам, это именно когда разработчики всерьёз заморочены и отчитываться по оптимизации. Тулинг, который есть, собственно, через онлайн pprof умеет всё, умеет спамить, умеет блокировки, умеет мьютекс, умеет стектрейсинг, умеет трейсинг прямо живой, то есть вы запустили и прямо на живую можете посмотреть, как это работало последние 5 секунд, и можете даже отслеживать, что вы из собственного треда запускаете, если вы по каким-то причинам запускаете много, это можно отловить. Есть магические вещи, например, есть build-race, это штука, которая актуальна, как только вы начинаете работать во много потоков, например, вы просто пытаетесь в одну и ту же переменную писать параллельно, или писать и считать, или ещё что-нибудь в таком духе. Build-race соберет приложение с поиском айдишенов, он будет работать в 4 раза медленнее, но при этом он вам отчитается, что вы работаете с блокировками не так, ну или просто работаете с памятью не так. Большего того, можно еще тестировать память, можно тестировать сколько локаций происходит и так далее, это полезно, когда вы начинаете работать с ГЦ. Но это все фигня. На самой деле самое интересное, это разработка. Крутость Go в том виде, в котором я с ним работал, это то, что мы апгрейдим Go, вот реально с 1.0 на 1.1, с 1.1 на 1.2, с 1.2 на 1.3 и так далее, с 1.11 на 1.12, и у вас он просто работает, то есть работает ваш код, работают все библиотеки, без единой адаптации, и это было у меня с 2012 года, то есть последние 7 лет. И вторая вещь, которая крутая, то есть она, конечно, спорная, потому что долгое время не было нормальной работы с зависимостями, но за счёт этого, ну то есть то, что мы делаем в Go и достается библиотека из мастера, прямо вот свежий мастер и он летит сразу всем пользователям. Это, конечно, понятный минус, но в связи с этим мэйнтенеры библиотек дисциплинированы, я не знаю, что если они что-то поменяют обратно не совместимо, но существенная доля пользователей будет сильно недовольна, причём очень сильно. Если первые годы это было неожиданно, то сейчас это стало привычкой, и если меняется, делается обратная несовместимость, это делают другой, просто другую ссылку на скачивание пакета, и оно так работает, оно так прижилось, и реально обратную совместимость не портят, за очень редким исключением. На моем пути, вот прям в живом соседнем проекте, взяли Python, обновили с 3.6 на 3.7, он не скомпилировался, сказал, что какие-то кьютовские библиотеки нормально не работают. Берём Python’овские либы и выясняем, что мажорная версия ломает обратную совместимость, причем ломает так, что это чинить неоправданно дорого. Возможно у вас другой опыт, но я это просто на живую видел в соседних проектах, меня это каждый раз расстраивало. В Go с зависимостям всё даже повеселее, чем в Python’е, потому что не принято зависеть от системных либ, принято, если вы что-то используете в системных либах, принято это писать самостоятельно. Я вижу скептические лица, но на самом деле это хорошая идея, потому что после этого оно переносимо, после этого если библиотека, которые там … , собственно, заявила, что она поддерживается на MAC, винде, Linux и еще на каком-то телефоне, то она там действительно поддерживается и не надо доставлять 5 либ и непонятных версий, причём, если эта версия случайно не 3.16 или 3.17, то всё развалится. Если она находиться по другому пути, все развалится, такой фигни просто нет. И в 1.12 появился Go Mod, именно как дефолтная штука работы с зависимостями, и всё стало здорово, то есть мы просто вызываем фактически Install, тут Go Install в данном случае, и он просто Go Mod файл заполняет сам прямо при вызове, собственно, Go Get или Go Get Update, прям сам, не спрашивая, то есть вам не нужно делать этот самый PipFreeze и прочее, не забыть сделать PipFreeze, вы вроде протестировали на одной версии, реально работает на другой, всей этой фигни нет, она просто хорошо работает. Больше того, зависимости byDefault не хранятся вместе с пакетом, то есть нет папочки virtual env, в которой лежит половина интернета, как в случае с какими-нибудь none modules. В Python этого, конечно, поменьше, но всё равно такие существенные папочки получаются, если много чего используете, еще в некоторых случаях Python складывается, ещё что-нибудь для надежности и прямо совсем хорошо получается, когда у вас 10 проектов. Здесь такой фигни нет, он просто все складывает в общий кэш общий на пользователе, и собственно, там версианирует, подписывает с ключиками и следит за тем, чтобы сохранялись, можно не бояться того, что … возьмет и перепишет что-нибудь, потому что это будет видно по контрольным суммам. И из прикольных вещей в Go библиотеки не пытаются что-то делать с рантентом, ГЦ паузы перенастраивать, так, чтобы ГЦ вызвался или, наоборот, ГЦ не вызывался, или пропатчить какие-то функции системные или ещё что-нибудь, этой фигни просто нет, то есть вы импортите либу, и она не влияет на то, на что не должна влиять.

Дальше, своя библиотека в Go. В чём фишка? Вы просто берете директорию, хоть в вашем проекте, просто копируете в соседнюю, там её инициализируете, заносите прямо в гитхаб и она сразу работает, больше того она заноситься в DocOrg и она будет документации к вашему проекту прямо из коробки. Больше того, если вы, например, патчите чужую библиотеку, у вас … приняли, и вы сразу можете пользоваться результатами, прям нативно, через тот самый Pip Install, вам смежили по request минуту назад, вы уже через Pip Install получаете пропатченную зависимость, без костылей. Больше того, если вам по каким-то причинам это не нравится, то есть мершат медленно или ещё что-то, вы просто берете и используете свой форк, что делается реально в одну команду, и это прикольно.

Раз уж мы заговорили про удобства, то прикольно в Go, это то, что есть документация, то есть вызываете Godoc http и у вас прям вся документация по Go и всем пакетам прямо в ноутбуке и работает в режиме самолета. Я реально в самолёте писал код, и это реально было прикольно, потому что я не всегда помню, как это реально работает. Система документации реально одна, то есть нет 10 способов, как это делать через какой-нибудь пайдок или ещё что-нибудь или финкс или три разных варианта, или ещё что-то. У нас GoDoc, он стандартный, он везде работает.

Теперь идём к самому интересному. Кто из вас слышал, что в Go нет Excepton’ов? Так вот это неправда, они там есть и называются паники, вот, есть прекрасный кейс от Никиты Соболева, о том, что просто берем функцию, которая не знаю, дергает интернет, спрашивает пользователя и пытается вернуть этого самого пользователя наружу. Как вы думаете, в скольких местах это может сломаться с Exception’ом? Ну типа того, действительно во многих. И очень хорошо будет Excepton’ы обработать, потому что они разные. Ну на самом деле их нужно наверх пробросить, потому что они с разными кодами и так далее и в идеале ещё и как-то перемолоть по-разному. На Go это просто делается выводом ошибок, т.е. у нас функции, которые могут вывести ошибку они последним параметром выводят ошибку, и мы можем их обработать, вот как здесь видно, просто берем http get, если вернулась ошибка, то возвращаем ошибку наверх, с какой-нибудь оберткой. Видно? Понятно? Ага. Следующий момент, это обработка статус квот, который внезапно не таймаут, т.е. нам сервер ответил корректно, просто 500, это меня каждый раз удивляет в http’шных либах, просто реально 500 ответ считается корректным, а 400 это точно корректный, корректнее некуда. Ну и собственно дальше, декодировать и обратно вывести. На самом деле это такой же говнокод как и предыдущий, только вместо неявного Excepton’а у нас появились явные ошибки, по которым мы хотя бы видим, что сломалось, в каком месте, и что он точно сломался в этом месте, а не в соседнем. Но здесь же можно внимательно подумать и обработать это, то есть, например, не вернуть ошибку и сказать, что знаете, если внешний серв лежит, то у нас нет пользователя. Почему нет? Кодировки. Это забавная штука, в Go это UTF-8, а в Python с 3.7 тоже сразу UTF-8, но на практике не все tools об этом догадываются. 

Митя меня ускоряет, поэтому перейдём к выводам. На самом деле все вот это вот фигня, реально если вы опытный бэкендер, вам без разницы на чём писать, на Python, Ruben, XSire, Go, Java, ещё на чём-то, если это не C++, то точно на чём угодно. Есть гораздо более важный показатель – зарплата. Да, мы берём статистику моего круга за вторую половину прошлого года, они говорят, что 75 процентиль зарплаты питониста 150 000, 90% процентиль – 185 000, Get It, у них есть бот в телеграме, они говорят, что senior питонист получает 175 – 200 тысяч. В принципе это соответствует тем цифрам, которые я знаю, но понятно, что есть отклонение вверх неопределённое, но в целом так. OverFlow, который тоже собирал статистику по 100 000 пользователей по всему миру, они говорят 98000 баксов за Python, с Go все куда веселее. Собственно, на мой круг дают 180 – 225 тысяч, Get It дает - 205 – 250, OverFlow 110 тысяч долларов в год, вот в целом инкамп + 20%. Просто то что вы пишете на Go и вот это важно.

Подытоживаю. Python лучше, если же вам на самом деле нужен быстрое приложение. То есть вы взяли накатили, хренак Jango up, вот это все, взять бустинг и если вас он устраивает, тот работает, в режиме на коленке собрали сложное приложение за недельку. Если вы делаете в Data science и вас устраивает то, что получается, тоже здорово. И самое важное, если вы тот прекрасный человек, которому действительно нравится, как устроена AsyncAwait, у вас есть Python mod и они вас очень ждут. Вот, если вы по каким-то причинам начинается упираться в скорость ответов, в latency ответов, в память, которую надо хранить или скажем вам нужно с диском как-то аккуратно работать. Внезапно выяснится, что Go существенно лучше, например, когда придётся как-то подпиливать то, что происходит или, когда выяснится, что она просто медленная и есть конечно кейс мозилы, которые говорят, что PayPay медленнее всего в 2-3 раза, но мы же понимаем, что он медленнее в 2-3 раза и по памяти всё так же плох. Это хорошо, когда вам важна простота operation, вы просто бросили бинарник в production и забыли про него, вы просто обработали все ошибки и поэтому вы хотя бы знаете какие ошибки есть, у вас приложение внезапно не падает в непонятном месте и вот этого всего. Вот, но и по последнему, конечно, если вы хотите уходить, то есть почему зарплаты в Go больше? потому что задача более сложная, потому что вы не формочки пилите, а вот с сетью работаете, там рекламу делаете, ещё что-нибудь такое там, не знаю, видео декодируйте. Вот, это более интересная задача и за них дают больше денег, и собственно в этом на самом деле вся разница, а не в том, что Go какой-то магический. Как-то так,

Вопросы? Давайте похлопаем.

Тема действительно предельно холиварная, при этом важная, потому что многие питонисты, как вы знаете уходят в Go или частично уходят. Давайте обсудим, у нас есть время на пару вопросов. Кто поднимал руки? Здравствуйте, спасибо за доклад, скажите пожалуйста, какой интерпретатор питона вы использовали в бенчмарках?

Си Python.

Спасибо.

На самом деле вот по первому бенчмарку, который http’ный, PayPay ипользовался, он не сильно лучше, но то есть там где-то хуже, где-то лучше, но вот самый первый бенчмарк, там неважно.

Вопрос относительно скорости разработки на Python и Go, потому что всё-таки Python считается более высокоуровневый и вот были какие-то исследование по данной теме?

Я специально искал такие исследования, я их не нашёл. Я могу поделиться своим опытом по этому вопросу. Внезапно скорость разработки, она ограничивается тем, с какой скоростью мы можем воспринимать текст, который написан, то есть вот скажем 90%, 80% времени мы читаем код, 10% пишем, ну или что-то патчим. И здесь более критично не количество букв, а сложность кода, то есть вот, если вы наворотили лямбду лямбду в лямбде, это читать тяжело и вот вы просто сидите и втыкаете в код 5 минут, чтобы понять, что в этих 5 строчках написано, ну у меня такой богатый опыт с клажурой был. Вот, код Go тупой как валенок, больше того его тяжело сделать по-разному и это замечательно, тут вот Python’овский формат должен быть один хороший путь сделать, он в Go действительно реализован и поскольку он реализован, вы с первого раза считываете паттерны, а потом просто читаете код с листа быстро, исходя из этого скорость разработки сравнимая. Я делаю Megafon TV, это вот прям магазин-магазин, и там продажи всяких фильмом, сериальчиков и прочего, как бы команда разработки и скорость плюс-минус такая же, что как бы с рельсами.

Холиварный вопрос, почему еще не все на Go?

Почему все на Go?

Ещё не все почему на Go?

Ну, люди консервативные, компании еще более консервативные, то есть вот, если взять рынок, который действительно конкурентный, т.е., например, OpenSource, то мы видим что все информационные утилиты пишут на Go, уже с года 2014, все эти Промитеи, Инфлюксы, Докеры, Кубернейтисы половина, и больше того все сервисы, которые начинает упираться в такие задачи, типа какого-нибудь не знаю, DropBox или CloudFleer, они просто берут и пишут на Go. То есть вот, есть OpenSource и в OpenSource информационные утилиты исключительно на Go пишут, даже базы данных как Rouch на Go пишу.

Спасибо за доклад, у меня такой вопрос, ты сказал….

У вас, по-моему, микрофон выключен, нет? Попробуйте…

Нет, включен. Ты сказал, что зарплаты самое важное, но если пойти на тот же Мой круг, окажется, что на Скала и на Элексире платят больше, это значит, что мне надо пойти туда, а не в Go?

Фишка в том, что именно в прошлом году Go стала более дорогим и в этом фишка.

Не к ночи был упомянут Элексир, у нас следующий докладчик как раз с этой темой.

Больше того, есть американский обзор по зарплатам, который говорит, что вот именно из скиллов, по языкам, Go самый дорогой. 

Можно?

Да, давайте последний вопрос.

У меня вопрос, смотри. Ты упомянул две вещи, которые мне кажется немножечко взаимоисключающими, ты сказал про читаемость кода, а как мы, ну я думаю, что вряд ли ты будешь с этим спорить, но любая обработка ошибок эксплицитная, она, как правило, замыливает глаз и ты глядя например на хороший написанный код на С, ты понимаешь что состоит примерно из 90% обработки ошибок потенциальных, а сама мекотка она размазана, потому что каждая строчка там была написана кровью и корками. При этом ты сказал о том, что, например, ты пишешь программу на Go? проверяешь в ней все ошибки, и видимо ты как раз, исходя из слайда это всё вручную, каждая ошибка явно эксплицитно проверяется и дальше пробрасывается. И при этом ты говоришь про чистоту и простоту написания кода, при этом когда мы говорим, про то, что мы пишем код на Python, да, ну мы пишем какой-то код, в котором мало обработки ошибок, как бы, чёрт с ним, дальше соберём стектрейс, соберём с утра в центре, посмотрим всё что было, отберем то, что критичное и с этим будем разбираться, при этом код остаётся гораздо меньше, потому что, ну и вот я не буду говорить, чем кода меньше, тем он читаем - это неверно, но чем меньше в нем явно ненужной специфной церемониальной обработки ошибок, тем, он тогда будет читаем. И как это совмещается? Эти вещи, про которые ты говорил.

Это хороший вопрос. На самом деле это моя личная боль. Ну прям, если бы была оценка лучшего вопроса, вот это он. В мякотку. Это больно, то есть эта ситуация сейчас решается, она в 1.13, сделают нормальную обертку для ошибок и там в 1.14 и 1.15 сделают возможность именно на уровне языка исключения, вообще есть несколько паттернов, которые позволяют избежать обработки ошибок, ну то есть заменить на паники, фактически аналогов Exception’ов.

Или нарост переписать.

Это решение не для каждого. Да нет, … просто сильно более низкоуровневый, поэтому не туда. Есть способы как это обработать, обратность обработки ошибок внутри функции, то есть сделать так, что функция в самом начале делают проверку в структуре, формат есть ошибка, нет ошибки, как бы, если есть, то даже не обрабатываются и за счет этого проглатывается существенная часть. 

Попытаюсь как-то подытожить, это сводится к сноровке, опять же, обращения с языком? Верно я тебя понял?

Ну, типа того, то есть, ну какой сноровке, просто приходит человек, он видит, как это написано, и пишет Си’шный код по аналогии. Вот, если код изначально был написан нормально, то, он пишет нормально, если он вдохновляется …, то код будет нормальный.

Хорошо. Так, ну, получается лучший вопрос, как ты сказал, мы вручаем Максиму. Давайте похлопаем.