![](https://habrastorage.org/webt/6p/1s/ag/6p1sagfqodqg4wvy6b8xmlinusm.png)

﻿Меня зовут Николай Сивко. Я тоже делаю мониторинг. Okmeter это 5 мониторинг, который я делою. Я решил что я спасу всех людей от ада мониторинга и мы избавим кого-то от этих страданий. Я всегда в своих презентациях стараюсь не рекламировать окметер. Естественно картинки будут оттуда. Но идея того, что я хочу рассказать заключается в том что мы делаем мониторинг несколько другим подходом, чем все делают обычно. Мы очень много об этом рассказываем. Когда мы каждого конкретного человека пытаемся в этом убедить, в итоге он убеждается. Я хочу рассказать о нашем подходе именно для того чтобы, если вы будете делать мониторинг сами, чтобы вы избежали наших граблей. 

![](https://habrastorage.org/webt/za/jq/fi/zajqfi-zfkopot4qxgmw79zgnp8.png)

Про окметер вкратце. Мы делаем то же самое что и вы, но есть всякие фишки. Фишки:

- детализация;
- большое количество преднастроенных триггеров, которые основаны на проблемах наших клиентов;
- автоконфигурация;

![](https://habrastorage.org/webt/nj/n0/vs/njn0vsnh6hfcyvjuejxy7qtopek.png)

Типичный клиент приходит к нам. У него есть две задачи:

1) понимать что все сломалось из мониторинга, когда вообще ничего нет.

2) быстро чинить это.

Он приходит в мониторинг за ответами что у него происходит.

![](https://habrastorage.org/webt/ig/ff/fl/igffflnnpqwybqqxxw1yd0nb9tw.png)

Первое что делают люди, у которых ничего нет - ставят https://www.pingdom.com/ и другие сервисы для проверки. Плюс этого решения в том, что это можно сделать за 5 минут. Вы уже не от звонков клиентов узнаете о проблеме. Там есть проблемы точностью, с тем что они пропускают проблемы. Но для простых сайтов это достаточно. 

![](https://habrastorage.org/webt/cz/hb/td/czhbtddbrnh7n0msqxzrqxn8_wq.png)

Второе что мы пропагандируем - считать по логам по статистику реальных пользователей. То есть сколько конкретный пользователь получает ошибок 5хх. Сколько время ответа по пользователям. Есть свои минусы, но в целом такая штука работает. 

![](https://habrastorage.org/webt/4g/ka/bk/4gkabkle1j9w96av-pkspnlwtis.png)

Про nginx: мы сделали так что любой клиент, который приходит, сразу ставит агент на фронтэнд и у него сразу все автоматом подхватывается, начинает парсится, начинают показываться ошибки и так далее. Ему почти ничего не надо настраивать.

![](https://habrastorage.org/webt/cf/ef/5p/cfef5pftq8hwwmmb1zqfwiqyf7m.png)

Но у большинства клиентов нет в логах таймеров в стандартном nginx. Это 90 процентов клиентов не хотят знать время ответа их сайта. Мы с этим все время сталкиваемся. Надо лог nginx расширять. Тогда из коробки автоматом мы начинаем показывать еще и гистограммы. Это наверное важный аспект того что время надо мерить.

![](https://habrastorage.org/webt/qv/yx/_x/qvyx_xrqm7dg8vqcxkonum__aws.png)

Что мы оттуда выдергиваем? На практике мы снимаем метрики в таких размерностях. Это не плоские метрики. Метрика называется index.request.rate - количество запросов в секунду. Она детализировано по:

- хосту, с которого вы сняли логи;

- логу, с которого эти данные снялись;

- http по методу;

- http статусу;

- cache статусу;

  

Это НЕ каждый конкретный URL со всеми аргументами. Мы не хотим снимать из лога 100000 метрик.

![](https://habrastorage.org/webt/u6/ah/0r/u6ah0rgp43jp3mnzdrqjpet5sza.png)

Мы хотим снять 1000 метрик. Поэтому мы пытаемся URL нормализовать, если это возможно. Берем топ URL. А для URL, которые значимы, мы показываем отдельную гистограмму, отдельно 5хх.

![](https://habrastorage.org/webt/ij/dr/ir/ijdrirnx6vwpo9jfteyepp3ax_e.png)

Вот пример того как эта простая метрика превращается в юзабельные графики. Это наш DSL сверху. Я пытался этим DSL объяснить примерную логику. Мы взяли все nginx request в секунду и разложили их по всем машинам, которые у нас есть. Получили знание о том как у нас это балансируется, сколько у нас суммарно RPS (request per second, запросов в секунду).

![](https://habrastorage.org/webt/mc/hv/sv/mchvsvendei8dswx2umciwkg8qu.png)

С другой стороны, мы можем эту метрику отфильтровать и показать только 4хх. На графике 4хх могут быть разложены по статусу, который настоящий. Напоминаю это та же самая метрика. 

![](https://habrastorage.org/webt/uk/14/vc/uk14vcumvrmenkg4mubhonkv5gu.png)

На графике можно показать 4хх с разбивкой по URL. Это одна и та же метрика.

![](https://habrastorage.org/webt/1l/n4/g8/1ln4g8sgwvteebpsdsbg_xmtly8.png)

Еще мы снимаем из логов гистограму. Гистограма это метрика response_time.histrogram, которая на самом деле RPS c дополнительным параметром level. Это как раз отсечка времени в какой bucket попадает запрос.

![](https://habrastorage.org/webt/cb/3b/nh/cb3bnh0wosk9il_ntyywe3z1kiq.png)

Мы рисуем запрос: просуммируем всю гистограмму и разложи по уровням:

- медленные запросы;
- быстрые запросы;
- средние запросы;

Мы имеем картинку, которая уже просуммирована по серверам. Метрика одна и та же. Ее физический смысл понятен. Но пользу извлекаем из нее совершенно по-разному.

![](https://habrastorage.org/webt/og/_q/0a/og_q0acmw4ianhc6n5sc3z4aokq.png)

На графике можно показать гистограмму только по URL, начинающимся с "/api". Таким образом мы смотрим гистограмму отдельно. Мы смотрим сколько в этот момент. Мы видим сколько в URL "/api" было RPS. Та же метрика, но другое применение.

![](https://habrastorage.org/webt/lt/jw/46/ltjw46topobrcyeyyhkkigncknm.png)

Пара слов про тайминги в nginx. Есть request_time, который включает себя время от начала запроса и до передачи последнего байта в сокет клиенту. А есть upstream_response_time. Их нужно мерить обе. Если просто снимаем request_time, то там вы увидите задержки из-за проблем связанности клиента с вашим сервером, вы увидите там задержки, если у нас у вас настроен limit request c burst и клиент в бане. Вы не будете понимать нужно сервер чинить или хостеру звонить. Соответственно снимаем обе и примерно понятно чего происходит.

![](https://habrastorage.org/webt/uy/xy/a9/uyxya9gyu-fr0ccab75sa7fjt3q.png)

С задачей понимать работает сайт или нет, я считаю что мы более менее разобрались. Там есть погрешности. Там есть неточности. Общие принципы такие.

Теперь про мониторинг многозвенной архитектуры. Потому что даже самый простой интернет магазин имеет как минимум frontend, за ним битрикс и базу. Это уже много звеньев. Общий смысл в том, что нужно снимать какие-то показатели с каждого уровня. То есть пользователь думает про frontend. Frontend думает про backend. Backend думает про соседние backend. И все они думают про базу. Вот так по слоям, по зависимостям и пробегаемся. Покрываем все какими-то метриками. Получаем что-то на выходе.

![](https://habrastorage.org/webt/nq/ov/rx/nqovrxw3la9pl8bkrufpklydrii.png)

Почему нельзя ограничиться одним слоем? Как правило между слоями находится сеть. Большая сеть под нагрузкой крайне нестабильная субстанция. Поэтому там бывает всякое. Плюс могут врать те замеры, которые вы делаете на каком слое. Если вы делаете замеры на слое "А" и слое "Б", и если они между собой взаимодействуют через сеть, то вы можете сравнивать их показания, находить какие-то аномалии и нестыковки.

![](https://habrastorage.org/webt/7a/kq/ov/7akqovksuz3bhdmaxez5vcibepk.png)

Про backend. Мы хотим понимать как backend мониторить. Что с ним делать чтоб понимать быстро что происходит. Напоминаю что мы уже перешли к задаче минимизации downtime. А про backend мы стандартно предлагаем понимать:

- Сколько это поедает ресурсов?
- Не уперлись ли мы в какой-нибудь лимит?
- Что у нас происходит с runtime? Например, платформа JVM runtime, Golang runtime и другие runtime. 
- Когда мы уже все это покрыли нам интересно уже ближе к своему коду. Мы можем либо воспользоваться автоматическими интруметрарием (statsd, *-metric), которые нам все это покажут. Либо заинструментировать сами, расставив таймеры, счетчики и т.д.

![](https://habrastorage.org/webt/wk/xq/6p/wkxq6ptxbigdvp3evm4t2mqh3lq.png)

Про ресурсы. У нас стандартный агент снимает потребление ресурсов всеми процессами. Поэтому для backend нам не надо отдельно снимать данные. Мы берем и смотрим сколько потребляет CPU процесс, например Python на серверах по маске. Мы показываем на одном графике все сервера в кластере, потому что мы хотим понять нет ли у нас разбалансировки и не взорвалось ли что-то на одной машине. Мы видим суммарное потребление от вчера к сегодня.

![](https://habrastorage.org/webt/t0/fl/oe/t0floe26e2hgev70o5fs18yf4ua.png)

То же самое про память. Когда мы рисуем это в таком виде. Мы выбираем Python RSS (RSS - размер страниц памяти, выделенных процессу операционной системой и в настоящее время находящихся в ОЗУ). Cуммируем по хостам. Cмотрим нигде не течет память. Везде память распределена равномерно. В принципе на свои вопросы мы ответ получили.

![](https://habrastorage.org/webt/zl/cu/yz/zlcuyz_bgzybypkgcndsotzdcsk.png)

Пример runtime. Наш агент написан на Golang. Golang агент присылает про себя метрики своего runtime. Это в частности количество секунд потраченных Golang garbage коллектором на сборку мусора в секунду. Мы здесь видим что у одних серверов метрики отличаются от других серверов. Увидели аномалию. Пытаемся это объяснить.

![](https://habrastorage.org/webt/oj/ir/pu/ojirpuokww_ym6e1hg1yrwz_8ao.png)

Есть другая метрика runtime. Сколько памяти аллоцируется в единицу времени. Видим что агенты с типом, которые верху, аллоцируют больше памяти, чем агенты которые внижу. Внизу агенты с менее агрессивным Garbage Collector. Это логично. Чем больше памяти через вас проходит, аллоцируется, освобождается, тем больше нагрузка на Garbage Collector. Дальше мы по своим внутренним метрикам понимаем почему мы столько памяти хотим на тех машинах и меньше на этих машинах.

![](https://habrastorage.org/webt/8h/4s/yq/8h4syq_g35fjhfdztsheuybzyrc.png)

Когда мы говорим про инструментирование, то приходят всякие инструменты вроде  <http://pinba.org/> для php. Pinba это extension для php от компании Badoo, который вы ставите и подключаете к php. Он вам позволяет сразу снимать и отправлять по протоколу UDP protobuf. У них есть Pinba-сервер. Но мы сделали встроенный Pinba-сервер в агенте. PHP про себя отправляет сколько она потратила CPU и памяти на такие-то скрипты, сколько трафика отдано такими скриптами и так далее. Здесь пример с Pinba. Мы показываем топ-5 скриптов по потреблению CPU. Мы видим фиолетовый выброс это замазанная точка PHP. Идем чинить замазанную точку PHP или разбираться почему она съедает CPU. Мы уже сузили область проблемы настолько, что мы понимаем следующие шаги. Мы идем смотреть код и чиним. 

![](https://habrastorage.org/webt/kx/el/vv/kxelvvi9xxzv4bfefhf4m0xbhay.png)

То же самое про трафик. Мы смотрим топ-5 скриптов по трафику. Если для нас это важно, то мы идем и разбираемся.

![](https://habrastorage.org/webt/nz/io/zs/nziozsp_7pzn3ej88j8t3bdpenk.png)

Это график про наши внутренние инструменты. Когда мы через statsd ставили таймер и измеряли метрики. Мы сделали так что количество суммарно проведенного времени в CPU или в ожидании какого-то ресурса раскладывается по хендлеру, который мы сейчас обрабатываем, и по важным стадиям вашего кода: ждали кассандру, ждали elasticsearch. На графике можно показать топ-5 стадий по хендлеру /metric/query. На графике можно показать топ-5 хендлеров по потреблению CPU, так и что там внутри происходит. Понятно что чинить.

Про backend можно дальше углубляться. Есть штуки, которые делают tracing. То есть вы можете видеть вот этот конкретный запрос пользователя с cookie такой-то и IP таким-то породил столько-то запросов в базу, они ждали столько то времени. Мы tracing не умеем. Мы tracing не делаем. Мы умеем по-прежнему считаем что applications and performance monitoring мы не делаем.

![](https://habrastorage.org/webt/pr/zy/yf/przyyfqriaulwy_fe4swdvq2bco.png)

Про базу данных. То же самое. Базы данных то же процесс. Он потребляет ресурсы. Если база очень чувствительна к latency, то там немножко другие особенности. Мы предлагаем проверять что ресурсов не стало меньше, деградации по ресурсам нет. Идеально понимать, что если база стала потреблять больше чем потребляла, то понять что конкретно в вашем коде изменилось.

![](https://habrastorage.org/webt/az/zc/lj/azzcljm0hkvcfxc1lgeqeukrdgq.png)

Про ресурсы. Точно так же смотрим сколько процесс MySQL порождает у нас операции записи на диск. Видим что в среднем столько то, но случается какие-то пики. Например, приходит туда много insert и он начинает писать на диск в 15.15, 16.15, 17.15. 

![](https://habrastorage.org/webt/u8/vb/rc/u8vbrcgbovxfcalnkk7sph4rw2i.png)

Про деградацию ресурсов. Например, у RAID батарейка ушла в maintenance режим. Она перестала контроллеру быть как живая батарейка. В этот момент отключается write cache, lateny дисков на запись возрастает. В этот момент, если база начала тупить при ожидании диска, и вы примерно знаете что у вас при такой же нагрузки на запись на диск latency было другое, то проверьте батарейку в RAID.

![](https://habrastorage.org/webt/j_/ap/qb/j_apqbgrlnp5zkiopa794hq0v4y.png)

Ресурсы по запросам. Тут все не так просто. Зависит от базы. База должна сама про себя уметь рассказывать: на какие запросы она тратит ресурсы и т.д. Самый лидер в этом это PostgreSQL. У него есть pg_stat_statements. Вы можете понимать какой запрос у вас использует много CPU, диска на чтение и на запись, трафика. 

В MySQL, если честно, все гораздо хуже. В нем есть performance_schema. Она с версии в 5.7 как-то работает. В отличие от одной view в PostgreSQL, performance_schema это 27 или 23 таблицы системных view в MySQL. Иногда если вы будете делать запросы к неправильным таблицам (к неправильным view), вы можете просадить MySQL.

В Redis есть статистика по командам. Вы видите что определенная команда использует много CPU и т.д.

В Cassandra есть времена по запросам к конкретным таблицам. Но так как кассандра проектируется так чтобы к таблице делался один тип запроса, то этого достаточно для мониторинга. 

![](https://habrastorage.org/webt/qx/gt/wo/qxgtwoplv8i3wnce0ncygyipfgw.png)

Это Redis. Видим что фиолетового использует много CPU. Фиолетовое это setex. Setex - запись ключа с установкой TTL. Если для нас это важно, идем с этим разбираться. Если для нас это не важно, просто знаем куда все ресурсы уходят.

![](https://habrastorage.org/webt/78/x_/ng/78x_nghhya3bc-3zd2xtgd2kyxy.png)

Cassandra. Видим топ-5 таблиц по запросам на чтение по суммарному времени ответа. Видим этот выброс. Это запросы к таблице и мы примерно понимаем что к этой таблице запрос у нас делает один кусок кода. Cassandra это не SQL база, в который мы можем делать разные запросы к таблицам. В Cassandra все более убого.

![](https://habrastorage.org/webt/z3/ho/s3/z3hos3wz7mgt5yvh3jbw9sgodxi.png)

Еще пару слов про workflow работ с инцидентами. Как я это вижу. 

Про alert. Наше видение workflow работ с инцидентами отличается от общепринятого.

![](https://habrastorage.org/webt/px/zd/n4/pxzdn4r9vdjznfd6hrr2x63nmxi.png)

Severy Critical. Мы уведомляем смсками и по всем real-time каналам связи.

Severy Info это лампочка которая может вам чем-то помочь при работе c инцидентами. Info никуда не уведомляется. Info просто висит и говорит вам о том, что что-то происходит. 

Severy Warning это то что можно уведомить, может и нет.

![](https://habrastorage.org/webt/op/hd/az/ophdazu-e1tchrtymsdxetidpq8.png)

Примеры Critical.

Сайт не работает вообще. Например, 5хх 100% или время ответа выросло и пользователи начали уходить. 

Ошибки бизнес-логики. То что критично. Надо мерить деньги в секунду. Деньги в секунду хороший источник данных для Critical. Например, количество заказов, открутка рекламы и другие.

![](https://habrastorage.org/webt/it/n3/em/itn3emdzqhe7lapptysa1nvlwjy.png)

Workflow с Critical такой что этот инцедент нельзя отложить. Нельзя нажать OK и пойти домой. Если вам пришел Critical и вы едете в метро, то вы должны выйти из метро, подняться на наружу, сесть на лавочку и начать чинить. В противном случае это не Critical. Из этих соображений остаточному признаку мы конструируем остальные severity.

![](https://habrastorage.org/webt/pq/ct/ww/pqctww-7nkjkjjdvmujjl9ntjrm.png)

Warning. Примеры Warning.

- Место на диске заканчивается.
- Внутренней сервис работает долго, но при этом если у вас Critical нет, то значит вам на это условная все равно.
- Много ошибок на сетевой интерфейсе.
- Самая спорная это сервер не доступен. На самом деле, если у вас больше одного сервера и сервер недоступен это Warning. Если у вас недоступен 1 backend из 100, то глупо просыпаться от смски и вы получите нервных админов. 

Все остальные Severy созданы для того чтобы помочь вам разобраться с Critical.

![](https://habrastorage.org/webt/ze/rv/je/zervjevqnr3gnnaqugze3h1jf-g.png)

Warning. Мы пропагандируем такой подход к работе с Warning. Желательно Warning закрыть течение дня. Большинство наших клиентов отключили нотификацию на Warning. Тем самым у них нет так называемый мониторинговой слепоты. Имеется ввиду складывание писем в почте отдельную директорию не читая. Клиенты отключили уведомление на Warning.

Если вы используете технику чистого мониторинга, если у вас появились 5 новых Warning, то вы в спокойном режиме их чините. Не успели сегодня починить, то отложили до завтра,  если не критично. Если Warning загораются и погашаются сами, то это надо крутить в мониторинге чтобы опять же вас не беспокоить. Тогда вы будете более терпимы к ним относиться и соответственно жизнь наладится. 

![](https://habrastorage.org/webt/xp/92/lv/xp92lvui1q6xbawczc9ltl0osys.png)

Примеры info. Тут спорно что высокое использование CPU у многих Critical. На самом деле, если при этом ничего не аффектится, то на это уведомление можно не обращать внимания. 

![](https://habrastorage.org/webt/xb/nn/p1/xbnnp13tktiqtspilpycu2oaxba.png)

Warning это лампочки, которые горят, когда вы приходите чинить Critical. Вы видите рядом два Warning. Они вам могут помочь в решение инцедента с Critical. Зачем знать про высокое использование CPU отдельно в СМС или в письме непонятно.

Бессмысленный Info это тоже плохо. Если вы их настроить в исключение, то вы будете Info любить тоже сильно. 





![](https://habrastorage.org/webt/ra/rc/uc/rarcucyuedf9vp6v6keycnnjbz4.png)

Общие принципы подхода к конструированию alert. Alert должны показывать причину. Это идеально. Но такого добиться сложно. Вот мы fulltime работаем над задачей и получается с какой-то части успеха.

Все говорят про то что нужны зависимости, автомагия. На самом деле, если у вас не приходят уведомления на то что вам не интересно, то лишнего много не будет. В моей практике статистика говорит о том что человек момент Critical инцидента отсмотрит глазами около сотни лампочек по диагонали. Он там найдет нужный и не будет думать о том что dependency скрыли какие-то лампочки, которые мне бы сейчас помогли. На практике это работает. Все что нужно делать это подчищать ненужные alert.

![](https://habrastorage.org/webt/wq/i8/mm/wqi8mmyravgx7yllannlhsgdvg8.png)

(Тут видео перескочило - прим. автора поста)



![](https://habrastorage.org/webt/ys/t_/0d/yst_0dqqfwqnra1yz0krnccw7ae.png)

Хорошо бы эти downtime классифицировать чтобы с ними потом можно было работать. Например, делать организационные выводы. Вам нужно понимать из-за чего вы лежали. Мы предлагаем классифицировать/разбивать на такие классы:

- человек налажал
- хостер налажал 
- пришли боты

Если вы их классифицируйте, то всем будет счастье.

![](https://habrastorage.org/webt/_c/kz/ll/_ckzllbldi_jkamyonco194favy.png)

Пришла СМС. Что делаем? Мы сначала бежим все чинить. Нам пока вообще ничего не важно, кроме того чтобы downtime закончился. Потому что мы мотивированную меньше лежать. Потом когда инцидент закрылся, он должен закрыться системе мониторинга. Мы считаем что инцидент должен проверить мониторингом. Если у вас мониторинг не настроен достаточно чтобы удостовериться что проблема закончилась. Это надо крутить. После того как инцидент закрыт он на самом деле не закрыт. Он ждет пока вы докопайтесь до причины. Любому руководителю на самом деле в первую очередь нужно чтобы проблемы не повторялись. Что проблемы не повторялись нужно докопаться до причины. После того как до причины докопались, у нас есть данные чтобы их классифицировать. Делаем разбор причин. Потом как мы докопались до причины, нам нужно сделать в будущем чтобы инцидент не повторялся: 

- нужно два человека в квартал для того чтобы вписать в backend такую-то логику. 

- нужно поставить больше реплик. 

Нужно сделать что-бы ровно такой же инцидент не произойдет. Когда вы работаете в таком workflow через N итераций вас ждет счастье, хороший аптайм.  Зачем мы их классифицировали? Мы можем взять статистику за квартал и понять что больше всего вам дало downtime. Потом в этом направлении работать. Работать по всем фронтам будет не очень эффективно, особенно если у вас там ресурсов в немного. 

![](https://habrastorage.org/webt/b9/os/oh/b9osohrb-qilf4mcb7ddcqjckju.png)

Мы там посчитали мы лежали столько-то времени, например 90% потому что хостер наш говнюк. Мы берем мы меняем этого хостера. Если у нас лажают люди, мы их отправляем на курсы. На самом деле, самый самый эффективный способ человеко-лажу победить это автоматизация. Машина лажает реже. Если у релизов плохое качество, то подразделениям устраиваем бойню. Когда вы знаете причину, вам легче искать решения. 

![](https://habrastorage.org/webt/js/gl/wp/jsglwpchvythu9tvkjnbruwtcr8.png)

Выводы капитанские. Мониторинг мы раскладываем на две задачи. 1) обнаружения проблем. 1) поиск причины. Если вас не устраивает время, которое вы ищете причину, то настраивайте точность мониторинга. Если точность вас не устраивает обнаружения, там тоже можно покрутить, настроить мониторинг.

![](https://habrastorage.org/webt/nn/rb/fl/nnrbflbnglbwbwggjdxuipfuxss.png)

Вопрос: мы добавляем новый сервис, новую сервер. Обычно как мы это делаем: подключили, добавили шаблоны, затем начинается творческий процесс. Мы убираем False Positive (ложные срабатывания), пытаемся убрать то, что мы не проверяем. Об этом подходе можете пару слов сказать.

Ответ: Когда вы добавили 10-й frontend сервер, то с него снимется все метрики. Если на соседних 9 frontend есть тайминги влаги, а на 10-м его нет, то мы вам зажжем вам warning alert без уведомлений: добавь тайминги. Вы в течение дня его добавляете и все метрики появились. машина такая же какая-то если вы делаете новый сервис то опять же там условно по ресурсам мы снимаем все там по каким-то еще там где мы можем дотянуться мы снимаем все ну то есть вот этот подход очень хорошо работает чтобы не забыть добавить метрику как бы если говорить о том что вот вы хотите что-то специфичная новый сервис ставите он там на другом runtime как вот не забыть ничего там вот померить или как за инструме тировать все наперед завист ремонтировать все наперед нельзя вот вот такой цвет и вы там до 1 факап ор живете в неведении потом вы понимаете чем нам а 

я имел немножечко имел ввиду немножечко про наверно пороге и на тифа и но речь о том что скажем есть у нас там тачка с базой скажем на ней lotte world штамм 4 норм там веб-сервер туда же 20 норм может быть открыта лишь вопрос не надо мире лет палаты а что надо делать alert пола ты веришь вообще никогда это нужно осмыслить до на самом деле в среднестатистическом проекте над на несколько сотен машин как правило пороге одинаковы для всех естественно есть цепную слышно ходу пах замучен всегда то есть бестолково умереть до а диск а то что dice кончился на базах чуть поменьше порог и вот как правило это вообще у всех работает у них есть два типа тогда все остальные триггеры там одинаковые и мы как бы когда делаем автоматически триггеры которые на всех пользователей выкатывается по умолчанию мы их делаем примерно общими то есть условно в любом проекте плохо работает по сгрыз и плохо настроен of the vacuum если worker of the вакууме в планке два часа и более значит что он никогда не сделает свою работу практически там для 99 процентах случаев вот такие общие триггер вам как бы как правило говорят с черпаю информацию а критиков и что для вашего проекта критика вам клиент крутит да то есть ли мой проект a critical 10 5 соток в секунду а меньше это не критика 

вот в какой момент и каким образом определяется порог и кто это делает то что это боевые навыки приходите мы хотим все-таки задать какой-нибудь критиковали своего проекта пробуйте что-то смотрите вот если бы сейчас поставили 10 5 соток сколько боли ртов получили недель назад мы вот так предлагаем 

коль здравствуйте спасибо за доклад очень интересно вопрос вот какой нагрузка всего этого доброго мониторинга nomos сервера там в амазоне очень хочется понять какого же нагрузка если вы снимаете все

 в среднем она вообще не заметно но если вы там парсить 50000 gps влоги это будет там единицы процентов ядра на десятки процентов ядра агента занимает мы короче как так как мы занимаемся только мониторинга мы на эту тему запарились да мы меряем производительность агента мы там когда вам не подходит работают и знаем есть такие люди которые говорят что писать логина наших рпс ах это вообще невозможно но мы им говорим но заправляйте нам пусть и слогу условно из-за джон экологии в память прям ну как то так то есть всегда есть возможность и ну как бы как админ я вам говорю что если у вас нету слота для мониторинга агента по ресурсам на сервере значит зачет делаете не так всегда должен быть слот это просто условно если вы этого не сделаете вы будете как как слепой вот на ощупь подменить ваш проект по силам













