![](https://habrastorage.org/webt/gm/5a/2f/gm5a2fgixjjacy6cmpqifeors2m.png)

Меня зовут Юрий Бушмелев. Я работаю в Lazada. Я сегодня буду рассказывать про то как мы делали, ннилаши логи, как мы их собирали и что мы туда пишем. TODO: Это ок, что разные времена ? Вроде бы да.

![](https://habrastorage.org/webt/_9/te/o5/_9teo5cvnhsihs4lyyyc_8pcx-m.png)

Откуда мы? Кто мы такие? Lazada это интернет-магазинам №1 в шести странах юго-восточной азии. Все эти страны у нас распределены по дата-центрам. Всего дата-центров сейчас 4. Почему это важно? Потому что некоторые решения были обусловлены тем, что между центрами есть очень слабый link. У нас микросервисная архитектура. На данный момент я, с удивлением, насчитал уже 80 микросервисов. Хотя, когда я начинал готовить доклад, их было 60. Когда я начинал над этим работать, их было примерно штук 20. Плюс, есть довольно большой кусок PHP legacy, с которым тоже приходится жить и мириться. Всё это генерирует нам, на данный момент, более 6 миллионов сообщений в минуту по системе в целом. Дальше я буду показывать как мы с этим пытаемся жить и почему это так.
TODO2: "Бушмелев. Я работаю в Lazada. Я сегодня буду рассказыват" - несогласованность

TODO: Мне не нравится тут слова "с удивлением, может быть лучше переписать - Я удивился, обнаружив, что у нас уже 80 микросервисов, когда я начинал задачу с логами их было всего 20"

![](https://habrastorage.org/webt/gy/fj/du/gyfjduafaipfx1ay5efm8xpwtdc.png)

С этими 6 миллионами сообщений надо как-то жить. Что мы с ними должны сделать? 6 миллионов сообщений, которые надо:

- отправить из приложения
- принять для доставки
- доставить для анализа и хранения.
- проанализировать
- как-то хранить.

![](https://habrastorage,.нилorg/webt/vq/kr/yt/vqkrytwk4c_gjs9fza8_zn9hanu.png)

Когда появилось три миллиона сообщений, у меня был примерно такой же вид. Потому что мы начинали с каких-то копеек. Понятно, что туда пишутся логи приложений. Например, не смог подключиться к базе данных, смог подключиться к базе данных, но не смог что-то прочитать. Кроме этого каждый наш микросервис пишет еще и access log. Каждый запрос прилетевший на микросервис падает в лог. Зачем мы это делаем? Разработчики хотят иметь возможность трейсинга. В каждом access логе лежит поле traceid, по которому дальше специальный интерфейс раскручивает все цепочку и красиво п
TODO2: ""казывает trace. Trace показывает как проходил запрос и это помогает нашим разработчикам быстрее справляться со всякой неопознанной фигней. 

![](https://habанализа и хранения.
- проанализировать
- как-то хранитьr - несогласованность
astorage.org/webt/me/fd/7d/mefd7deeshbhsvsjb8n7dh81ok4.png)

Как с этим жить? Сейчас я вкратце расскажу поле вариантов - как вообще эта проблема решается. Как решать задачу сбора, передачи и хранения логов. 

![](https://habrastorage.org/webt/0m/f8/4r/0mf84rxvsn0ewrqui_4egxgb2hk.png) 

Как писать из приложения? Понятно, что есть разные способы. В частности, есть best practice как, нилнам завещают модные товарищи. Есть old school в двух видах, как завещали деды. Есть другие способы. 
TODO: "Как нам завещают" - плохое слово, может быть "рассказывают" ? 
TODO: "Что значит old school в 2х видах" ?

![](https://habrastorage.org/webt/le/7-/_n/le7-_n8o7wl8nh4g0qadwz-jucq.png)

Со сбором логов примерно такая же ситуация. Вариантов решения этой конкретной части не так много. Их уже больше, но ещё не так много. 

![](https://habrastorage.org/webt/kn/id/o2/knido22_ecpa0cgvfjzissz3fei.png)
TODO2: "приложения? Понятно, что есть разные способы. В частнос" - несогласованность

А вот с доставкой и последующим анализом - количество вариаций начинает взрываться. Описывать каждый вариант сейчас не буду. Думаю, основные варианты на слуху у всех, кто интересовался темой. 

![](https://habrastorage.org/webt/qu/zg/z0/quzgz0a21pgjdb0o8ek5d3nxmwc.png)

Я покажу как мы делали это в Lazada и как собственно все это начиналось. 
TODO:, Нунилжно тут запятыми выделять или нет ? По идее да.

![](https://habrastorage.org/webt/tu/b_/xt/tub_xtgrnyznjspm2vxf_lepv7o.png)

Год назад я пришёл, Lazadнилa и меня отправили на проект про логи. Там было примерно вот так: Log из приложения писался в stdout и stderr. Все сделали по-модному. Но дальше разработчики это выкинули из стандартных потоков, а дальше там, как-нибудь, специалисты по инфраструктуре разберутся. Между инфраструктурным специалистом и разработчиком есть релизеры, которые сказали "давайте файл завернем просто в shell". А поскольку всё это в контейнере, то завернули прям в самом контейнере, промапили внутрь каталог и положили это туда. Думаю, что всем примерно очевидно что из этого получилось
TODO2: ""
TODO: "Всё это в контейнере" -> "Дело происходит вЯ покажу как мы делали это в Lazada и как собственн  - несогласованность
контейнере"
TODO2: "положили это туда" -> "Положили файл туда"
TODO2: ""webt/tu/b_/xt/tub_xtgrnyznjspm2vxf_lepv7o.png
 - несогласованность

![](https://habrastorage.org/webt/l2/pl/lz/l2pllz5jvccjhbz0zzxcitkrmsk.png)

Посмотрим пока чуть подальше. Как мы эти логи доставляли. Кто-то выбрал td-agent, который на самом деле fluentd, но не совсем fluentd. Я так и не понял отношения этих двух проектов. Они вроде бы об одном и том же. Вот этот вот fluentd, написанный на Ruby, читал файлы логов, парсил их в JSON по каким-то регуляркам. Потом он отправлял их в Kafka. Причем в Kafka на каждую API у нас было 4 отдельных топика. Почему 4? Потому что есть live, есть staging и потому что есть stdout и stderr. Разработчики их плодят, а инфраструктурщики должны их создавать в Kafka. Контролировал Kafka другой отдел. Поэтому над было создавать ticket, чтобы они создали там 4 топика на каждый api. Все про это забывали. В общем был треш и угар. 

![](https://habrastorage.org/webt/x9/2-/5s/x92-5sqis-vgly1ccyh0gjt2mxy.png)

Что мы дальше с этим делали? Мы отправляли это в Kafka. Дальше из кафки половина логов улетало в Logstash. Другая половина логов улетала в Graylog. В итоге всё это улетало в один кластер Elasticsearch.

![](https://habrastorage.org/webt/ge/w9/kz/gew9kzazsmr5pwee16cuyor6n2w.png)

Вот так это выглядит, если отдаленно посмотреть. Здесь вот цифрами сразу отмечены проблемные места. Это вот прям совсем проблемные, с которыми надо что-то делать. Про каждые грабли отдельно расскажу. 
TODO: "каждые" кажется лишним

![](https://habrastorage.org/webt/2w/xu/eb/2wxuebmewhsjvjbr_9taaq5zhn4.png)

Под пунктами 1,2,3 пишутся файлы и соответственно здесь три грабли. 

1 - это нам надо их куда-то писать. Не всегда хотелось бы давать API. Желательно, чтобы запись логов была запущена в контейнере. 

2,3 - это у нас много запросов приходит в API. API пишет много данных в файл. Файлы растут. Нам их надо ротировать, потому что иначе никаких дисков не напасешься. Ротировать их плохо, потому что они сделаны редиректом через shell в каталог. Мы никак не можем его отротировать. Нельзя сказать приложению, чтобы он переоткрыл дескрипторы. Потому что разработчики на тебя посмотрят, как на дурака. Какие дескрипторы? Мы вообще в stdout пишем. Инфраструктурщики сделали copy-truncate в logrotate, который делает просто копию файла, truncate оригинал. Между этими процессами копирования обычно кончается место на диске.
TODO: "копирование" -> "ротирование"

4 - У нас были разные форматы были в разных API. Они немножко отличались. Но regexp надо было писать разные. Поскольку всё это управлялось Puppet, то там была большая вязанка классов со своими тараканами. Плюс еще td-agent большую часть времени мог есть память, тупить, он мог просто делать вид, что он работает и ничего не делать. Снаружи понять, что он ничего не делает было невозможно. В лучшем случае он упадет и его кто-нибудь поднимет потом. Точнее пролетит alert и кто-нибудь пойдет руками переподнимет. 

![](https://habrastorage.org/webt/x9/8i/a-/x98ia-rs9owg6hl2qqkfjpza-yg.png)

6 - И самый трэш и угар это был elasticsearch. Потому что это была старая версия. Потому что у нас не было выделенных экспертов по нему на тот момент. У нас были разнородные логи, у которых поля могли пересекаться. Разные логи разных приложений могли писаться с одинаковыми названиями полей. Но при этом внутри могли быть разные данные. Один лог приходит с Integer в поле, например, level. Другой лог приходит с String в том же поле. В отсутствие статического маппинга получается такая замечательно вещь. Если после ротации индекса elasticsearch первым прилетело сообщение со строкой, то мы живем нормально. Если вот первым пролетел с Integer, то все последующие сообщения, которые прилетели со String просто отбрасываются. Потому что не совпадает тип поля.
TODO: Заменил "Не было мастеров", на "Не было экспертов".

![](https://habrastorage.org/webt/c9/ym/lq/c9ymlqxv89qg1oohi-lnhknyedg.png)

Мы начали задаваться вот этими вопросами. Мы решили не искать виноватых. 

![](https://habrastorage.org/webt/fl/r1/kv/flr1kvyks_o2kdciv4pekiwtbiy.png)

А на второй вопрос ответ - "Надо что то делать". Очевидная вещь - надо завести стандарты. Некоторые стандарты у нас уже были. Некоторые мы завели чуть позже. К счастью, единый формат логов для всех api уже утвердили на тот момент. Формат прописали прямо в стандарт взаимодействия сервисов. Кто хочет получать логи, они должны писать их в этом формате. Если кто-то не пишет логи в этом формате, значит мы ничего не гарантируем. Далее, хотелось бы завести единый стандарт на способы записи, доставки и сбора логов. Собственно, куда их писать и чем их доставлять. Идеальная ситуация это когда в проектах используется одна и та же библиотека. Вот есть отдельная библиотека логирования для Go, есть отдельный библиотека для PHP. Все, кто у нас, все должны их использовать. Но на данный момент я бы сказал, что процентов на 80 у нас это получается. Но некоторые продолжают есть кактусы. И вот там вот еле-еле начинает проступать SLA на доставку логов. Его пока нет, но мы над этим работаем. Потому что это очень удобно, когда инфра говорит, что если вы пишете в таком формате туда-то в такое-то место и не более N сообщений в секунду, то с вероятностью такой-то доставим туда-то. Это снимает кучу головняка. Просто, если SLA есть, это прямо замечательно. 

![](https://habrastorage.org/webt/im/aq/aj/imaqajo2oi-qoyb--oja3i2fnyy.png)

Как мы начали решать проблему? Основная грабля была с td-agent. Было непонятно, куда у нас деваются логи. Поэтому, первым пунктом было решено заменить td-agent. Вот вкратце варианты на что его заменить. Я здесь набросал то, что мы вспоминали. Fluentd, во-первых, я с ним сталкивался на предыдущей работе и он там тоже периодически падал. Во-вторых, это тоже самое, только в профиль. Filebeat чем был удобен для нас? Тем, что он на Go, а у нас большая экспертиза в Go. Cоответственно, если что, мы могли его под себя как-то дописать. Поэтому мы его и не взяли. Чтобы даже соблазна никакого не было начинать его под себя записывать. Как бы очевидным решением оставшимся это всякие сислоги в таком вот количестве. Либо написать что-то свое, но мы это отбросили ровно как filebeat. Если что-то писать, то лучше писать что-то полезное для бизнеса. Для доставки логов лучше взять что-то готовое. Поэтому выбор фактически свелся к syslog-ng/rsyslog. Cклонился в сторону rsyslog просто потому, что у нас в Puppet уже были классы для rsyslog и я не нашел между ними очевидной разницы. Что там syslog, что тут syslog.
TODO: "Основная грабля?", может всё же множественное число ?
TODO2: "Как бы очевидным решением оставшимся"

,!нил[](https://habrastorage.org/webt/xl/wg/7y/xlwg7yv4du2k8ktumnmdltwci78.png)

И немножко про rsyslog. Во-первых, он клёвый? потому что у него есть много модулей. У него человеко-понятный RainerScript. Офигенный бонус - что мы могли его штатными средствами сэмулировать поведение td-agent и для приложений ничего не поменялось. Мы меняем td-agent на rsyslog, все остальное пока не трогаем. Cразу получаем работающую доставку. Далее, mmnormalize это офигенная штука в rsyslog. Она позволяет парсить логи, но не с, помонилщью Grok и Regexp. Она делает abstract index tree. Она парсит логи примерно, как компилятор парсит исходники. Поэтому это позволяет работать очень быстро, жрать мало CPU. В общем, прям очень клёвая штука. Есть куча других бонусов. Я на них не буду останавливаться.
TODO2: ""
![](https://может всё же множественное число ?
TODO2: "Как бh - несогласованность
abrastorage.org/webt/tf/6w/c0/tf6wc0eulg65fu-dy64mmkjyr4g.png)

У неё есть ещё куча недостатков. Они примерно такие же как и бонусы. Основная проблема надо уметь его готовить и надо подбирать версию.
TODO: "У кого у неё?"

![](https://habrastorage.org/webt/p-/2l/l5/p
TODO2: ""2ll5-sxmfivl2136kopu1oipi.png, mmnormalize это офигенная штука в rsyslog. Она позволяет парсит) - несогласованность


Мы решили, что будем писать логи в socket unix away. Причем не в /dev/log, потому что там у нас каша из системных логов, там journald в этом pipeline. Поэтому давайте писать в кастомные socket. Мы его прицеплен к отдельному rules. Не будем ничего мешать. Будет все прозрачно и понятно. Так мы собственно и сделали. Каталог с этими сокетами стандартизирован и пробрасывается во все контейнеры. Контейнеры могут видеть нужный ему socket, открывать и писать него. Почему не файл? Потому что все читали [статью про подушечку](https://habr.com/ru/company/badoo/blog/280606/), которая пыталась пробросить файл в docker и обнаруживалось, что после рестарта rsyslog меняется file descriptor и docker теряет этот файл. Он держит открытом что-то другое, но уже не тот socket, куда пишут. Мы решили, что мы обойдем эту проблему и заодно обойдем проблему блокировки.
TODO: "socket unix away" - что это такое ?
TODO2: "Мы его прицеплен к отдельному rules. Не будем ничего мешать" - переформулировать по видео.

![](https://habrastorage.org/webt/ri/kn/r9/riknr9odspcwgtmywqapeq1htmi.png)

Rsyslog делает действия, указанные на слайде. Отправляет либо в релей либо в Kafka. Kafka соответствует старому способу. Релей - это я попытался использовать чисто rsyslog для доставки логов без Message Queue, стандартными средствами rsyslog. В принципе это работает. 

![](https://habrastorage.org/webt/wh/bl/fv/whblfvj4lnbmdryfev1fypctp74.png)

Но есть нюансы с тем, как запихивать их потом в, соответственно, в эту часть. Эта часть используется между дата центрами. Здесь compressed tcp link, который позволяет сэкономить dd и, соответственно, как-то увеличить вероятность того, что мы получим какие-то логи из другого datacenter в условиях, когда сильно загружен bandwidth. Потому что у нас есть Индонезия, в которой все плохо. Вот там это постоянная проблема. Я подсветил часть с Logstash, Graylog и elasticsearch красным. 
TODO: "в, соответственно, в эту часть" - переформулировать
TODO2: Не понятно к чему "подсветил красным"

![](https://habrastorage.org/webt/zx/bx/gi/zxbxgimmd3xniduuuw0spq0vg1o.png)

Мы задумались над тем, как нам, собственно, промониторить логи, которые мы записали из приложения. С какой вероятностью они доезжают до того конца? Мы решили завести метрики. У rsyslog есть свой модуль сбора статистики, в котором есть какие-то счетчики. Он может показать вам размер в очереди, сколько сообщение пришло в такой-то action. Из них уже можно что-то взять. У нее есть кастомный счетчик, который можно настроить и он будет вам показывать количество сообщений, который записало ваше приложение. Я написал rsyslog_exporter на Python. Мы все это отправили в Prometheus, построили графики. 

![](https://habrastorage.org/webt/kq/b0/-1/kqb0-1ox3sevtkje8qnb7ekp1is.png)

С чем возникли проблемы? Проблемы возникли в том, что у нас обнаружилось, внезапно, что наши Live API пишут по 50к сообщений в секунду. Это только Live API без staging. FА Greylog показывает только 12 тысяч сообщений в секунду. Возник резонный вопрос: А где остатки? Из чего мы сделали вывод, что Greylog просто не справляется. Посмотрели,действительно, Greylog с elasticsearch не справлялся с этим потоком. Далее другие открытия, которые мы сделали в процессе. Запись socket блокируются. Как это случилось? Когда я использовал rsyslog для доставки. В какой-то момент у нас сломался канал между дата центрами. Встала доставка в одном месте, встала доставка в другом месте. Все это докатилось до машины с Live API, которые пишут в socket rsyslog. Там заполнилась очередь. Потом, заполнилась очередь на запись в unix socket, который по умолчанию 128 пакетов. Следующий write в приложении блокируется. Когда мы смотрели в библиотечку, которая пользуемся в Go приложениях, там было написано, что мы пишем в socket в неблокирующем  режиме. Мы были уверенны, что ничего не блокируется. Потому что мы читали [статью про подушечку](https://habr.com/ru/company/badoo/blog/280606/), в которой про это написано. Но есть момент. Вокруг этого дата центрами был еще бесконечный цикл, в котором он постоянно пытался его запихать в этот socket. Вот его мы не заметили. Пришлось переписать библиотеку. С тех пор она несколько раз менялась, но сейчас мы избавились от блокировок во всех подсистемах. Поэтому можно останавливать rsyslog и ничего не упадет. Нужно мониторить размер очередей. Это помогает не наступить на эти грабли. Во-первых, мы можем мониторить, когда мы начинаем терять сообщения. Во-вторых, можем мониторить, что у нас в принципе проблемы с доставкой. И еще неприятный момент, амплификация в 10 раз в микросервисной архитектуре - это очень легко. У нас входящих запросов не так много. Но в итоге мы реально увеличиваем нагрузку по логам примерно раз в десять. Я, к сожалению, не успел посчитать точные цифры, но микросервисы они такие. Это надо иметь ввиду. Получается, что на данный момент подсистема сбора логов самая нагруженная в Lazada. 
TODO: "Запись socket блокируются" Переформулировать по видео
TODO2: "до машины с Live API, которые пишут в socket rsyslog" - несогласованность
TODO3: "Вокруг этого дата центрами был еще бесконечный цикл, в котором он постоянно пытался его запихать в этот socket" - переформулировать по видео

![](https://habrastorage.org/webt/6a/fd/mi/6afdmitt6iid3yqd6cmdgq8_lkq.png)

Как решить проблему elasticsearch? Если надо быстро получить логи в одном месте, чтобы не бегать по всем машинам и не собирать их там, то нужно использовать файловые хранилища. Это просто гарантированно работает. Дальше уже можно будет настраивать elasticsearch, graylog. У вас уже будут все логи и вы их сможете хранить насколько хватает рейдов.
TODO: Это просто гарантированно работает - лишнее слово "просто"

![](https://habrastorage.org/webt/cx/1d/g3/cx1dg33kkvufonoxpwrrpmmabza.png)

На момент моего доклада схема стала выглядеть вот так. В файл мы практически перестали писать. Сейчас скорее всего на локальных машинах, на которых запущены API, в файлы писать перестанем. Во-первых, есть файловое хранилище, которое работает очень хорошо. Во-вторых, там постоянно кончается место. Вот эта часть с LogStash и Graylog, она реально парит. Поэтому надо от нее избавиться. Надо выбрать что-то одно. Потому что иначе это вечный геморрой.

![](https://habrastorage.org/webt/w4/bj/po/w4bjpoxgbdggewegwtrg1ao9mpi.png)

Мы решили выкинуть LogStash и Kibana. Потому что у нас есть отдел безопасности. Какая связь? Связь в том что Kibana без X-Pack и без Shield не позволяет сделать аутентификацию и авторизацию. Поэтому взяли Graylog. В нем все это есть. Мне не нравится, но он работает. Мы купили нового железа, поставили там свежий Graylog, перенесли все структурированые логи (логи со строгими форматами) в отдельный Graylog. 

![](https://habrastorage.org/webt/cn/hr/41/cnhr41tyx4sf0c0skbjqrgbyb90.png)

Что собственно новый Graylog входит. Мы просто записали все в докер. Взяли кучу серверов, раскатали три инстанса кафки, 7 серверов Graylog. Все это подняли на рейдах из HDD. Увидели indexing rate до 100 тысяч сообщений в секунду. Увидели цифру, что 140 терабайт данных в неделю. 
TODO: "Что собственно новый Graylog входит. " - переформулировать с видео.

![](https://habrastorage.org/webt/wv/yd/yj/wvydyj9d_mfo3xztj9elcbvpdaa.png)

У нас идут 2 распродажи. Мы переехали за 6 миллионов. У нас Graylog не успевает прожевывать. Как-то надо опять выживать. 

![](https://habrastorage.org/webt/xf/2j/lc/xf2jlclf52i3oi3nsxdksth-xpe.png)

Выжили мы вот так. Добавили еще немножко серверов и ssd. На данный момент мы живем вот таким способом. Сейчас мы прожёвываем уже 160к сообщений в секунду. Мы не уперлись в лимит. Пока непонятно сколько мы реально сможем вытянуть из этого. 

![](https://habrastorage.org/webt/bn/ck/ss/bnckssznjbmcrm7v4zh7yhgoltw.png)

Вот такие у нас планы на будущее. Из них реально самое важное наверное high availability. У нас его пока нет. Собрать метрики с Graylog. Сделать rate limit чтобы у нас одна сошедшая с ума API не убивала нам bandwidth и все остальное. И, наконец, подписать какой-то SLA c разработчиками. И написать документацию.

![](https://habrastorage.org/webt/cc/hr/az/cchrazzrhareykxd78meljitrso.png)

Кратенько в итоге всё что мы пережили. Во-первых, стандарты. Во-вторых, syslog - торт. В-третьих, rsyslog работает именно вот так, как написано. 



Вопросы.

Вопрос: Почему все-таки решили не брать ....

Ответ: Надо писать в файл. Очень не хотелось. Когда у тебя API пишет тысячи сообщений в секунду, ты даже, если раз в час будешь ротировать, то это все равно не вариант. Можно писать в pipe. На что меня разработчики спросили: А что будет если, процесс, в который мы пишем, упадет? Я просто не нашел что им ответить. Сказал, давайте мы не будем так делать. 

Вопрос: Почему вы не пишете логе просто в HDFS?

Ответ: Это следующий этап. Мы про него подумали самом начале, но, поскольку, нет ресурсов этим заниматься, то он у нас висит в long term solution. 

Вопрос: Колоночный формат был бы более подходящий.

Ответ: Я все понимаю. Мы за обеими руками. 

Вопрос: Вы пишите в rsyslog. Там можно и TCP, и UDP. Но если UDP, то тогда как вы гарантируете доставку?

Ответ: Есть два момента. Первый, я сразу всем говорю, что мы не гарантируем доставку логов. 

Вопрос: Когда API генерирует какое-то сообщение в лог и передает управление микросервисом, то не сталкивались ли вы с проблемой, что сообщение от разных микросервисов приходят в неправильном порядке? Из-за этого возникает путаница.

Ответ: Это нормально, что они приходят в разном порядке. К этому надо быть готовым. Потому что любая сетевая доставка вам не гарантирует порядок. Если возьмем файловые хранилища, то каждая API сохраняет логи в свой файл. Вернее, там rsyslog раскладывает их по директориям. У каждого API есть свои логи, куда можно пойти посмотреть и потом по timestamp в этом логе можно их потом сопоставлять. Если они идут смотреть в Graylog, то там они сортируется по timestamp.
TODO: "Если они идут смотреть в Graylog" - кто они ? 

Вопрос: Timestamp может отличаться на миллисекунды.

Ответ: Timestamp генерит сама API. В этом, собственно, вся фишка. У нас есть NTP. API генерит timestamp уже в самом сообщении. Его не rsyslog добавляет.

Вопрос: Не очень понятно взаимодействие между датацентрами. В рамках датацентра понятно как логи собрали, обработали. Как проиходит взаимодействие между датацентрами? Или каждый датацентр живет своей жизнью?

Ответ: Почти. У нас каждая страна находится в каком-то одном датацентре. У нас нет на данный момент размазывания, чтобы одна страна была размещена по разным датацентрам. Поэтому не надо их объединять. Внутри каждого центра есть Log Relay. Это Rsyslog сервер. На самом деле, две менеджмент машины. Они одинаково настроены. Но пока просто трафик идет через одну из них. Она логи все агрегирует. У нее есть дисковая очередь на всякий случай. Она жмет логи и отправляет их в центральный датацентр (сингапурский). Дальше, они уже отравляются в Graylog. И в каждом датацентре есть свой файловый storage на случай, как раз, если у нас пропала connectivity. Мы имеем все логи там. Они все они там останутся. Они там будут сохранены. 
TODO: "Мы имеем все логи там. Они все они там останутся. Они там будут сохранены" - видео

Вопрос: При нештатных ситациях оттуда вы получаете логи?

Ответ: Можно пойти туда и посмотреть.
TODO: Куда "туда" ?

Вопрос: Как вы мониторите то, что вы не теряете логи?

Ответ: Мы их теряем на самом деле и мы это мониторим. Мониторинг запустили месяц назад. В библиотеке, которые используют Go API, есть метрики. Она умеет считать сколько раз она не смогла записать в socket. Там, на данный момент, есть хитрая эвристика. Там есть буфер. Он пытается записывать из него сообщение в socket. Если буфер переполнится, он начинает их дропать. И считает, сколько он их подропал. Если там начинают переполняться счетчики, мы об этом узнаем. Они сейчас приезжают также в prometheus и в Grafana можно посмотреть графики. Можно настроить оповещения. Но, пока непонятно кому их отправлять. 
TODO: "В библиотеке, которые используют Go API" - несогласованность

Вопрос: В elasticsearch вы с резервированием храните логи. Сколько у вас реплик? 
Ответ: Одна реплика

Вопрос: Это всего одна реплика?

Ответ: Это мастер и реплика. В двух экземплярах данные хранятся. 

Вопрос: Размер буфера rsyslog вы как-то подкручивали? 

Ответ: Мы пишем дейтаграммы в кастомный unix socket. Это нам сразу же накладывает ограничение 128 килобайт. Мы не можем записать в него больше. Прописали в стандарт. Библиотеки обрезают и ставит флаг, что сообщение обрезано. У нас в стандарте самого сообщения есть специальные поле, которое показывает была ли она обрезано сообщение при записи или нет. 

Вопроc: Пишете ли вы битые JSON? 

Ответ: Битый JSON будет отброшен во время relay, потому что слишком большой пакет. Либо будет отброшен Graylog потом, потому что не сможет JSON распарсить. Но здесь есть нюансы, которые надо фиксить и они большей частью завязанны на rsyslog. Я уже заполнил туда несколько issue. 
TODO: "заполнил туда несколько issue"

Вопроc: Почему Kafka? Пробовали ли RabbitMQ? Не складывается Graylog при таких нагрузках?

Ответ: У нас с Graylog не складывается. А c Graylog у нас складывается. С ним реально проблемно. Он своеобразная штука. На самом деле он не нужен. Я бы предпочел писать из rsyslog напрямую в elasticsearch и смотреть потом Kibana. Но? надо утрясти вопрос с безопасниками. Это возможный вариант нашего развития. Logstash использовать смысла не будет. Потому что я могу все это же самое сделать rsyslog. У rsyslog есть модуль для записи elasticsearch. С Graylog мы пытаемся как-то жить. Мы его даже немножко потюнили. Насчет Kafka. Так исторически сложилось. Когда я пришел, она уже была и в нее уже писали логи. Мы просто подняли наш кластер и переехали в него логами. Насчет RabbitMQ у нас не складывается c RabbitMQ. RabbitMQ у нас складывается. До недавнего момента в продакшене он есть и с ним были проблемы. Сейчас, перед распродажей, его зашаманили и он стал нормально работать. Но до этого я был не готов его выпускать в production. Есть еще один момент. Graylog умеет читать версию MQP 0.9, а rsyslog умеет писать версию MQP 1.0. И нет ни одного программного обеспечения, которая умеет и то и другое. Есть либо то, либо другое. Поэтому, на данный момент только Kafka. Но там тоже свои нюансы. Потому что Kafka той версии rsyslog, которую мы используем может потерять вот весь буфер сообщений, которое она выгребла из rsyslog. Пока мы с этим миримся. 

Вопроc: Вы используете Kafka потому, что она у вас в была? Больше ни для каких целей не используется?

Ответ: Kafka, которая была, используется командой Data Sсience. Это совсем отдельный проект, про который, к сожалению, нечего рассказать. Не в курсе. Она была в ведении команды Data Sience. Когда логи заводили, решили использовать ее, чтобы не ставить еще и свою. Сейчас мы обновили Graylog и у нас потерялась совместимость, потому что там старая версия. Заодно, мы избавились от этих четырех топиков на каждый API. Мы сделали один широкий топик на все лайвы, один широкий-широкий топик на все staging и просто всё туда пуляем. 

Вопроc: Зачем нужно вот это шаманство с сокетами? Пробовали ли использовать log-драйвер syslog для контейнеров.

Ответ: На тот момент, когда мы этим вопросом задавались, версия docker была 1.0 или 0.9. Docker сам по себе был странный. Во-вторых, если в него еще и логи пихать. У меня есть непроверенное подозрение, что он пропускает все логи через себя, через демон докера. Если у нас 1 API сходит с ума, то остальные API утыкаются в то, что они не могут отправить stdout и stderr. Я не знаю к чему это приведет. У меня есть подозрение, что не надо log драйвер вот в этом месте использовать. У нас функциональное тестирование. У них есть свой собственный кластер с логами. Они используют log-драйверы. У них там вроде бы даже все хорошо. Мы на тот момент, когда все это затевали, нам надо было, чтобы оно просто работало. 

Вопроc: Вы доставку между датацентрами делаете на rsyslog. Почему не на Kafka?

Ответ: Мы делаем и так и так на самом деле. По двум причинам. Если канал совсем убитый, то у нас все логи даже в сжатом виде не пролазят него. А кафка позволяет их просто терять в процессе. Мы этим способом избавляемся от залипания вот этих логов. Мы просто используем Kafka в этом случае напрямую. Eсли у нас канал хороший и хочется освободить его, то мы используем их rsyslog. Но на самом деле можно его настроить его так, чтобы он дропал то, что не пролезло. На данный момент, мы просто где-то используем доставку rsyslog напрямую, где-то Kafka.