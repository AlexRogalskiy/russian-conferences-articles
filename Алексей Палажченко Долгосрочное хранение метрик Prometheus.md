![](https://habrastorage.org/webt/nc/xy/yx/ncxyyxbouopsyyobm4rhgfrjxn0.png)

Друзья! Всем привет! Меня зовут Алексей Палажченко. Я работаю в компании Percona. Я хотел бы вам рассказать про долгосрочное хранение метрик в Prometheus.

![](https://habrastorage.org/webt/rw/vv/4v/rwvv4v4zpjhn5tlmrclgeii9mkq.png)

Я работаю в компании Percona и делаю продукт это называется percona monitoring and management. Это коробочное решение, которое наши клиенты ставит себе. Это полностью open source, который состоит из prometheus, grafana для рисования графиков, custom софта query analytics и нашей собственной обертки, которая позволяет вам делать некоторое управление. Вы можете добавить scrape target в prometheus. Это новые цели откуда он будет брать метрики без того чтобы руками заходить в контейнер или в виртуальную машину и руками править файл конфигурации.

Важно понимать что это не SaaS. У нас нет продакшена. Наш production находится у наших клиентов. На нем экспериментировать не очень хорошо. У нас есть ближайшая вещь, которую можно бы назвать production этот pmmdemo.percona.com. Cейчас он не работает. Кто может предположить почему pmmdemo.percona.com не работает? Из-за GDPR сейчас нам пришлось его выключить. 

Мы ставим PMM клиентам - коробочное решение docker-контейнер или виртуальная машина. Им всем нравится prometheus. Некоторые люди, которые первый раз смотрять на prometheus сталкиваются с pull моделью. Для новичков это неудобно. Вообще отдельный большой разговор. Можно спорить о pull или push методах. В среднем это примерно одно и то же.

Некоторые вещи в prometheus очень крутые. 

- Prometheus query language это действительно крутая вещь, которой нет аналога практически негде.

- Второе что нравится это service discovery. Если у вас какая-то динамическая инфраструктура, kubernetes, то автоматически не нужно добавлять руками все цели для мониторинга. Если статическое тоже это можно сделать довольно просто. Нужно использовать файл конфигурации.


Клиентам prometheus нравится. Они хотят хранить метрики дольше и больше. Кто-то использует prometheus только для оперативного мониторинга. Но кто-то хочет хранить  дольше, смотреть динамику, сравнивать со графиками год назад. При этом цель долгосрочного хранения метрик не является целью для проекта prometheus. Изначально он создавался для того чтобы хранить метрики недолго. Soundcloud хранит метрики буквально несколько дней. В prometheus есть механизмы, которые за это делать дольше, но они устроены немножко сбоку. Поэтому мы можем сделать решение для экосистемы prometheus не меняя само ядро системы. Мы на основе них можем сделать свое собственное решение в рамках этой же экосистемы. 

![](https://habrastorage.org/webt/ws/1n/7k/ws1n7kpa7ohpkxwpxuli5dcpwuc.png)

Это не доклад про готовые решения. это доклад про наш опыт, про нашу боль, про наши попытки. Если вы рассчитывали что после этого доклада, вы скачайте репозиторий или докер-контейнером, запустите и все заработает, то это не так. Но при этом это достаточно близко к тому чтобы быть так. У нас есть наработки. Они всего opensource. Вы можете взять попробовать. Они не готовы к production пока еще. Но с той информацией, которая есть в этом докладе, вы сможете понять почему так, что можно сделать лучше. Вы можете сделать свое решение, которое будет хорошо подходить вам.

![](https://habrastorage.org/webt/ej/4j/sm/ej4jsmldkwk-bsme40kqehzilqc.png)

Как метрики хранится в prometheus? Есть локальный storage. Есть remote storage. Это фактически два разных мира. Они слабо пересекаются. Поэтому и доклад тоже разделен на 2 части.

![](https://habrastorage.org/webt/jm/ic/81/jmic81mrsu49j-zxv6spn9aopac.png)

Если вы были на предыдущем докладе в главном зале где как раз была хорошее интро в prometheus, вы знаете что локальный storage это отдельная библиотека, который называется TSDB. TSDB не имеет ничего общего с OpenTSDB. TSDB это отдельный Go пакет, который можно использовать из своей программы на Go. На уровне библиотеки TSDB нет никакого клиента и сервера.

Эта библиотека оптимизирована для работы именно с time series данными. Например, в TSDB есть дельта encoding, который позволяет вам хранить не сами числа, а именно изменения между этими числами. Это позволяет вам вместо того чтобы хранить 16 байт - хранить 8 байт. 8 байт под время и 8 байт под значение. То есть вы храните в среднем 1 или 2 байта именно за счет этого хорошего сжатия. 

TSDB оптимизирован для pull модели. Данные туда только добавляется. В prometheus нельзя запихать исторические данные. Для этого нет API. Максимальная дельта примерно 5 минут. Если данные более старые, они не будут приниматься. 

В TSDB нет никакого встроенного downsamping [tsdb#313](https://github.com/prometheus/tsdb/issues/313). Есть открытый issue, в котором была дискуссия на тему того что в целом есть проекты, которые вокруг prometheus что-то делают и там есть downsamping. Пока что решение такое что в TSDB не будут добавлять downsamping.

![](https://habrastorage.org/webt/5q/pb/vt/5qpbvtptau3gz7ygcxb9cl3nmzg.png)

Как бы нам получить данные с TSDB. TSDB это база данных на диске. Работать с ней можно, если вы пишете Go программу. Но если вы не пишите программу на Go, то есть JSON API, который позволяет вам сделать query запросы. Если вы хоть раз пользовались prometheus и хоть раз строили какой-нибудь график, вы знаете стандартный Query API, в котором есть параметр query, в котором можно выполнить любой PromQL запрос и опционально время. Если время отсутствует, то берется текущее время.

На слайде выделено специфическое query, которое в реальной жизни вы редко увидите. Это хак. Это позволяет нам вытащить все метрики, которые есть в prometheus. Как это работает? На уровне PromQL говорится что нельзя написать такое выражение, которое бы заматчила все time seriers. Это прямо в правилах написано. Еще одно правило говорит о том, что нельзя сделать такой matcher, в котором все значения пустые. Если вы напишете просто фигурные скобочки это не будет работать. Если вы напишите name не равно чему-нибудь (не пустое значение), то не будет работать. А вот это реальный хак, который позволяет это сделать. При этом он даже не освобо документирован. В самом коде есть комментарии о том что это работает.

Второй запрос это query_range, который делает то же самое, но возращает вам данные в диапазоне и с неким шагом. Он по сути делает query несколько раз каждая step начиная с начала и до конца. Это тот API, который используется для того чтобы рисовать графики. Первый API использует для получения моментальных значений.

![](https://habrastorage.org/webt/nc/cm/cs/nccmcsyofzxubtca8yoyas7p6ag.png)

У нас есть API для получения метаданных. Если мы хотим получить все названия метрик, мы делаем вот такой запрос, где match это массив метрик. Их может быть несколько аргументов, но в данном случае мы передаем тот же самый match, который нам возвращает все.

Второй мета API, который возвращает нам значение всех лейбов. Если мы хотим увидеть список всех job, мы ее вместо label_name пишем job и получаем этот список. Эти API возвращают нам JSON API. 

![](https://habrastorage.org/webt/qe/ku/j4/qekuj4lgm4zp-x07kvhoorux_w0.png)

Есть еще один API, который возвращает нам все метрики самого prometheus в формате, который является нативным для экспортеров. Формат называется expfmt. Это то что вот метрики фигурных скобках пара K/V. В самом prometheus есть Federation API, который вам позволяет сделать такой запрос. Для чего это нужно? Самый простой вариант, если у вас есть какой-то код, который уже работает с expfmt, то вам не нужно его переучивать на то чтобы работать его с каким-то custom JSON API. Этот формат гораздо проще стримится, потому что если у вас JSON где-то на верхнем уровне объекта, чаще всего вам нужно этот объект распарсить целиком. Здесь это можно делать по строчке.

Самое главное это то что это отдельный API. Он работают именно как настоящие export. Вы можете взять и другим prometheus его заскрепить. Это обычный job с обычными параметрами. Вам нужно передать параметр - query url. Если вы сделаете запрос curl, вы получите. Здесь тоже самое. Мы получаем все метрики для текущего значения времени. Единственный нюанс необходимо установить honor_labels для того чтобы prometheus, который будет скрейпить другой prometheus через этот API не перетирал значение job и instance label. Используя этот Federation API вы можете загрузить все данные из одного prometheus в другой. 

![](https://habrastorage.org/webt/64/lc/yy/64lcyynloe47gbkjlelzyddss60.png)

Как это можно использовать? 

Во-первых, надо самое главное сказать что так делать не надо. TSDB оптимизирован для другого режимы работы. Если у вас есть prometheus, которой скрейпит много данных, то он делает большое количество ввода-вывода. Если вы используете Federation API, то количество ввода вывода увеличивется примерно в 2 раза. Есть нюансы. В зависимости от того как часто вы делаете скрейпинг на federate и как часто вы скрейпите таргеты. Если время не меняли, то это действительно увеличивает нагрузку в два раза. Поэтому если вы хотите соскейлить ваш prometheus и включить федерейшн, то вы его убьёте. Нагрузка увеличится два раза.

Второй момент. Вы будете пропускать данные. Вы получите конфликт данных. Почему так? Этот API как почти любой API в prometheus не атомарный. Если придут новые данные, закончится новый скрайп в тот момент когда ваш federate запрос еще идет, вы можете получить для одной time series одни данные, для другой уже новые. Если это не связаные time series это в целом не страшно. Но если у вас summary или гистограмма, которые на уровне expfmt представляется нескольким базовым метриками, то будет между ними несогласованность.

![](https://habrastorage.org/webt/_0/5h/lk/_05hlkpn57b8klhjwborhup4bds.png)

Как мы можем решить эту проблему атомарности? В prometheus есть recording rules, который позволяет создать новую time series из существующей time series. Это можно делать реже. Это один из способов сделать downsampling. Например, скрепите target каждую секунду, но дальше мы хотим сделать агрегацию node_cpu за одну минуту. Группировка в prometheus 2.0 позволяет вам делать эти агрегации последовательно. Правила, которые находятся в одной группе выполняется строго последовательно. В этот момент нет проблемы атомарности, нет проблемы что данные по меняются в процессе. **Но это не решает проблему того что это если допустим какие-то другие данные которые связаны логически с этими но они связаны с точки зрения модели данных.** Чистой атомарности пока нет. Есть открытый issue на эту тему. Можно делать снапшоты. Можно сделать запрос PromQL к базе данных TSDB и из полученных значений отбрасываем все сэмплы, которые меньше какого-то значения времени, в которое началось в evaluation. Это был бы самый простой способ, но пока его не сделали.

Важно понимать что recording rules нужно делать на нижнем prometheus, а не на том, который делает federation. Иначе вы будете пропускать пики, у вас неправильно будет работать мониторинг.

![](https://habrastorage.org/webt/kp/jf/hd/kpjfhdz8boamndunerhcx2vgebq.png)

Как мы можем использовать эту комбинацию этих вещей для того чтобы сделать downsampling и долговременное хранение. 

Первое. Мы просто ставим federation и загружаем все данные с того prometheus. Это странное регулярное выражение похожее на зойдберга - это на самом деле просто двоеточие. Слева и справа от двоеточия звездочка. Мы используем стандартное название для recording rules, которое добавляет двоеточие в середину. При делении оригинального имени слева будет уровень агрегации, а справа функция. В нормальный метрики двоеточия нет. Если есть двоеточие, то это признак того что это агрегация. После этого мы используем это название метрики в нашем графике. Если мы хотим чтобы наш график, наш дашборд в grafana работал и c главным prometheus и c тем кто стоит выше, мы можем использовать выражение **or**. Мы берем либо одну метрику, либо другую, в зависимости которая есть. Мы можем схитрить и при помощи relabeling переименовать новую в метрику старое имя. Этот довольно опасный подход. Можно не правильно написать регулярные вложения и у вас будет конфликт time series. Prometheus будет писать много warning в лог. Вы это увидите, но найти причину может быть довольно сложно. Но если сделать аккуратно, например сгенерировать эти регулярные выражения программно, то это будет работать. Дальше у вас будет обычный дашборд, где используется только node_cpu. В зависимости от того какой prometheus используется, вы будете получать либо исходные данные либо же агрегированные.

![](https://habrastorage.org/webt/hd/ft/1x/hdft1xwgw_61j7tab_3feehhsgy.png)

Как я уже сказал, recording rules можно генерировать довольно просто. Мы просто получаем все  time series через api, которые я уже показывал. Мы создаем правила и эти правила должны использовать правильные функции и операторы. Не нужно использовать там рейд с gauge. Это будет не правильно работать. Нужно использовали только с count. На том уровне где вы работаете у вас может быть не быть информации о типах данных. Например, если вы используете expfmt. Там информация о типах есть. Если JSON API, там этого нет. Как следствие, выражение, которые вы автоматом сгенерируете, может не иметь никакого физического смысла. Поэтому, можно использовать там либо белый список либо черный список. В зависимости от этого генерировать либо нужное вам правило либо выкидывайте те правила, которые не имеют смысла. Есть инструмент promtool, который позволяет вам проверить что те правила, которые вы сгенерировали, тот конфиг, который вы сгенерировали, он имеет смысл. Он имеет корректный синтаксис.

![](https://habrastorage.org/webt/om/wc/fv/omwcfvvh7mc8bky3zkble7vimqm.png)

Если у нас есть grafana и там несколько prometheus, нам нужно знать на какой prometheus отсылать запрос. Как бы нам это сделать?

Один из способов это поставить специальный прокси, который будет смотреть на время в запросе и в зависимости от этого выбирать prometheus. В запросах есть время начала и время конца. В зависимости от этого можно руками делать роутинг. Можно было бы написать какую-то программу, которая это делает. На практике это делается nginx с модулем lua или небольшой программой.

![](https://habrastorage.org/webt/ff/ve/vz/ffvevzimxzpnrjrqirbkuqssiqm.png)

А нужен ли нам вообще API? Можем ли мы работать с TSDB напрямую? Есть нюанс. Во-первых, если мы пытаемся использовать TSDB, который используется prometheus сейчас, мы этого сделать не сможем. Там есть специальный lock файл, который предотвращает это. Если мы напишем код, который будет игнорировать это и будем пытаеться данные читать или писать это мы гарантировано их повредим. При этом даже чтением. Что можно сделать? Можем читать данные через API и создавать TSDB рядом. Дальше prometheus остановить и его TSDB подменить. Но при этом мы можем просдить производительность, если будем читать все данные черех API. Я об этом немного позже скажу.

Второй вариант. Можно скопировать (сделать hot backup) этих файлов, то есть скопировать как есть. Да они будут поврежденные. Когда вы откроете у вас будет warning о том что данные повреждены. Их нужно починить. Вы можете потерять новые данные. Но нам это не важно. Мы хотим downsamping старые данные. Downsamping можно сделать используя PromQL. Но есть нюанс. Его оторвать от prometheus гораздо сложнее чем TSDB. Если вы немного знакомы с Go и c управлением зависимостями, то вендоринг (vendor) PromQL это большая боль. Я бы вам не советовал. По возможности избегайте этого.

![](https://habrastorage.org/webt/2z/ii/fs/2ziifscme6pqm1ppf1n9ripipuq.png)

Переходим к Remote Storage. Кто-нибудь работал с Remote Storage в prometheus? Несколько рук. Remote Storage - это API, которое давно уже существует. Сейчас в версии 2.2 Remote Storage -  помечен как экспериментальный. Более того известность что API Remote Storage точно поменяется. 

Remote Storage позволяет вам работать только с сырыми данными. Там нет никакого PromQL ни на входе ни на выходе. Когда вы читаете, вы не можете использовать всю мощь PromQL. Он по сути выкачивает все данные из Remote Storage, которые соответствуют условию. Дальше PromQL работает уже с ними. это имеет довольно большой overhead. Вам нужно много данных прокачивать по сети. Поэтому в prometheus 2.3, который пока не вышел, но уже это вмержили, будут read hint. Мы чуть позже об этом поговорим.

API для metadata пока что отстутстует. Вы не можете сделать API, которое возвращает все time series из Remote Storage. Если вы сделаете запрос в API у prometheus, то он в Remote Storage не пойдет. Он вернет вам  time series, которые есть в его локальной базе данных. Если у вас локальная база данных отключена, он вернет вам 0. Что может быть немного неожиданно. Сейчас этот API использует ProtoBuf и точно его поменяем на gRPC в будущем. Cейчас это пока что не сделали потому что gRPC требует HTTP2. А у них на практике с ним были проблемы.

![](https://habrastorage.org/webt/lc/7v/ij/lc7vijvha-gtzs_oj74xszwk488.png)

API для записи выглядит вот так. В запросе есть набор лейблов. Набор лейблов как раз уникально индефицирует time series. Name это на самом деле просто лейбл со специальным именем. А сэмплы это набор времени и значения - два float64. При записи порядок неважен. Предполагается что база данных, которая это пишет в себя сама все сделает правильно. Prometheus сможет сделать некоторую оптимизацию и не сортировать это лишний раз. Соответственно запрос на запись это просто несколько time series.

![](https://habrastorage.org/webt/xd/qr/ud/xdqrudjh5lpomcepkdmkjcfegt0.png)

У конфигурации на запись довольно гибкая большая конфигурация. Там много параметров для конфигурации параллелизма записи. То что prometheus шардингом - это по сути конкурентные запросы. Можно ограничить максимальное количество сэмплов в одном запросе, максимум параллельных запросов, timeout, как повторять, какой backoff. Для многих баз данных 100 сэмплов за раз это может быть очень мало. Если вы используем clickhouse как используем мы, то конечно значение надо сильно увеличивать. Иначе это будет очень неэфективно.

![](https://habrastorage.org/webt/eb/gk/ew/ebgkewr7k-i2yho9mh7dbogcipm.png)

Remote read API выглядит вот так. Это просто диапазон по времени от начала до конца и набор match.

![](https://habrastorage.org/webt/hv/kn/jo/hvknjowgf6h9pebjvvhbspe7jac.png)

Match это по сути набор пар name и value - обычный лейбл и тип значений. В сравнении есть равенства, неравенства или регулярное выражение. Тип значений это обычный match, который вместе видите в PromQL. Никаких функций здесь нет.

![](https://habrastorage.org/webt/ri/hu/ap/rihuap_py_s_hrp0oywqnyx6yee.png)

Ответ это несколько time series, которые соответствуют этому запросу. Здесь сэмплы должны быть отсортированы по времени. опять же это помогает prometheus немного сэкономить cpu - не нужно сортировать. Но предполагается что ваша база данных должна это делать. В большинстве случаев и так будет, потому что скорее всего там будет индекс по времени.

![](https://habrastorage.org/webt/zu/yd/fx/zuydfxti6mozpsz5v60f_i1laic.png)

В prometheus 2.3 появились read hint. Что это такое? Это возможность подсказать prometheus какая внутренняя функция, которая работает с time series, которая запрашивается будет преминина. Это может быть либо функция либо оператор агрегации. Это может быть rate. То есть это назыается func, но на самом деле это может быть sum, который с точки зрения PromQL на самом деле совсем не функция. Это оператор. И шаг. На предыдущем примере там был rate 1 минута. Здесь rate это будет функция и одна минута в миллисекундах как шаг. Этот hint он может игнорироваться ремонт базой данных. При этом в ответе нет никакого признака игнорировался или нет.

![](https://habrastorage.org/webt/6_/ms/9_/6_ms9_zh9txqwdafxjoenbdn8so.png)

Какая конфигурация у read? 

Во-первых, есть такая конфигурация required_matchers. Это позволяет вам отсылать запрос на Remote Storage, которые соответствуют выражению. Чтобы читать агрегированные данные из Remote Storage, необходимо использовать запрос, в составе которого присутстует двоеточие. 

Есть опция, которая позволяет вам читать или не читать недавние данные из Remote Storage, которые есть в TSDB. Обычно в стандартной конфигурации есть небольшая локальная TSDB, которая пишется на локальный диск. Она там хранить несколько часов или несколько дней. Данные, которые используете сейчас, которые используется для оповещения, которые используются для построения dashboard, читаются только из локального TSDB. Он быстрый, не позволяет нам хранить очень много данных.

Старые исторические данные будут читаться из Remote Storage. Это дает понять как локальный Storage и Remote Storage между собой связываются. Отсутсвует какая-либо дедупликация.

По сути что происходит. Данные берутся из локального Storage, данные берутся из Remote Storage, если read_recent включен. Они просто сливаются вместе. Казалось бы это не проблема. Если предполагается что мы недавние данные никак не downsamping, это точно те же данные, они полностью совпадают с локальными данными, у нас будет в два раза больше семплов, но ни какие функции не должны влиять. На самом деле нет. Есть функция irate() и парная ей для gauge, которая возвращает нам разницу между двумя последними значениями. Она заглядывает на указанный диапазон назад во времени, но при этом использует только два последних значения. Если у нас два последних значения имеют одинаковое время, то разница будет ноль. Это баг и практически невозможно найти это. Это починили буквально четыре дня назад. Bот [ticket](https://github.com/prometheus/prometheus/issues/4184) кому интересно.

![](https://habrastorage.org/webt/-q/3w/dw/-q3wdw3czygpvkk009lrnhumska.png)

Интересно что remote read реализуется самим prometheus начиная с версии 1.8. Именно тот способ, который позволяет вычитывать данные старого prometheus, когда вы делаете миграцию на версию 2.x. Официальный способ советует подключать его как remote read. Данные будут вычитываться по мере необходимости.

Remote read можно использовать для того чтобы делать query роутинг без прокси. На одном из предыдущих слайдов я показывал что в зависимости от времени, мы можем делать роутинг на один prometheus или другой. Точно так же можем этого избежать. Просто подключаем тот prometheus, который стоит ниже как remote read и данные будут читаться оттуда. Но есть поправка на то что конечно много данных будет перекачиваться. Особенно если вы не используете query hint.

![](https://habrastorage.org/webt/gq/op/fz/gqopfzpgoe3kidpqufnedn5gfrk.png)

Почему Clickhouse?

- Для нашего исследовательского решения мы выбрали привлекли clickhouse, потому что мы на него уже давно смотрели. У нас есть люди, которые постоянно занимается перфомансом баз данных, постоянно проверяют новые базы данных. Наша компания занимается opensource базами данных.
- Нам очень нравится его сырая производительность. Его мощность в пересчете на CPU, время очень хорошее. Большинство подобных систем говорят про бесконечную машстабируемость, но мало говорят про эффективность для 1 сервера. Многих наши клиенты хранят метрики на паре серверов.
- Встроенная репликация, шардирование.

- GraphiteMergeTree - это специальный движок для хранения данных графита. Нас он в начале очень сильно заинтересовал.


![](https://habrastorage.org/webt/ot/nk/sn/otnksno5uhhcgn7j5-fp5xnxneq.png)

Движок предназначен для rollup (прореживания и агрегирования/усреднения) данных Graphite. Он может быть интересен разработчикам, которые хотят использовать ClickHouse как хранилище данных для Graphite.

идем читаем открываем документацию смотрим буквально прочитаю назначена для лап прорезывания и агрегации усреднения данных графит графит хранит в crack house полные данные и получать их может и дальше там написано что с прореживанием используется графит мышцы без поражений использоваться мар 4 обычно то есть все не такое что данные хранятся всегда полный на место они не переписываются это просто оптимизация чтение но в целом это неплохо да то есть как бы когда мы делаем числе не ем и не вы качаемся данные они автоматически корректируется он получаем мало это хорошо - для нас тоже данных нет со всех я готовлюсь в начале месяца к докладу вот кто то заходите грамм чатик спрашивает графит марсе данный дом запрет нет я уже пишу станет цветным документации написано что нет то другой отвечает да да он сам придти по нужно optima использовать отлично короче запускаю проверяю да правда здорово там документации по сути пока потом слазил в год проверил доказать совместимая с есть а прямо из файлов office file как раз изначально создавался именно для графе творчестве то есть на самом деле dancing он делает но его надо вызвать руками то есть автоматом или нет или у людей не работать и не знаю 

но к сожалению мы все-таки даже в этот момент когда мы это уже обнаружили ну во-первых был уже поздновато конечно ты начала месяца то есть это был месяц назад но как бы графит хранит у нее другая модель данных то есть по сути название метрики там нет у него лейблов и эффективно запихать вот это все в названии метрик не очень хорошо получается такая проблема то что название только у него хранится одна таблица название метрик имеет разную длину это полет потому что если мы делаем поиск как contain экспо название метрики из того что длина разные этот индекс ним будет так эффективен как если бы это был и значение фиксированной длины ну потому что вам нужно делать поиск по файлу да и дальше нельзя точно указать q должна приземлиться для того чтоб на pastel что здесь получили чтобы такой 

![](https://habrastorage.org/webt/c3/v4/zf/c3v4zfriducmhoahjwujmp1zdga.png)

вот поэтому сделала свою собственную схему вот так это показывается как у нас хранятся там селес в базе данных дейт которая нужна к хаосу просто всегда есть некий fingerprint вот если вы когда нибудь смотрели исходники prometheus или тесты бы вы знаете что fingerprint это по сути короткое быстро чик сумма полного названия time series то есть комбинация всех лейблов ключей и значений и опять же им это просто-напросто обычный лейбл вот это checksum она быстро мы использует тот же сам алгоритм для совместимости никакого особого смысла в этом не было но если что-то де бо жить так далее то это может быть удобно совпадает можно принять в тсд быв нашему ставлю же что они одинаково вот лыбу все хранятся в таком специальном g sonic который позволяет к хаусу работать с ним вот его с нашими функциями то есть это компактный джейсон без пробелов там с немножко упрощенного скальпингом и так далее на самом деле это таблица реально во время работа она не используется она она всегда хранится в памяти нашего собственно решения которой оказаться house вот и она использовать только в тот момент когда мы запускаем сервер для того чтобы узнать у нас там series есть она учитывается и потом по мере того как новый там сели с приходит мы их туда записываем и всей несколько месяцев про хауса могут читать одну и ту же таблицу и соответственно для place and на 4 и говорит нам о том что эти там heroes есть несколько разных и став пишет одну и ту же там сервиса не с манджи циника вы проблемы здесь не будет 

![](https://habrastorage.org/webt/ms/lg/1t/mslg1tnbc97nrlauwcr9ekj-p3k.png)

вот сэмплы мы храним в отдельной таблице очень эффективны при значении эффективной длины это fingerprint тот же самый время и значения а то у вас получается как бы двадцать четыре байта на sample но на ином режиме да и она имеет строго фиксирую длину каждая колонка хранится отдельно при этом поиск по fingerprint у эффективен и потому что мы точно знаем что размер фиксированный да нет нет такой проблемы как скрипит на 4 когда эта строка вот мы используем красный противника на это в общем-то не очень важный и первичный индекс нас по fingerprint у и по времени 

вот но до двадцать четыре байта это такой наивный на самом деле вам довольно хорошо сжимается и поэтому по факту сильно сильный меньше места есть наших последних тестах там примерно степень компрессии 1 к 40 2 и 2 4 байта сильно сильно меньше

![](https://habrastorage.org/webt/wu/5y/24/wu5y2499vjvrcm37ct1gudfrpym.png)

вот соответственно как можем сделать ручной дал сэмберг если у нас график март 4 как бы вроде есть коктейли выяснилось новыми не совсем то что нам хочется так мы сделали когда еще не знали что он вообще есть по сути мы можем сделать эту руками да то есть как раньше делать родирование проецирование тогда когда то ничего странного не был просто берем делаем руками новую таблица заранее выбираем таблицу заранее судом и когда к нам приходит сэмпл по времени определяем какую таблицу пишем 

дальше соответственно выбираем по времени и запроса из какой таблицы читай при этом если это происходит на границе читаем несколько таблиц и дальше можем эти данные вот казалось бы можно было бы использовать для этого до сделать какой-то view для несколько таблиц которые позволяют это читаете просто одним запросом не отделаться руками нов кликов есть бак о том что предикат ice view не подставляются запросы поэтому если делается в русскую он где-то на все таблицы что нам не хотелось поэтому даже view ним не можем использовать

вот соответственно на практике это как просто ну и как мы делаем но у самок мы задаем временно таблицу копируем из нее данные просто по сути entered in the select да опять же используя правильные функцию что важно потому что если опять же чтобы имела физический смысл и так далее

дальше делаем марина им который атомарный под глобальным лаком то есть мы переименовываем существующая в таблицу старую новую существующие дальше драпаем старую таблицу все тем самым у нас данные за сто сорок восьмой день для примеру да это типа день года это сегодня кстати вот то есть данные у нас в эту таблицу уже за дом , здесь проблема на свои очень большая вода тут insert into это так красиво выглядит на самом деле нам нужно же применить правильные функции потом тайна агрегации сделал так далее на практике это не получается сделать там под ним большим запрос или даже несколькими большими запросами это приходится делать из кода . посылает довольно большое количество небольших запросов мы по-максимуму старались это сделать нашими запросами комбинирует тогда митя но это не очень эффективно получается сожалению тамблинг данных одного дня ну пока что грублю занимает меньше дня и на чем вообще смысл не был но в зависимость это качество данных может занимать до говорю 

![](https://habrastorage.org/webt/7c/94/h4/7c94h4t2bbvk5x-3acoftonomxa.png)

что будет лучше сегодня утром был митап папе хаосу который продлился вместо часа ника что достойный даже три и народ потом выгнал нас всех из ком-то докладчиков и там сидели вот соответственно там будет апдейты делита дели ты уже в менеджере как говорят вот только что первую версию уже в мир джоли и соответственно там дели ты будут как пообещали до конца недели сделаны что прекрасно соответственно если будет работать обретут роба 1019 схемами да он самый данных может довольно сильно упростится это приятно от 

во вторых есть задача про то что сделать кастомные сжатие от типа дельта дельта v дельта как раз то что делает sdb то что хорошо подходит для time сидя с данных да то есть необычное и там не знаю активация когда и универсально работы для всего одинаково хорошо или одинаково плохо а именно специально это очень полезно потому что там особенно если мы будем иметь возможность выбирать тип компрессии зависимости от типов данных типа то есть аккаунт автору только растет для этого удалите компрессии или галч который колеблется вокруг причины поэтому соответственно в дельте хороший работает 

вот и соответственно будут другие доклада про клика установите в частности завтра будет доклад алексея он как раз расскажет про планы кать-кать что еще там будет такого хороший много из этого мы конечно будем рады использовать 

![](https://habrastorage.org/webt/od/mv/hx/odmvhxcnnite5wum9k9gwfifbec.png)

вот есть другие стороны джейн которые работу это видимо буду уже ускоряться немного вот есть яндекс baby который работает из коробки его принято ругать за скорость но то что работая с коробки вам ничего не нужно делать это сабли хорошо

есть опыт тест db и графит каору только на запись стандартный адаптер из prometheus а у меня слабо работают

есть крае тебе который я так моя приторный баз данных а может есть time scale деби который ford под колеса для темпе лес баз данных говорят работает неплохо но сами мы не пробовали 

есть cortex который также был здесь как проект франкенштейн вот это очень хорошо его описывать это ребята пытаются сделать решение на основе федерации пролечился при этом они хранят данные vs3 ну может быть я не знаю то

 есть вполне может весь то нас точно кто-нибудь спросил поэтому я решил не ждать этого вопроса сразу добавил слайд у него очень интересная архитектура то есть есть prometheus который использует локальный тест db делается между ними кластер и рядом с каждым play prometheus он ставится специальный сайт карл который по риму три терема утра этапе принимает запросы и соответственно продает их prometheus и parameters можете использовать его ремоут риски могут райт дальше эти все свои карты соединенные между собой и между мастером кастомным пиаре которые через записи там есть репликация есть перетрудил не таяли так если вам кажется что это сложно это сложно это правда сложно то есть там сложной архитектуры я так говорю но вот то что я описал это реально вот она довольно сыровато то есть пока когда это запускал где-то там не знаю пару месяцев назад она разваливалась у пинка когда запускалась пока что ну пока сыру но как как мы знаем до нас вернется и как бы в девятнадцатом году следующем году вместе с новыми стилями да можно будет на нас попробовать а пока просыпаемся в пыль 

![](https://habrastorage.org/webt/a9/rc/zm/a9rczmkse4viit4vvr4hgqfcuts.png)

нагрузочное тестирование в чем проблем по модель много данных не записать нам либо просто запустили и и ждем и ждем и год ждем пока количество данных наберем или пытаемся как-то их туда запихивать

вот в прометее все есть ли могут ли встроенный но нет remove the right поэтому локальный тсд б взять то записать много не получится вот 

вторая проблема если мы генерим какие-то данные нагрузочные да не так что просто ждать то они часто могут быть слишком хорош слишком хорошо жмутся здесь например если мы берем существующие данные простыни рим там сотни 100 совфед одинаковые данные там коэффициент сжатия будет такой прекрасный что в реальности они случаются и метрики будут числа будет нереально

![](https://habrastorage.org/webt/fy/mc/uk/fymcukkdoo3hi78gkw1eb2ipvsk.png)

написали fake экспорт который выглядит как обычный exports который проведет сможет скрепить когда приходится crate он идет на какой-то апстримом ский экспорта какого мы используем нотекс паттернов это не очень важно пойдёт с ним даны видите тут много инстансов то есть ну там допустим 100 это делам и на выходе получаем 100 и немножко данные меняет плюс минус 10 процентов для контуров и гоу джей гистограммы и сам или мы к сожалению пока с вами работать нормально не можем потому что он не очень понятно как и вот не меняет простые значение тип 01 почему потому что если есть метрика об которые или да или нет и очень понятно что означает 098 вот так немножко работает вот и не меняем целые числа на вещественной наоборот но в общем по такой же логике если у нас есть часть запросов и это не рейд какой-то несёт такой то вряд ли имеет смысл его делает в честно вот мы просто дает данные вас в обычном спасёшь формате вот 1 ст метро инструментов 

![](https://habrastorage.org/webt/c0/ja/hc/c0jahc5ryn-oxhwxnke_vaj1mia.png)

инструмент пара mode который грузит данные там либо из там ну из разных мест да то есть он может читать из файлов своем формате можете зримого 3d может из-за какого-то экспорта рассчитай и соответственно пишет в разных форматов там в том числе вдохнул если мы хотим погрузить постить именно как чтение быстро работает вот и сейчас это фокус на скорость инструмент нагрузочного тестирования для не только для пром хаоса вообще для любого решения кто-то использует ему три ты ли использовать parameters 

![](https://habrastorage.org/webt/qr/xa/-2/qrxa-2wg58troaskuqhqowwicq4.png)

вот но мы хотим сделать немножко расшили да мы хотим добавить кошерные чтения потому что в наших тестах часто узким местом был им нафиг экспортер который но довольно долгий данные и в целом мы могли бы их кэшировать пусть они будут там не реально хорошие дома зато вам мы не будем тормозить нам не нужно быть днями ждать на глубину тестируем какая-то фильтрация на лету к это модификация на лету то что делает треках спортом и видимо это придется скопировать фронту потому from out потому что иначе слишком долго получается и найденные поддержка тсд b для того чтобы именно работать с файлами с баз данных на диске не через api через api да и 

фокус на аккуратность для миграционных такой история я дерусь премиум дима положил просто как я взял подключился и запивал чо получил все метрики если вы делаете это на нем способом то есть дайте мне список всех метрик приведешь что делает он открывает sdb поднимаю все там сели с диска поднимает индексы дальше лезет в чат файлы по евро что они реальны и так далее поэтому он все может просто лечь вот и как бы на южный подход туда просто взять как бы все time series и начинает там со старых данных до новых читать до в этом он там лежит вам нужно сделать наоборот нужно сначала получить список аккуратно как-то вот там не за этом допустим несколькими запрос мысль являются выражением там да не считаем сирийскую получается на а потом дай мне считаем сервис означается на b и так далее и дальше грузить их именно по метрикам не по времени что нелогично но так это работает вот но это просто от нюанс если вы будете делать что-то такое подобно если увидите что вас там он случился вы будете значит этой за вас я вот так узнал 

![](https://habrastorage.org/webt/mz/z9/kh/mzz9khnykox-3r2wlp_lpgphil4.png)

вот внезапно ключ на тестирование самое главное графики и так далее их не будет вот потому что как я уже говорил накручены тестера не занимает много времени и к сожалению из-за ошибки конфигурации в насоса крошилась и поэтому результаты не получился 

вот в блоге перк он и мы напишем когда это все будет 

но если так псевдо научно да без графиков так далее как она была запись была линейно что странно ну окей ладно ни странно но поначалу было неожиданно когда данных нет запись была довольно медленно откуда много данных скорость записи практически не изменилась очищение вот довольно неплохо скакала вот и в целом было не очень быстрое устройство на мне очень важно и чтение текущих данных до которые можно там через кенты можно ускорить или trees and тоже включить все будет хорошо а для старых данных это нормально работает

![](https://habrastorage.org/webt/b6/7f/4w/b67f4wmgkuzi6zyqnitxlwuv2t8.png) 

выводы люди хотят долговременны хранилища с такой спрос есть мы делали доклад программ house на пром кони там это прямо было очень горячие темы это нас тоже там активно развивается поэтому видно что все этого хотят

 это уже возможно сейчас то есть если это решение есть теперь есть какие-то интеграции но все это нужно дорабатывать напильником вот пока ничего такого к сожалению production соединить нету 

![](https://habrastorage.org/webt/ms/qk/ec/msqkechijgscjucb7sx8knkzwx0.png)

ссылки где посмотреть это смотри по истории нашего прав хаоса это тон откуда она скорее все приедет вот но там любом случае будет интеллект потом почему потому сейчас в одном репозиторий несколько разных вещей не очень тесно связанных между собой поэтому нужно будет их поносить 

вот и соответственно в нашем блоге будет информация про performance но вообще какие-то новости все спасибо

вопросам

раз-два-три спасибо спасибо леса доклад мне вопрос есть по поводу рефлюкс baby не проверяли вы вот эти знаменитые слухи про то что friends говно если китайцев ты 

но царей он был действительно очень хороший он сильно силу лучше стал то есть все эти байки про то что influx там медленные разваливать стали они все просто рвемся не знаете подняли денег подняли людей которые умеют писать баз данных и в целом текущая версия работает стабильно я бы не сказал что она прям супер супер быстро работать стабильно плюс influx baby на мой взгляд 

во первых то что не нужно делать что-то рядом потому что она работает из коробки во вторых как и не знаю как река у сада как какие-то других решений на основе базу данных но нет сдпр вы можете использовать язык запросов когда вам более знаком то есть influx кий язык запросу похож на сквере достаточно для того чтобы на нем можно было делать аналитику который на промке ель делал сложно вот а если вы там используя томский где пятна вообще настоящий стиль вот нос перфоманса но последних чисел меня к сожалению нет я знаю что по стабильности лучше стал не могу сказать 

еще вопрос графический движок только на запись получается получать если мы хотим пока графики показывать нужно графону настраивать на графит что показывают и долговременное хранилище 

да да то есть то интеграция тот не про графит mach3 а именно про интеграция которая есть сам управитель зато такой на запись соответственно он только пишется дальше либо из grafana выходите в графит либо еще факты

он теряет по моему лейбл на при этом куда пишут там есть конфигурация которая говорит как бы что с ними делаете либо с как и вставляете либо куда вставлять либо кидай да но в общем это настраивается но на самом деле там очень мало коды по большому счету если нужно там это как-то память от легко steel 

но выявит еще рассказывал недавно что они планируют перед свое решение для записей с правительством графит ну я могу добавить что чатик это церковь метрик скорее 

да telegramе да это было неожиданно

спасибо вообщем спасибо за доклад немножко не тот слайд ну суть какая вопроса там был вывод что записи все хорошо вот на сервер долговременного хранения хотелось бы спросить насколько хорошо есть у вас такая информация может быть вы работали с вашими клиентами там да да вот вот запись там линейно примеру там миллион метрик летят там пятиминутки там или 15 минут и что нам хватит ли нам там raid 6 sata дисков ленты как 

сколько вы храните ходить по времени 

примеру там год 

смотрите то есть у нас если три да я понимаю м м у нас основной интервал скрейпинг это на секунду то есть мы больше часть метрик никак 30 секундах было в докладе в главном зале у нас каждую секунду вот при этом был сэм link мы делаем там начиная помощи 14 дня и делаем там до 1 минуты и второй шаг мы пока ещё ни разу не делали то есть мы не агрегирует данные по одной минуте еще меньше вот соответственно это занимало там загрузим тестирует максимум сколько мы находились сотни гигабайт не было колебания скорости записи 

ну то есть я данных погиб сам там нет 

я говорил то есть это вот нижняя часть да это такая псевдонаучная почему потому что поэтому и спрашиваю жопу 

да да вот наш опыт пока ничего плохого и не заметил но конечно нам бы хотелось завести над нормально тестируем до конца и патологию описать результаты сделать графики оформить уже спасибо

спасибо а мне еще быть небольшой вопрос возник если мы пишем вин flux то мы тогда запрашиваем tool к дому долговременных данные за не флюкса 

кроме понимаешь что очень хочет fx зайдешь где не да но это зависит от 30 см на самом деле то есть это точно так же как и с любым другим ремонт ничего уникального для инструкций от зависимости от настройки если или trees and включен тогда будем читать

 потому что вот мы берем допустим для оперативных данных prometheus и долговременно пишем в influx мы тогда граф она тоже самое настраиваем на prometheus и prometheus может вытаскивать и то что оперативно это что записал винкс так провести то и другое то но и он будет попробовали вытаскивать до править уже выходить в includes вытаскивать данному протоколу и рисовать 

да и при этом в текущей релизной версии не будет делать дедупликации у вас будет проблема которые я вот сайтом