**Здесь переводится из видео и пишется статья Алексея Вахова из Учи.ру "Облака в облаках, опыт Учи.ру"**

Ссылка на видеодоклад 
https://www.youtube.com/watch?v=C7utdhh6UCk

**Текст сделан из субтитров. Буду очень благодарен в форматировании текста, его вычитки. Для этого достаточно знать русский язык.
Присылайте свои pull request или присылайте текст на почту patsev.anton[собака]gmail.com**



Как раз об этом расшифровка доклада Алексея Вахова из компании Учи.ру. "Облака в облаках, опыт Учи.ру" на DevOpsDays Moscow 2018. В докладе: использование Ansible, Terraform, хранение в Git, ревью, пересборка, финансовые выгоды, горизонтальное масштабирование в 1 клик.

https://www.youtube.com/watch?v=C7utdhh6UCk

Меня зовут Алексей Вахов. Я работаю техническим директором в компании Учи.ру. Мы хостимся в публичных облаках. Активно используем Terraform, Ansible. С тех пор мы полностью перешли на Docker. Очень довольны. Насколько довольны, как мы довольны буду рассказывать.

Компания Учи.ру занимается производством продуктов для школьного образования. У нас есть основная платформа, на которой дети решают интерактивные задачи по разным предметам в России, в Бразилии, в США. Мы проводим онлайн олимпиады, конкурсы, кружки, лагеря. С каждым годом эта активность растет.

С точки зрения инженерии классический Web Stack (Ruby, Python, NodeJS, Redis, ELK, PostgreSQL. Основная особенность что много приложений. Приложения размещаются в 5 облаках по всему миру. Каждый день идут выкатки в production. 

Вторая особенность у нас очень часто меняются схемы. Просят поднять новое приложение, старое остановить, добавить Cron для background jobs. Каждые 2 недели идет новая олимпиада - это новое приложение. Это все нужно сопровождать, мониторить, быкапить. Поэтому окружение супердинамическое. эта динамичность это наш основной вызов.

Рабочей единицей у нас является площадка. В терминах облачных провайдеров это Project. Наша площадка это полностью изолированная сущность с API и приватной подсетью. Когда мы заходим в страну, мы ищем местные облачные провайдеры. Не везде есть Google и Amazon. Иногда бывают что отсутствует API к облачну провайдеру. Наружу публикуем VPN и HTTP, HTTPS на балансеры. Все остальные сервисы общаются внутри облака. 

Под каждую площадку у нас создан свой Ansible репозитарий. В репозитории есть hosts.yml, playbook, роли и 3 тайных папки, про которые дальше буду рассказывать. Это terraform, provision, routing. Мы фанаты стандартизации. У нас репозитарий должен всегда называться "ansible-имя площадки". Каждое название файла, внутренняя структура мы стандартизируем. Чтобы потом  накручивать крутую автоматизацию.

Terraform полтора года назад настроили, так им и пользуемся. Terraform без модулей, без  файловой структуры (используется плоская структура). Файловая структура terraform: 1 сервер - 1 файл. В terraform используем настройки сети, dns-записей, дисков, сетей. Terraform на площадке полностью готовит железо.

Ansible. Из-за того что у нас везде  используется одна и таже версия операционной системы, то мы все роли написали с нуля. В интернете обычно публикуются Ansible роли под все операционные системы, которые не работают ни нигде. Мы все взяли Ansible роли и оставили только то что нам нужно. Стандартизировали Ansible роли. У нас 6 базовых playbook. При запуске Ansible устанавливает стандартный список ПО: OpenVPN, PostgreSQL, Nginx, Docker. Kubernetes мы не используем.

Мы используем Consul + Nomad. Это очень простые программы. Запускаются 2 программы, написанные на Golang на каждом сервере. Consul отвечает за service Discovery, health check и key-value для хранения конфигурации. Nomad отвечает за scheduling, за выкатку. Nomad поддерживает rolling deploy health check. Nomad останавливает старую версию приложения в кластере, поднимает новоую версию приложения в кластере. Nomad поддерживает распределенный Cron.

После того как мы зашли в площадку, Ansible выполняет playbook, расположенные в директории provision. Playbook в этой директории отвечают за установку программного обеспечения в docker кластер, который используют администраторы. Устанавливаются prometheus, grafana и тайный софт shaman. 

Shaman это Web-dashboard для nomad. Nomad низкоуровневой и к нему пускать разработчиков не очень хочется. В shaman видим список приложений, разработчикам выдаем кнопку деплоя приложений. Разработчики могут менять конфигурации: добавлять контейнеры,  переменные окружения, заводить сервисы.

В завершении, Ansible выполняет playbook, расположенные в директории routing. В кластер мы выкатываем свой routing. Routing сделан на nginx + consul template. Используется nginx без дополнительных модулей, у который время от времени меняется конфиг. 

Таким образом каждая площадка состоит из 5 слоев. Мы достаточно долго прорабатывали слои, чтобы они были независимы друг от друга. Applications содержит приложения пользователей и приложения администраторов. Provision, routing, apps полностью на 100% совпадают везде независимо от облака. Если ИТ-специалисты переключается с проекта на проект, то попадают в полностью типовое окружение. В ansible мы не смогли сделать идентичными настройки firewall, VPN для разных облачных провайдеров. С сетью все облачные провайдеры работают по-разному.  Terraform везде свой, потому что он содержит специфичные конструкции для каждого облачного провайдера.

У нас 14 production площадок. Возникает вопрос: как ими управлять? Мы сделали 15 мастер площадку, в которую пускаем только админов. Она работает по схеме федерации. Идею взяли из prometheus. В prometheus есть режим, когда в каждой площадке устанавливаем prometheus. Prometheus публикуем наружу через HTTPS basic auth авторизацию. Prometheus мастер забирает только нужные метрики c удаленных prometheus. Это дает возможность сравнить метрики приложения в разных облаках, найти самые загруженные или незагруженные приложения. Централизованное оповещение (alerting) идет через мастер prometheus для админов. Разработчики получают оповещения от локальных prometheus.

По такой же схеме настроен shaman. Через главную площадку администраторы могут деплоить, конфигурировать на любой площадке через единый интерфейс. Достаточно большой класс задач решаем не выходя с этой мастер площадки.

Расскажу, как мы переходили на docker. Этот процесс очень небыстрый. Мы переходили примерно 10 месяцев. Летом 2017 года у нас было 0 контейнеров production. В апреле 2018 мы докеризировали и выкатили в production последнее наше приложение.

Мы из мира ruby on rails. Раньше было Ruby on Rails 99% приложений.  Rails выкатывается через Capistrano. Технически Capistrano работает он следующим образом: разработчик набирает cap deploy, capistrano идет по application серверам по ssh, забирает последнюю версию кода, собирает asset, миграции бд, делает делает simlink и посылает сигнал на Unicorn Puma. Веб-сервер видит новый новый код.

Последний шаг в docker так не делается. В docker нужно старый контейнер остановить, новый контейнер поднять. Тут возникает вопрос: как переключать трафик? В облачном мире за это отвечает service discovery. Поэтому мы на каждую площадку добавили consul. Consul добавили потому что использовали Terraform. Все конфиги nginx мы завернули в consul template. Формально то же самое, но уже мы были готовы динамически управлять трафиком внутри площадок.

Следующий этап. Шаги по умолчанию в capistrano заменили на следующие: capistrano заходил на мастер ноду, собирал docker образ, отправлял его в docker registry на площадке, заходил по ssh на сервера, забирал этот образ, останавливал старую версию, запускал новую версию docker образа. Фактически это был ручной scheduling с помощью скрипта. Потом поняли что это тупиковый способ. Cкрипт увеличился до 600 строк. 

Решение: установили nomad рядом с consul. В итоге разработчики также вызывали cap deploy. Cap deploy уже выкатывал на кластер. 

Дальше отлаженную схему с capistrano добавили в docker. Это схему реализовали в виде веб-сервера. У разработчиков отняли ssh доступ, выдали зеленые кнопки деплоя. На каждом этапе использовали весь стек целиком.

Выкатки. Все инструкции выкатывают в docker 10 nginx либо 10 redis. Это плохой пример, потому что образы уже собраны, образы легкие. Мы запаковали наши rails приложения в docker. Размер docker образов был 2-3 гигабайта. Они выкатыаются не так быстро.

Вторая проблема пришла от хипстерского веба. Хипстерский веб это всегда Github Flow. В 2011 году был эпохальный был пост что Github Flow рулит, так весь веб и катится. Как это выглядит? Master ветка всегда production. При добавлении нового функционала делаем ветку. При мерже делаем code-review, запускаем тесты, поднимает staging окружение. Бизнес смотрит staging окружение. В момент Х если все успешно, то мы мержим ветку в master и выкатываем в production.

На capistrano это работало отлично, потому что он для этого и был создан. Docker нам всегда продает pipeline. Cобрали контейнер. Контейнер можно передать разработчику, тестеру, передать в production. Но в момент мержа в master код уже другой. Все docker образа, которые собирали из feature-ветки, они собраны не из master. 

Как мы сделали? Собираем образ, кладем его в локальный docker registry. И после этого делаем  остальные операции: миграции, деплоим в production. Чтобы быстро этот образ собрать мы используем  Docker-in-Docker. В интернете все пишут что это анти-паттерн, он крешится. У нас ничего подобного не было. Сколько уже работает с ним никогда проблем не было. Директорию /var/lib/docker мы пробрасываем на основной сервер, ипользуя Persistent volume. Все промежуточные образы лежат на основном сервере. Сборка нового образа укладывается за  несколько минут. 

Для каждого приложения делаем локальный внутренний docker registry и свой build volume. Потому что docker сохраняет все слои на диске и их сложно чистить. Сейчас мы знаем дисковую утилизацию кадого локального docker registry. Мы знаем сколько он диска требуют. Можно получать оповещения через централизованную Grafana и чистить. Пока мы их руками чистим. Но мы будем это автоматизировать. 

Еще один момент. Docker образ собрали. Теперь этот образ нужно разложить по серверам. При копировании большого docker образа не справляется сеть. В обалке у нас 1Gbit/s. В облаке происходит глобальный затык. Сейчас мы деплоим docker образ на 4 тяжелых production сервера. На графике видно диск работал на 1 пачке серверов.  Затем деплоится вторая пачка серверов. Снизу видно утилизацию канала. Примерно 1 Gbit/s мы почти вытягиваем. Больше там особо особо уже не ускорить.

Мой любимый production это Южная Африка. Там очень дорогое и медленное железо. В четыре раза дороже, чем в России. Там очень плохой интернет. Интернет модемного уровня, но не глючный. Там мы выкатываемприложения за 40 минут с учетом тюнига кешей, параметром таймаутов.

Последняя проблема, которая меня волновала перед тем как связывались Docker - это нагрузка. На самом деле нагрузка такая же как и без докера с идентичным железом. Единственный нюанс мы уперлись всего в одну точку. Если из Docker engine собирать логи через встроенный fluentd драйвер, то на нагрузке около 1000 rps/s начинался замусориваться внутренний буфер fluentd и запросы начинают тормозить. Мы вынесли логирование в sidecar контейнеры. В nomad это называется log-shipper. Рядом с большим контейнером приложения висит небольшой контейнер. Единственная задача его забирать логи и отправлять в централизованное хранилище.

Какие были проблемы/решения/вызовы. Я попытался проанализировать какая была задача. Особенности нашей проблематики это: 

- много независимых приложений
- постоянные изменения схемы инфраструктуры
- Github flow и большие docker образы 

Наши решения

- Федерация докер кластеров. С точки зрения управляемости тяжело. Но docker хорош с точки зрения выкатки бизнес функционала в production. Мы работаем с персональными данными и нас в каждой стране сертификация. В изолированный площадки такую сертификацию легко проходить. При сертификации все возникают вопросы: где вы хоститесь, как у вас облачный провайдер, где вы храните персональные данные, куда бекапите, кто имеет доступ данным. Когда все изолировано, то круг подозреваемых описать гораздо легче и следить за всем этим гораздо легче.
- Оркестрация. Понятно что kubernetes он везде. Но хочу сказать что Consul + Nomad это вполне production решение. 
- Сборка образов. Быстро собирать образы в Docker-in-Docker можно. 
- При использовании Docker держать нагрузку 1000 rps тоже можно.

Вектор направления развития

Сейчас одна из больших проблем рассинхронизация версий программного обеспечения на площадках. Раньше мы настраивали сервера руками. Потом мы стали devops-инженерами. Теперь настраиваем сервера с помощью ansible. Cейчас у нас тотальная унификация, стандартизация. Внедряем обычное мышление в голову. У нас нельзя поправить PostgreSQL руками на сервере. Если нужна какая-то тонкая настройка именно на 1 сервер, то думаем как эту настройку распространить везде. Если не стандартизировать, то будет обычный зоопарк настроек. не знаю как вы назвать соответственно поэтому у нас есть еще там куча безумных идей как бы это далеко не конец вот наверное не именно не хотел углубляться прям куда-то в космосе там на самом деле когда вот мы разбирались мы иногда там закапывались успеть до отладки g-кода ну кто уже документациях читали в основу на самом деле вот но там был цикл следующий то есть вот мы настроили эти все утилита не залетают мгновенно то есть вы за вечер получаете реально рабочий докер кластер соответственно потом вы его мы долго долго внедряли продакшн это всяко качали качали качали потом провели уборку соответственно все сократили и вся вот сама вот удивительная вещь которая получилась в том что те настройки которые у нас сейчас есть они практически на 100 процентов совпадают с теми настройками которые получаешь 1 вечером работы то есть вот то что то как tutorial в принципе так вот и у нас то есть в этом конечно маги вот этих там современных таких простых опыт собственных штучек что он все таки да ну миссия компании у нас учить детей вот соответственно мы не хотим там становится в специалисты в комнату супер 3 keys of те там или в чем то другом вот и соответственно то что мы из коробки бесплатно получаем ну такую совершенно как действительно очень приятную в работе очень мощную там эту структуру ну как я вот этим так даже восхищенные рад что мы за кладом удачное время все живем этим занимаемся вот на как бы этом все то есть я там приглашают я из контактов я мне показалось что наиболее док строчных фейсбук то есть этом если мы чуть такой новенькое хорошие сделаем этом напишу поэтому он добавится как бы я его как адресную книгу держу всех добавляем вот соответственно по вопросам есть у нас время есть временно вопросы да вот все тогда давайте вопросы за хорошие вопросы призыв [аплодисменты] но у меня несколько даже на самом деле вопросов в чем преимущество консул темплейт в придании полки плитами почему и пользуйтесь консулом они ангельскими тимплей там например для настройки каких-то правил фаерболов кручу арт уже просто в группу wars все определить ее фигачить с русскими тимплей коми но потому что сейчас у нас пока что вот эти внешние балансира ней трафик ведут прямо на контейнеры то есть там нет никого промежуточного слоя поэтому там формируется конфиг который вот как раз он собственно сами вот и печники порты кластера пробрасывать только поэтому но и в принципе у нас из-за того что все настройки балансиров лежат в кафе соответственно съесть мечте и к который мы уже очень близко подошли отдать настройки роутинга девелопером ну через интерфейс понятно безопасной что мне ничего не сломали поэтому то есть как бы самом к поэтому именно за счет того чтобы роутинг такой динамический был и по поводу однородности всех площадок то есть неужели не бывает запрос под бизнес или от разработки что нужно вот на этой площадке вы качеством детка этом тарантул с кассандрой и не знают бывает но это не но это прямо именно очень большая редкость и это мы как бы как оформляем такой внутренний отдельный артефакт то есть мы ну допустим нас есть там кучеру он в облаке у него база не работает у нас там дедики которые про брошены в облака но cts на по сгрыз над языках для облака это вот как раз такой артефакт все остальное одинаковое не да это есть такая проблема но она редко реально и наконец вот проблема доставки на площадку мне кажется можно просто про его от регистре в каждой площадке завести фисташка а и правит registry в каждой площадке на каждом облаке и оттуда уже быстро лиц а как туда засовывать ну как снаружи и снаружи очень долго кашапова пену за засовывать ноты ввп нужно срочно одно правило регистре они не ним не у нас с внутри засохнет и где интерес они подожди прежде смотри а как ты засунешь ты все вернут упрешься это то что когда мы раскладываем если мы раскладываем на там 15 рядов одновременно мы упираемся в сеть так так то внутри части она мы внутри вам его внутренняя упираясь внутри что люди конечно мы в гигабит упираемся в смысле как упираемся там то есть у нас допустим да докера у нас было выкатка 13 минут на нашу любимую капистрано нас как бы теперь был не испортить ну не сильно испортить мы 18 час к темно это уже конечно же с кого-то ну окей пофиксим но выбираем вы прям во внутреннюю сеть конечно ну хорошо спасибо еще вопросы вопросов а вот есть вопросы с таким количеством продуктов и таким огромным количеством судя по всему доки контейнеров которые вас бегут они в принципе все основаны примерно на одном и том же стаки так ну например на там утопи там надо ну да как часто вы тестируете или проверяете на обновление своей образы докера и как вы имеете дело с проблемы обновления когда там жили psy надо починить тут везде прям щас или там у панаса цель надо починить когда такой огромной коллегу то есть это в принципе проблема для любого админа она в том числе карта столько разных ну если такую штуку нашел на неделю садимся и чиним вот если именно выкатить то мы можем выкатить там целиком целое облако ну то есть мы можем стали накатить просто через вот эта через федерацию мы можем все приложения ну то есть мы можем там про кликать вот эти зеленые кнопки рядком и утечек чай пить они выглядят совсем то есть этот шаман там с бубном пляж это вообще санс аманда не зря вы собираетесь свой шаманом выпустить для вообще людей как чтобы поделиться своим опытом своим качествам вот смотри позволяете шаман вот андрей он обещает нам среднюю осенью за концерт шумана потому что надо же надо добавить там поддержку кубера потому что не серьёзны иначе ну и почистить там всяких лампочек ну вот и почистить заодно ну не на суше но это как бы опыта всегда должен быть более качественно конечно же на конференции рассказываю как мы там и жить там как бы все это приходим там ну что ж там как как по-всякому может быть как об руку в концепт шаман там выйдет в какой-то момент мне не ним мы собираемся мы собираемся то есть ну смысле как это там я вот андрея надо собирать соберется штука прикольно кстати получилось то есть мы как бы и пилили исключительно для себя она на реакции на bootstrap и четвертом то есть такая молода еще вопросы нет вопросов хорошо спасибо большое [аплодисменты]