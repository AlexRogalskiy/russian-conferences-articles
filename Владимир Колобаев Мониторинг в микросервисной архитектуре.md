**Здесь переводится видео в статью Владимира Колобаева из Avito "Мониторинг в микросервисной архитектуре"**

Ссылка на видеодоклад https://www.youtube.com/watch?v=5rdIbu4hNfQ

**Текст сделан из субтитров. Буду очень благодарен в форматировании текста, его вычитки. Для этого достаточно знать русский язык.
Присылайте свои pull request или присылайте текст на почту patsev.anton[собака]gmail.com**

всем привет ещё раз меня зовут владимир колобаев я занимаюсь развитием систем мониторинга lavita наша компания постоянно растет и решение микро сервисной архитектуры мы используем все больше и больше на данный момент у нас помимо монолита порядка 40 микро сервисов каждый проект требует того чтобы его мониторили соответственно такие объема мониторить силами 2 псов достаточно проблематично и мы разработали систему мониторинга качества сервиса сейчас нет ap нет ну sora no money мы мы разработали систему мультик качество сервиса наш сервис предоставляет нашим разработчикам возможность самостоятельно писать метрики в систему мониторинга самостоятельно брать эти метрики пользоваться ими строить на основании них душ барды и прикручивать к ним а лифтинг при достижении каких-то каких-то значений до пороговых вот таким образом наши ребята самостоятельно мониторе свои проекты и это как бы очень удобно мы предоставляем им им инфраструктуру и документацию сами своей проекта не мониторит самостоятельно для того чтобы понять как все это вообще произошло как мы к этому пришли нам надо отправиться в прошлое в далеке 2012 15 год извиняюсь 2015 год и для этого нам поможет марте его машина времени марте заводи вот в общем мы сейчас с вами в 2015 году на данный момент наша система мониторинга приблизительно выглядит вот так у нас порядка двадцати четырех различных узлов которые так или иначе отвечаю за мониторинг у нас есть целая пачка различных кранов скриптов и демонов которые что-то где-то как-то мониторят отправляют кому-то какие то сообщения и выполняют какие-то функции мы посидели подумали и решили что вот такая система она в принципе дальше но не жизни способное и развивать как бы смысла не имеет она слишком громоздко и в ней слишком много всего на вершиной от не надо отказываться мы подумали и решили выбрать какие элементы надо оставить для жизни дали какие можно развивать от каких надо просто отказаться значит подумали и сделали вот так то есть мы отказались от 19 из 24 пунктов оставив только графит и агрегаторы и графа ну качестве даже барда в тот же момент мы начали думать и как будет выглядеть новая система то есть хорошо от этих мы как-то избавимся что-то там переделаем надо какая-то новая схема новое представление выглядела на в наших представлений вот таким образом у нас есть хранилище метрик это графит и которые будут базироваться на ssd дисках вы страх это агрегаторы определенные которые будут эти метрики в себе агрегировать собирать и отправлять их графит дальше это grafana для вывода душ бардов и мойра в качестве а лифтинга также мы хотели разработать еще систему поиска аномалий вот и вот такие планы у нас были в 2015 году в тот же момент так как нам надо разрабатывать не только инфраструктуру и сам сервис но нам еще надо разрабатывать документацию мы для себя разработали корпоративный стандарт который назвали мониторинг 20 в этом корпоративном стандарте мы отразили первое мы отразили требования к системе система должна быть постоянно доступной она должна хранить метрики с интервалами в 10 секунд по крайней мере мы должны к этому стремиться структура хранение метрика дешвордов должна быть строгой то есть мы точно должны понимать где как у нас будут храниться метрики чтобы у нас не было бардака и мы хотим собирать и винтовые метрики через vdp потому что у нас достаточно большой поток трафика у нас очень большой поток ивентов и все эти эвента генерирую в конечном счете метрики если мы будем все эти метрики сразу записывать графит у нас наши сторож ляжет соответственно мы хотим эти метрики собирать через youtube и потому что через tcp мы просто не уложимся в буфер а вот и могут быть проблемы поэтому решили через и типе использовать также мы выбрали префикс первого уровня вот эту таблице которую вы сейчас видите на экране это те фикса первого уровня для всех метрик то есть мы все метрики разбили на префиксы и этих префиксы мы строго описали что каждый из них четко отвечает за свою какую-то свое свойство но нас есть метрики по серверам метрики по сетям метрики по контейнерам по ресурсам по applications мы строго это определили и заложили на это фильтрацию то есть у нас есть четкая строго типизированный а фильтрация где мы принимаем метрики с первому с первым уровнем префикса именно вот таким все остальные мы просто дропаем вот так вот в 2015 далеком году мы решили что дальше мы будем развивать нашу наш сервис мониторинга подобным образом а теперь вернёмся назад марте заводи вот возвращаемся в наше светлое настоящая и видим здесь схему взаимодействия компонентов мониторинга да у нас есть мониторинг вот он но сейчас продакшене он работает каким же образом он вообще работает у нас есть первым шагом это ресурсы которые метрики нам отправляют тем или иным образом или те ресурсы которые нам надо за мониторить то есть у нас есть что-то для чего вообще это система мониторинга и существует первым шагом мы мониторим applications ну то есть это наш php код это наше все приложения это все наши микро сервисы в общем все все все все что пишут наши разработчики относятся к applications он в основном значит все applications через youtube и отправляет метрики в брюс big blue big это агрегатор это стад sd переписанный носи почему blue big мы взяли все агрегаторы которые только смогли найти под нашу инфраструктуру проверили прогнали по ней наши синтетические тесты в риббек оказался самым быстрым и взяли его что умеет делать в рубикон имеет различные в общем полностью полностью как бы копируют технологию ssd и соответственно у него есть различные типы метре которые он себя принимает через дтп и отправляет эти метрики агрегированные в графе через tcp соединение соответственно он у него например есть такой тип метрик как таймер и таймер это самое как бы классная штука которая в нем есть вы отправляете на каждой вашей event события метрику в брюлик в агрегатор он и все схлопывается в 10 метрик но он их все скупают внутри себя агрегирует и выкатывает кати в качестве 10 метрик то есть вы например отправляйте респон ставим на каждый каждое соединение пользователя к вашему проекту вы отправляете метрику с response time а в конце вы получаете 10 метрик то есть вам предположим пришел миллион человек а вы получили на выходе 10 метрик вы получили количество вам пришедших людей вы получили максимальное время отклика минимальное время отклика средняя медиану и 4 persantine 75 95 99 999 то есть вы это получаете все из коробки вам ничего не надо делать метрики отправляем в brubeck брюликах переваривает отправляет в графит вы получаете картину в состоянии ваших applications это что касается агрегации для applications of дальше у нас есть агрегация также и для наших системных метрик у нас есть метрики по железу у нас есть системные метрики метрики по софту например это там различные редис и базы данных все прочее у нас есть старая система мониторинга мунин которая была до 2015 года он так в исторически сложилось что мы использовали ее это все мы опрашиваем через калек d и пишем через коллектив графит калек да это хищный демон который вшита целая пачка различных плагинов он умеет опрашивает все ресурсы кастовой системы на которой он установлен вы в конфиге ему указывайте куда эти данные писать и он впишет в графит предположим вы в кафе г указали что там хост какой то он все эти данные будут писать графит прям из коробки также он поддерживает плагины и python и shell и скрипты соответственно вы можете писать свои какие-то кастомные решения и таким образом коли где будет эти данные собирать локалхост или с удалённого хоста там предположим есть курлык да с какие-то метрики будет собирать отправлять их графит очень удобная штука дальше следующим шагом все все метрики которые мы собрали через агрегаторы мы отправляем в карбон сериалы карбон сирила и это карбон relay из пакета графита переписаны на си как вы видите мы почти все решения используем от графита но они все переписаны на чем-то в общем мы используем карбон сирил и это роутер он принимает в себя все метрики которые мы отправляем с наших агрегаторов и маршрутизировать их по нашим нодом в дальнейшем по нашим родом куда мы ему отправим также на стадии можете заяц маршрутизации он проверяет валидность метрик то есть во первых метрики должны соответствовать вот той схеме которую я показывал раньше с префиксами первого уровня во вторых данные метрики они должны быть валидный то есть сам графе должен потом их суметь принять если они не валидные или они не подходят по префиксом их просто дропаем карбон сериалы и метрики получил следующее что он делает он отправляет их класс стр графиков мы используем в качестве основного хранилище для метре графит и точнее сказать мы используем go карбон и это карбон кэше переписанные нога вот они намного карбона намного быстрее работает с данными потому что он многопоточный общем он действительно во много раз превосходит по производительности карбон кэш стандартный который идет в пакете графита и соответственно он принимает все данные в себя и записывает их на винт на диске с помощью пакета whisper которая написана питоне мы его используем как бы стандартный на данный момент это самая быстрая быстрый способ записать данные на винт и прочитать данные sinta вот мы используем да я говорил ssd диски и для того чтобы записать данные на диск мы используем гу карбон а для того чтобы прочитать данные с наших страхов мы используем графит api это аналог графит вэба стандартного который ведет в стандартном приложении стандартном пакете графит of за исключением кластеризации и вып интерфейса это просто как бы api-интерфейс для взаимодействия с остальными системами вы к нему можете обращаться получать метрики и выводите предположим в графа ну работает намного быстрее и поэтому мы используем графит api ну вот данные ваши приехали в графит и что с ними дальше происходит дальше данные мы выводим в графа ну в качестве основного datasource а datasource а у нас используется наши кластера графиков у нас есть графа на как web-интерфейс для отображения метрик для построения дашбордов наши разработчики все они знают и на каждый свой сервис мы заводим но они сами заводят себе дашборд с названием их него сервиса дальше они строят по ним графики и на этих графиках они отображают метрики которые пишутся своих applications вот так же у нас есть помимо grove on и у нас есть еще с.м. это питания чей плагин который для , чей демон который считает и slay на основании данных из графита то есть у нас есть определенное количество микро сервисов да там порядка 40 у каждого микро сервиса есть свои требования psl мы берем ходим с помощью наших питания чик димона входим в документацию где описаны эти требования берем получаем оттуда список требований идем в графит и сравниваем насколько требования соответствуют доступности наших сервисов это что касается компонентов которые ходят в графит и дальше у нас есть allure think a лифтинг у нас организован с помощью майры мойра это сильная независимая система а лифтинга она независима и потому что у нее под капотом свой собственный графит ее разработали ребята из скб контур а она написана на питоне и нога она полностью open source на и соответственно ее достаточно просто поддерживать и развивать вот в общем мойра получает в себя целый поток метрик всех вообще все метрики которые идут в графит и она не так же идут в май ру и если вдруг у вас отъедет хранилище по какой-то причине не важно по какой у вас ваша лифтинг не умрет он будет жить с ним будет всё в порядке мы мой развернули в кубе r'nessa базу данных она используя в качестве основной базы данных на использовать редис редис и мы сделали ну как бы кластер из редис и среди серверов соответственно нас получилось отказоустойчивая мойра так вот у нас есть поток метре который летит в майору мойры берет весь этот поток метрик идет в список триггеров которые у нее созданы и сравнивает если такая метрика есть упоминается в каком-либо из триггеров она с ним с этой метрикой производит какие-то операции если у нее нету никаких записей адонай метрики она ее дропает таким образом она способна переварить там ну вот гигабайты метрик в минуту легко что еще в ней очень здорово она мы ее как бы как сделали мы взяли майору и прикрутили к ней наш корпоративный ldap теперь у нас каждый наш пользователь который в корпоративной сети как-то заведен может в ней авторизироваться как только он авторизируемся в море у него появляется возможность создавать по существующим триггером конкретность для себя notification также он может самостоятельно создавать триггеры если они ему нужны и каждый пользователь создает конкретно для себя нотификации но конкретно своим метрикам так как мойра держит в себе графит она поддерживает все функции графита поэтому вы сначала берете строчку и копируете и в графа ну смотрите как отображаются данные в на графиках а потом берете эту же строчку и копирующие в мои ру и мэр точно так же с ними работу это вмешиваете и этими лимитами и получаете на выходе а лифтинг для того чтобы все это делать вам как бы не нужны никакие специфические знания мойра умеет allure тесь по спискам по емейлом и умеет allure сеть в slug залететь это значит я так думаю все знают соответственно мойра также поддерживает выполнение кастомных скриптов что это значит когда у майры случается триггеры она подписана к это кастомный скрипт или бинарник она его запускает и отдает на is to dream этому бинарник у джейсон соответственно ваша программа должна распарсить и тот же сон и что вы дальше будете с этим же сонам делать вы решаете сами хотите отправляете ее там в telegram хотите создавайте там тоски в жире хотите делать что угодно мы таким образом реализовали тоски в жире и у нас есть наша разработка по imac и мага так у нас есть такая такой опыт в общем был один проект когда нам дали порядка 300 электронных ценников которые используются в магазинах и сказали вот ребята у нас был там проект по которым мы хотели отображать на этих ценниках какую-то информацию но этот проект не взлетел давайте вам в мониторинг отдадим и вы чем придумайте с ними но мы значит подумали посидели и запилили вот такую вот штуку то есть а сейчас и запилили вот такую вот штуку это панелька это как раз таки есть электронный ценник и мы вывели на нее триггеры из мойры мы указываем вот она кстати у меня вот она физически есть мы выводим на нее триггеры из мэры мы пишем каком состоянии триггер когда он случился ну то есть если показывать вам здесь вот название триггера вот когда он случился и соответственно если красно это значит плохо если белые то значит хорошо чем эта штука удобно ну предположим вы вышли из отпуска вы посмотрели на свою панельку и вы знаете в каком состоянии находятся все ваши триггер вот прям на данный момент времени и что когда там произошло это очень удобная штука у нас часть наших ребят из разработки отказались от а уведомлений флаг и в почту ради вот этой вот панельки так как мы прогрессивная компании у нас есть кубер не 100 км еще и cabernet сюда за мониторили мы вставили в нашу систему мониторинга ну точнее впаяли нашу систему мониторинга кубер не с помощью хипстера мы установили хипстеров к остатку вернется уходит пока вернется собирает данные отправляет их грозит таким образом мы за мониторе лику бернес дальше настоящие компоненты вот вам ссылочки тех компонентов которые мы используем они все open source иные пожалуйста они все есть на гитхабе или на своих личных проектах например как графа надо вы можете ими пользоваться прямо сейчас то есть они все концертные они открыты устанавливаете интегрируете в вашу инфраструктуру пользуетесь мониторингом ваших проектов немножко статистики у нас есть вот агрегатор в риббек количество метрик мы принимаем порядка 300 тысяч метрик в секунду в гробик через ю д п и с 30 секундном интервала мы отправляем эти метрики агрегированные уже в графит мы используем порядка шести процентов циpкa ну чтобы быть тут более честно мы используем успешные сервера как бы это необычная там ноутбуки до или компьютером используем полноценные сервера и спешно и на них собственно нагрузка следующая на агрегаторе который принимает в минуту порядка 18 миллионов метрик мы используем всего лишь 6 процентов по типу 1 гигабайт оперативной и 3 мегабайта персик он по сети то есть нагрузка там вообще как бы детская на сам сервер хотя функционал который он организует очень даже такой серьезный дальше по поводу сани графита хранилищ вот у нас есть графит и количество метрик в минуту которая прилетает и сохраняется на жесткие диски порядка 1 и 6 миллионов в минуту то есть 16 миллионов в минуту метрик мы сохраняем на наши диски интервал обновления метрик 30 секунд то есть мы раз в 30 секунд скидываем эти данные на винты схема хранения данных графит и поддерживают следующую следующие схемы то есть вы можете брать одну метрику и хранить ее предположим первые 30 дней с интервалом за одну минуту но или за 30 секунд или за 10 секунд короче говоря с каким-то коротким интервалом дальше предположим вот вам недели интересно вы ее с 10 секундам интервалом хранить и дальше как бы вам нужна данная метрика только для аналитики вы ее храните предположим течение 30 дней или например 3 там трех месяцев вы ее храните в течение там 5 минут с интервалом 5 минут а дальше вы ее хранить и с интервалом в 10 минут целый год или например 10 лет вы храните вашу метрику с интервалом в 10 минут шагом 10 минут таким образом вы на графиках будете видеть как раз ли данные по вашим сервисом точных данных детом какие пики были вы конечно не увидите но вы будете знать как вы растете или как вы падаете или что происходит с вашим сервисом на продолжительном периоде времени дальше у нас так как наша компания обращает внимание на как бы различные особенности систем мы также очень ценим в нашем сервисы мониторинга гибкость во-первых почему как бы он гибкий мы для себя выявили два свойства одно это взаимозаменяемость все вот те кубики из схемы они взаимозаменяемые вы можете поменять карбон кэш на go карбон вы можете поменять carbon серийно сирила и там на карбон relay вы можете поменять майру на какую-то другую систему а лифтинга там предположим на капот или примеру на сирену то есть вы можете меня эти компоненты так как вам удобно вы можете налету менять сами конфигурации хранилище предположим сейчас вышла новая версия графита вы можете где-то песочница проверить как оно в принципе зайдет к вам и прямо в бою и и сразу деплоить соответственно у вас будет новое новое какое-то решение вашей системе мониторинга и при этом оно все будет работать взаимозаменяемость и гибкость это очень как бы здоровски качества и следующий фактор это поддерживаемых так как весь так как весь проект на open source и весь открытый вы самостоятельно можете править код вы можете сами вносить изменения вы можете реализовывать те функции которые нужны конкретно вам но из коробки они не были реализованы так как используется стыке достаточно распространенное того и питон в основном то это делается достаточно просто но и например и вот у нас была ошибка не ошибка у нас была проблема метрика в граффити это файл у файла есть имя имя файла это имя метрики по факту ну и путь до до этого файла папочки соответственно имя файла ограничена в линуксе 200 255 символами а у нас есть например ребята из баз данных и они говорят а мы хотим мониторить нашей сквер запросы sql запросы не 255 символов а 8 мегабайт и мы хотим вот их полностью отображать графа нее смотреть сколько раз был вызван данный запрос сколько времени он работал ну и остальные какие-то параметры по данному запросу а еще лучше мы хотим видеть топ такие за и мы хотим это видеть всю real time а еще и лучше в мониторинг прямо и в allure think запихнуть и тогда мы берем и пишем делаем как бы следующую штуку мы поднимаем сервер редиса и нашими или где плагинами которые ходят в пост берут оттуда все данные отправляем метрики в графит но заменяем ими метрики нахожу и тот же хэш в этот же момент времени как мы его записываем в грозит мы отправляем в редис и вместе с этим hашем редисом и отправляем весь эскивель запрос туда же в качестве ключа и вот у нас есть кошек в одессе и у него есть ключ эскивель запрос нам осталось сделать чтобы граф она умела каким-то образом ходить в редис и забирать оттуда эту информацию что мы делаем мы берем открываем графит api так как это основной интерфейс взаимодействие всех компонентов мониторинга с графитом отрываем графит api у него есть файл ханшин мы открываем его там все функции которые вообще он знает и вписываем туда новую функцию которую назвали оля сбои хэш и теперь когда наша граф она видит что у нее алиас buy хэш она идет freddy's берёт оттуда туда дает туда фишек и получает оттуда эскель запрос таким образом мы в graph они вывели полностью эскель запрос который вот он отображен на графике который по идее сюда никак запихнуть было нельзя и мы видим по ней статистику вот мы видим минимальное значение максимальные среднее и на нынешний момент это к слову о поддерживаем асти данная система очень проста поддерживаем и этого наша система наш сервис мониторинга он является доступным он доступен 24 на 7 и и из любого вашего applications любого кода вы можете писать в него данные где бы вы не были как бы если у вас есть доступ к нашим хранилищем вы можете писать данный какой там язык не важно какие решения не важны краны не краны доступно вам надо знать только как открыть socket закинуть туда метрику и закрыть все этого достаточно для того чтобы пользоваться данной системой надежность все наши компоненты отказоустойчивые графит и отказоустойчивые в любеке отказоустойчивого общему она достаточно надежно и как бы с нашими нагрузками очень ну скажем очень хорошо справляется поэтому у нас как бы к ней претензий нету низкий порог вхождения для того чтобы вам пользоваться этой системой вам не надо изучать не какие языки программирования он не надо изучать какие-то запросы в grafana вам нечего изучать не надо вы просто берете открываете ваш ваше приложение вписываете у него socket который будет отправлять метрики в графит закрываете этот socket открываете графа ну создаете там дашборд и не смотрите на поведение ваших метрик как бы ничего знать не надо для того чтобы а лифтинг повесить открываете мой руи в море создаете триггер и самостоятельность самая классная во всем этом что вы это можете делать сами без использования devops of то есть и это вообще overkill over фича потому что вы можете мониторить свой проект прямо сейчас прямо вот уже прямо сейчас вам не надо просить кого-то чтобы они вам сделали что так а потом вы решили что вам надо что-то поменять и снова надо кого-то просить таким образом вместо того чтобы получить за мониторе ный проект вы потратите месяц времени а вам боксу будут говорить о вы знаете у нас как бы времени нету мы вот и доз переживаем да теперь в будущее мортиза воде в будущее значит к чему мы стремимся мы хотим запилить у себя детектор аномалий то есть это такой демон который будет ходить в наших графит хранилище и каждую метрику проверять по различным по различным алгоритмам и чтобы было больше более понятно вот то к чему мы стремимся это что мы желаем это как бы не какие то абстрактные мысли это то к чему мы сделали уже хотя бы первый шаг то есть мы хотим детектор аномалий у нас уже есть определенный алгоритм и какими мы хотим аномалии просматривать у нас есть скажем сами данные мы имеем с ними работать поэтому детектор аномалий мы хотим и мы его сделаем метаданные мета-данные это в общем такая штука у нас есть целая куча сервисов эти сервисы постоянно дополняются постоянно новые новые сервисы кто-то там меняется люди увольняются люди приходят кто следит за сервисом кто его разработал и с какими с какими компонентами взаимодействуй в конечном счете понять очень сложно постоянно вести документацию вручную это как бы такой себе вариант поэтому сейчас в наших микро сервисов встраивается такая штука как метаданные мы каждому сервису пишем это данный где пишем кто его разработчик на каком языке он написан с какими системами он взаимодействуют в нем же в этих метаданных написав требования ps лей в этих же метаданных мы описываем нотификации пишем куда и кому какие нотификацию высылать и при тепло и вашего сервиса все данные сущности создаются самостоятельно то есть у вас создается самостоятельно 3 герой море у вас самостоятельно создается dash барды в графа не и таким образом в конце тепло и получаете две ссылки одна ссылка на ваши триггера вторая ссылка на кошмарных графа не и вам вообще ничего делать не надо как бы только деплоить мониторинг в каждый дом это такая прям идея идеологическая штука мы считаем что вот такой системой или подобной системой должны пользоваться все разработчики потому что таким образом вы сами осознаете в каком состоянии находится ваш проект вы понимаете когда у вас приходит трафик что с ним происходит где он падает где у него слабые места и вы это делаете сами и вы видите что предположим к вам пришло нечто и завалило ваш сервис вы видите это сразу прямо на моменте когда это случилось и видите это не от того что вам позвонили менеджеры и сказали вы знаете а вот у нас сервис не работает а потому что вам пришел alert и вы прям в момент когда он пришел можете открыть логии посмотреть что там произошло это намного удобнее чем если вам звонит менеджер или вы через полдня подходите смотрите логе а там уже как бы таких логов миллионы дать найти достаточно проблематично вот вопросы всем спасибо спасибо владимир здравствуйте меня зовут антон спасибо за доклад хотел меня вам вопрос один маленький дом большой маленький это сколько по времени вы понимали всю нашу структуру и ну по человек ресурсам скажем так с 12 ну вот с 2015 года по сей день вот и и развиваем развиваем постоянно что-то делаем она на данный момент продакшене ну делали you got a команды один человек мучает вами здоровы и 2 чего просто не больше почему dealer think использовали сторонние решение если в графа не насколько знаю сюжет просто граф она во-первых граф она выпустила данную технологию совсем недавно это во-первых а во-вторых граф она обращается к графиту то есть у вас есть предположим триггер на состояние ваших скажем температурных датчиков на серверах таких температурных датчиков на серверах порядка 40 вам надо видеть еще три школы то есть это еще как бы вам надо видеть какое сейчас состояние какое максимальное состоянии еще и как бы одно от другого отнимать таким образом вы будете использовать порядка 40 метрик два раза до 80 метрик и это умножаем на количество серверов предположим у вас серверов по получается 4000 метрик по каждый покажет этим по каждой за каждый метрикой мы ходим в графе то мы нагружаем наш сторож мы нагружаем графа ну которая как бы нужно для того чтобы ребятам быстро отображать их не на аналитику какую-то да а мы используем систему как бы отображение аналитики для того чтобы вы new триггеры она банально не потянет такие нагрузки то есть если мы сейчас возьмем и миллион метрик заставим графа ну проварить мы просто возьмем создадим какой-то дашборд и скажем проварено миллион метрик она просто сразу повиснет мойра в этом плане она не хочет никуда не в какие графит и она сама сама по себе во вторых она не зависимо от графита работает то есть если ваш сторож упадет график вашему а лифтингом будет все в порядке во вторых она переваривает огромные потоки данных и она делает этого достаточно просто вот я к тому что граф она она очень хорошо использовать ее для allure тинга когда у вас там три дэш барда два разработчика и вот вы там пожалуйста да но когда у вас возникают вопросы о том что вам надо мониторить и создавать alert и на какие-то тысячи или десятки тысяч метрик грифа на это не про это отображение график графика ну как раз grafana на как бы с большой то нагрузкой как будет достаточно плохо справляются со всё верно граф она справляется с большой нагрузкой когда вы на нее а лифтинг вешаете если мы говорим о метриках там уже не граф она плохо справляется плохо справляется ваш ноутбук с вашим браузером потому что он все ваши метрики выгружает вам браузер и javascript формирует нас на логане пришедший к вам миллионов данных вам графики и просто повисают ваши браузеры ну кстати говоря может и графитового вас могут подтапливать что мало конечном итоге все равно пользователь видит эту зависшую страницу когда эта страница пожрала всю память то есть это неоднократно наблюдало как хотя даже просто период как то неудачно выберешь как бы и просто виснет все как бы памяти поднялась но я могу я могу сказать if или вообще как-то может правильно настроить надо граф она однозначно надо настраивать правильно даже не графа ну а правильно надо настраивать интерфейс взаимодействия с графитом то есть графит api или графит web или какое-то решение которое но предположим может у вас influx или например у вас какой-нибудь prometheus надо уметь настраивать именно вот тот компонент потому что это тот компонент отдает вам данные если вы делаете какой-то большой запрос заданными он идет туда и там скорее всего повисает у вас просто крутится вот этот кружочек в графа не то что она ждет банально данный пока и отдаст сторож если вы берете и выгружайте к себе предположим 10 тысяч метрик к вам на страничку да то тут вопрос к производительности вашей вашего ноутбука там или вашего компьютера потому что это он должен быть способен проводить данные метрики и 1 нарисовать на ней графики через java script агрегацию вот агрегации это все таки вот это чтобы за большой период вывести график должна быть агрегата на на стороне графит или графа и так и так и на стороне графита происходит как бы агрегирование данных и они складываются с определенными промежутками времени она стороне граф анны если вы выберете интервал год то у вас будут шаги по метрика мне там раз в десять секунд до например раз в десять часов но это если вы предположим зале и объем данных точек придет меньше в grafana нет точек точек придет в меньше в графа ну это действительно так да ну их придёт ровно столько же сколько бы их пришло если бы вы просто сделали запрос за час именно в графа ну придет точек одинаково как бы граф они все равно у вас просто как бы будут интервалы между этими точками разное количество точек будет одинаковая и вы упомянули про мета-данные как глаз как бы может немножко поистине как общего с это организовано то есть как каждый сервис да то есть разработчик пишет документацию по своему сервису в качестве метаданных и указывать там свое имя указывай с какими ну как бы я уже сказал он указывает там целый ряд информации в том числе если и свои адреса для нотификации это чисто такой вопрос даже не к системе а к вашему как-то сказать как ваши карпук вашей корпоративной политики то есть это вопрос именно как вы это на документах отразите как вы заставите людей писать формат зависимости владимировне спасибо за доклад скажите пожалуйста вы говорили первый ключ у вас метрики это вот определенный четко список дальше у вас идет разбиение там нет то есть по серверам вот у нас как бы есть наши системные у нас есть калле где который установлен на всех серверах и соответственно сначала идет префикс сервер дальше идет . дальше идет имя сервера идет . дальше идет коли где идет . и дальше уже циpкa memory сети и все остальное что и второй вопрос дмитрий и удаляйте как или хранить их вечно метрики но у нас есть краны которые следят за тем чтобы если метрика не обновляется в течении 30 дней мы и дропаем если ну скажем вам надо просто взять и удалить метрику какую-то которая вот ребята попробовали она им как бы больше вроде не нужно нам заходим на стороже гуляем метрику это файл linux спасибо большое за доплату на вы можете просьба такая а сейчас так в жук я быстренько сделать анализ по страница вашей системой и связка графа grafana плюс люкс а также ваш системой и прометазин этот первый может быть там вкратце плюсы upper вопроса второй вопрос я не совсем уловил связку как с помощью вашей системы быстро посмотреть логи что же произошло и есин причин но первое мы пробовали у себя influx of us не справился с нашими нагрузками и мы отказались от конфликта по поводу prometheus а он у нас есть как бы на радаре мы хотим его посмотреть но пока мы развиваем у себя графит потому что графит отвечает всем нашим требованиям и скажем мы его достаточно долго развиваем и мы умеем что касается второго вопроса это а логирование за логирование у нас отвечают киба на то есть логе все летят киба ну как бы мы киба надо эластики бана и собственно блоге смотрим там в докладе упомянули что нет м м говорит о том что проблема возникла а вот вы можете очень быстро оперативно узнать что была проблема также при прологе сказали уже на третьем есть как бумага если у вас есть например метрика которая отвечает за взаимодействие вашего сервиса предположим с базы какой-нибудь это и это как бы метрика вдруг начала выдавать что потеряна связь с редисом вы получили alert о том что у вас нарушена данное данное взаимодействие как бы вы понимаете у вас пропал пропали пропала связь с редисом открываем кабану смотрим как бы так это или не так ну скорее всего вы сразу полезете и посмотрите на сервер и что там с ним случилось да я к тому что вы в метрике можете заложить всю вот эту логику которая вы отображаете влогах то есть конечном счете у вас в логе помимо стресс этого помимо вывода logo у вас там как бы основная информация о том что у вас где перестала работать в эту логику можете переложить на числовые данные спасибо 1 ремонт не смотрели после нет нет нет не смотрели здравствуйте у меня такой вопрос мониторить или вы систему мониторинга чем и как да конечно мы мониторим нашу систему мониторинга у нас есть мойра которая следит за граф анны и нас есть графа на которая следит за мой рай здравствуйте вот вы сказали что вы используете cabernet и а где он там обычные контейнеры микро сервисы и вот эти сервисы которые там крутятся он сегодня может быть их 10 завтра 100 послезавтра 50 вы как-то разделяете метрики который приходит с различных мы контейнеров одного приложения да мы стараемся мы стараемся все это агрегировать то есть мы стараемся уходить от вот этих уникальных дополнений к потом ну конечно поднимается под у него появляется там контейнер в контейнер присваивается какое-то рандомной очистил к мы таки силки отрезаем у нас появляются метрики которая соответствует имени сервиса и мы таким образом все это отправляем в агрегатор и мы как бы видим сводную информацию по нашим по всем потом то есть если у нас например 8 кодов мы увидим с агрегированные данные что у нас на данный момент пришло 8 метрикам из них минимальное значение такое максимальное такое если максимальное значение выходит там за какие-то лимиты тогда мы сделаем alert и у нас таким образом получается при каждом перри поднятие кода у нас получается не скажем миллион наших кодов а один который соответствует процессу мы теряем в таком случае разбиение вот на данный момент по именам то есть мы точно конкретно не знаем на каком из наших кодов выросли значения за какие-то лимиты до зашли но мы точно знаем что это где-то случилась и мы можем идти и смотреть то есть как бы alert в этом случае придет спасибо еще один вопрос последний потом можно будет владимиру подойти все вопросы задать кулуарах здравствуйте меня зовут константин спасибо за доклад а наверное вопрос такой сторону метаданных как я понимаю какой то template для дашбордов используйте у графа не и вот мы недавно об этом задумались хотелось бы узнать вашу практику на данный момент из тех вещей которые мы планируем и метаданные я повторяю это мы планируем там сейчас на самом деле организована автоматическое создание триггера в море создание самих триггеров из подписания на триггер и конкретных лиц что касается графа но мы пока еще не касались этого момента но там действительно есть решение которое создает уникальный там скажем какие-то паттерн и template of и в них можно запихнуть различные данные на данный момент мы предположим сохраняем у себя все наши json и всех трех бордов то есть графа ну по идее может зайти любой человек и удалить там все и это как бы минус такой дамы jison чеками держим у себя то есть мы складываем дам пим их нет стоп именно что касается бэкапов джейсона fda просто мы их складываем в баку лу то есть у нас есть система резервного копирования мы туда отправляем данные которые нужно задать внутренние забэкапить а что касается новых каких-то видений там конечно все подбитом да ну то есть как я и говорю не то данные нужны для того чтобы когда вы выкладываете ваш сервис подхватывали данные создавались сущности спасибо большое владимир давайте еще раз аплодируем ему [аплодисменты]
