# Артём Выборнов – Kafka в условиях повышенной нагрузки

![](https://habrastorage.org/webt/kf/uf/ph/kfufphgwbcht75siqrdkjoxmwz8.png)

Доклад 2017 года. Версия Kafka 0.8. Сейчас реалии уже могут немного отличаться. Но основные концепции остались такие же.



Когда я кому-то говорю, что работаю в Rambler, люди отвечают довольно однотипно. Вторым по популярности ответом является: «О, я знаю Rambler. Это поиск». На самом деле Rambler – это уже давно не поиск. Мы сейчас конкурируем за пол процента аудитории с Bing и это вообще не интересно. 

![](https://habrastorage.org/webt/lc/oq/zp/lcoqzp7q9neexqpnl8j_ujqx9hc.png)

На этом графике более интересно то, что Google, наконец-то, обогнал Яндекс.

Какой, по вашему мнению, самый популярный ответ на то, что я работаю в Rambler? Самый популярный ответ: «А Rambler еще жив?». На самом деле Rambler жив. 

Сейчас это крупнейший в мире медиахолдинг. В него входят такие известные ресурсы, как: Rambler, Лента газета, Афиша, Чемпионат и много-много других. Аудитория составляет более 40 000 000 человек. 

Преимущественно это медиаресурсы. На них заходят пользователи, чтобы сравнить новости, почитать статьи. И это надо как-то монетизировать. Монетизируется это все за счет рекламы. Для того чтобы делать это наиболее эффективным образом, Rambler несколько лет назад создал отдел рекламных технологий, в котором я работаю. 

![](https://habrastorage.org/webt/kc/qu/nu/kcqunuxjeut_opmddlsqiogr9pe.png)

Мы собираем данные со всех этих порталов с целью извлечь полезную информацию, чтобы подороже показать рекламу. 

Когда мы собрали информацию, то, что мы с ней делаем?

- Мы занимаемся сегментацией аудитории. К примеру, классифицируем пользователей по полу, возрасту. Определяем пользовательские интересы. Например, какие люди интересуются автомобилем Ford или кто собирается поехать в отпуск в следующем месяце. Находим аудиторию, похожую на аудиторию какого-то более маленького сайта. 
- Мы прогнозируем трафик.
- Предсказываем CTR.
- Строим рекомендации. 
- Делаем аналитику.
- И многое другое. 

Но прежде чем все эти полезные действия совершить, нам нужно получить данные. 

![](https://habrastorage.org/webt/dg/if/am/dgifam-uphefx7b_b6f6ujm-nru.png)

Для этого раньше у нас был целый зоопарк способов. Здесь приведены самые популярные из них. 

![](https://habrastorage.org/webt/bm/ic/eb/bmicebcqlprq0ctv9rw3ccnbs70.png)

У нас был worker. На самом деле их было много, просто здесь на рисунке изображен один. Он забирал на себя данные, локально их складывал, а затем перекладывал в Hadoop. 

![](https://habrastorage.org/webt/tf/wz/nh/tfwznhle0anme9z5ibnvsy1ohe4.png)

Мы данные храним в HDFS. Обрабатываем на Spark, Hive. После того как они обработаны, мы хотим полученной информацией поделиться с окружающим миром. Нам нужда для этого key-value база. Мы используем Aerospike. 

![](https://habrastorage.org/webt/du/dk/cc/dudkcce-au7im-yxpiyek9zj1gw.png)

 Вроде бы все хорошо, но на самом деле в этом подходе кроется много проблем: 

- Во-первых, это зоопарк. Он проявляется в том, что добавление каждого нового партнера – это решение отдельной новой задачи как со стороны этого партнера (источника данных), так и со стороны нас (потребителя).
- Во-вторых, нода здесь обозначена как сервер. На самом деле это какой-то набор workers, но каждый файл должен был сначала пройти через одну ноду, где будет сохранен на диск, а потом уже будет выгружен в Hadoop. Это узкое место.
- В-третьих, таким способом нельзя реализовать никакой онлайн pipeline обработки данных. Предположим нам какой-то партнер начал выкладывать данные микробатчами по одной минуте. Мы их забрали и сохранили на сервере. Потом переложили в HDFS. Потом на HDFS обработали. Потом выгрузили в Aerospike. Пока это все пройдет ни о каком real time речь не идет. Это задержки в десятки минут, когда все идет идеально. 

![](https://habrastorage.org/webt/6f/wb/tu/6fwbtuiahk4mxcowmxe0qszukxq.png)

Что мы сделали? Мы все, что находится слева выбросили и заменили на Kafka. Kafka – это брокер сообщений для больших данных. Он не обладает такой крутой семантикой и доставкой сообщений, как <font color ="red">Mirrored Repeat</font>. Он более простой, но при этом он более надежный. 

В Kafka много поставщиков данных могут писать и много потребителей с нее могут читать. 

Вот у нас есть Kafka и нам данные надо каким-то образом доставить до HDFS. Для этого мы используем Gobblin. 

![](https://habrastorage.org/webt/ic/om/a2/icoma2l-a6te5rkxjvqrngstsem.png)

Это решение от создателей Kafka компании LinkedIn. Это <font color ="red">…</font> задача, которая крутится на Hadoop. И данные из Kafka кладет в HDFS. 

Также у нас есть возможность, наконец-то, приблизится к real time. 

![](https://habrastorage.org/webt/vg/kq/9r/vgkq9rowjk_wdovwfmz0zageppe.png)

Мы его реализуем с помощью Spark streaming. Это такая задача, которая постоянно висит на Hadoop. От Kafka он откусывает микробатч информации, как-то обрабатывает и складывает в key-value базу. 

Самое главное преимущество такого подхода в том, что мы полностью минуем HDFS, т. е. все происходит в памяти. Мы с Kafka считали, обработали, положили в Aerospike. 

И при таком подходе, при работе микробатчами, у нас задержки между наступлением события и обновлением соответствующей информации в Aerospike составляет где-то минуту. 

![](https://habrastorage.org/webt/1_/b1/-x/1_b1-xwib1simwojr3jtgbahyyg.png)

Про что сегодня расскажу?

- Во-первых, я расскажу, как приготовить Kafka и обеспечить с помощью нее семантику exactly once.
- Во-вторых, я расскажу, как из-за нашей ошибки у нас выросла нагрузка на Kafka в 4 раза и о том, что мы из этого интересного вынесли. 
- В-третьих, я подытожу, рассказав, на какие метрики Kafka стоит обратить отдельное внимание. 

![](https://habrastorage.org/webt/ol/xs/pd/olxspd-fhyfvxjstq4b_gxnkdhw.png)

Для начала посмотрим, как устроена Kafka. 

Kafka состоит из наборов брокеров (каждый брокер – это отдельная машина) и ZooKeeper для хранения метаданных. На самих брокерах хранятся сообщения. В Kafka пишут продюсеры, из Kafka читает consumers. 

![](https://habrastorage.org/webt/3a/h_/wm/3ah_wm8_7zgi6ic-bhy4skngpq8.png)

Базовым элементом Kafka является поток сообщений. Он называется топиком. Топик состоит из партиций. 

![](https://habrastorage.org/webt/bp/se/po/bpsepos4wrsyuxvu2x04rzcy1om.png)

Каждую партицию можно представить как <font color ="red">r...</font> . Это какая-то последовательность сообщений, где все сообщения пронумерованы, причем чем сообщение свежее, тем больше у него номер. 

![](https://habrastorage.org/webt/rf/wz/ht/rfwzhtk0h0pw-xwmaqgmpltwc-4.png)

В партиции дописывает продюсер. И партициям дополнительно сообщение шардируется. 

![](https://habrastorage.org/webt/5s/v5/qh/5sv5qhnb82t-xasnczqtmgk_hec.png)

Если вы раньше слышали про Kafka, то одним из бонусов Kafka и отличием от других популярных решений является то, что она хранит данные на диске и то, что она их реплицирует. 

Репликация в Kafka происходит с точностью до партиций. Она обычно master, slave. Мастер в терминах Kafka называется лидер. 

![](https://habrastorage.org/webt/lu/wx/0z/luwx0zfeso85_lzxhzk6brs7ddc.png)

Все consumers, продюсеры всегда пишут только в лидер, т. е. пишут и читают только с лидера. И потом данные с лидера синхронно доезжают до всех slaves.

![](https://habrastorage.org/webt/rg/lq/7c/rglq7cgjnc3prieoznbvh6oz2vw.png)

Все бы хорошо, но у асинхронной репликации есть типичная проблема. Это дельта. Slave постоянно каким-то образом отстает от мастера. Это тоже не так уж и страшно, но в Kafka есть такой процесс, как переизбрание лидеров. 

К примеру, наш мастер выпал, т. е. нода, на которой лежал мастер этой партиции, выпала, Kafka производит переизбрание лидеров. И лидером становится какой-то из slaves. И получается потеря данных в виде этой дельты. 

![](https://habrastorage.org/webt/4a/f8/he/4af8hefy6hrwjvlfr1cztfyxp4c.png)

В документации по Kafka есть 2 вида переизбрание лидеров: это clean и unclean. Кажется, что unclean – это какой-то плохой способ, т. е. мы тогда теряем данные, а clean – это хороший, мы тогда не теряем данные. Но на самом деле это не так. Между ними различие в размере этой дельты. 

![](https://habrastorage.org/webt/u3/hm/_c/u3hm_czwyhctvn1hzwzvmvlhmek.png)

Если slave в момент переключения отставал от мастера больше, чем на какое-то количество сообщений, то мы говорим, что это unclean, а если меньше, то говорим, что это clean. 

![](https://habrastorage.org/webt/bn/rk/yk/bnrkyk4i8fle5x4zpcxlmkcwsuw.png)

Очень важно понимать, что переизбрании лидеров мы всегда теряем данные. 

А также есть еще неочевидный нюанс, который не описан в документации: переизбрание может происходить, когда у вас все хорошо. У вас работает Kafka, все стабильно, равномерно распределена нагрузка. Kafka отвечает довольно живо, у вас нет никаких проблем, а Kafka взяла и переизбрала лидеров, и вы потеряли данные. 

Это грустно, а что же с этим делать?

![](https://habrastorage.org/webt/ux/7s/ak/ux7sakt7-jevklz0msstlbdbcee.png)

Для этого пойдем дальше. Есть такой стандартный механизм в любой системе, когда вы куда-то пишите данные. Вам эта система отвечает каким-то подтверждением о том, что данные успешно записаны. Такая же функциональность есть и в Kafka. 

К примеру, вот продюсер пишет в Kafka. И если этот параметр на продюсере выставите в ноль, то никакого подтверждения не будет получено. 

![](https://habrastorage.org/webt/9z/fd/h1/9zfdh1hectn40lmqyswyz_sdona.png)

Выставите в единичку, тогда вам придет подтверждение, если была записана одна реплика, т. е. лидер, т. к. продюсер пишет напрямую в лидер. 

![](https://habrastorage.org/webt/ig/7t/fi/ig7tfihm0czrctupbrcg2eluss8.png)

Интересный момент. Если значение этого параметра -1, то Kafka вышлет подтверждение, когда пришла успешная запись во все синхронные партиции. А что Kafka понимаем под синхронными партициями, это конфигурируется отдельной ключом на самой Kafka. Вы для каждого топика определяете количество партиций, которые Kafka должна держать в insync. И если insync стоит 1, то это никак не отличается от предыдущего примера. 

![](https://habrastorage.org/webt/wd/5i/w3/wd5iw3udoje_wxsxrur6t6iuyt0.png)

А если вы поставить insync.replicas = 2, тогда Kafka отправит продюсеру подтверждение, когда обе реплики будут успешно записаны. 

Вроде бы все хорошо. В таком случае, если будет переизбрание лидеров у нас не будет никаких броблем с потерей данных, потому что все синхронно. Да, бесспорно, будет какой-то overhead на продюсер, т. е. он будет просто дольше ждать ответа от Kafka, но это не так страшно. 

Более страшная проблема в том, что если в данной ситуации одна из реплик выпадает, то мы вообще не сможем писать в топик, потому что Kafka ждет, что будет записаны две реплики, а доступна только одна. 

![](https://habrastorage.org/webt/4x/pe/st/4xpesttbddpc1oclduxyl3el5ie.png)

И это довольно плохо. Мы используем для этого такой подход. Насколько он хорош, это уже вам решать. Мы держим для важных топиков insync.replicas = 2, а replication factor =3 . И если у нас одна из insync реплик выпадет, оставшаяся реплика сможет догнаться, и продюсер сможет продолжить писать нам в Kafka. 

![](https://habrastorage.org/webt/st/fo/it/stfoitlhsyv4ehrltbnjcptw_jq.png)

Одним из фундаментальных понятий Kafka являются отступы. 

Как вы помните, партиция представляет собой набор пронумерованных сообщений. И отступ – это как раз и есть номер этого сообщения. 

Latest offset – это номер самого свежего последовательно добавленного в партицию сообщения. 

Earliest offset – это номер самого старого доступного сообщения. 

Kafka хранит данные не все время, а какой-то период, который вы определяете в настройках. И она потихоньку в прошлом их очищает. Поэтому earliest offset потихоньку движется вперед и, по сути, между earliest и latest offset – это такое окно, которое потихоньку движется по времени. 

![](https://habrastorage.org/webt/fm/de/ih/fmdeihpdeo07tiq6kpjbb4uobcu.png)

Предположим у вас отработал какой-то consumer. Он зафетчил сколько-то данных, вплоть до какого-то отступа. Все хорошо, т. е. данные от earliest до offset у нас уже были зафетчены и куда-то сложены. И при следующем запуске вашего consumer вам нужно будет забрать вот этот выделенный розовым кусочек данных с offset до latest offset и сохранить отступы. 

Здесь нюанс в том, как мы обновляем отступы для того, чтобы при следующем запуске consumer стал читать с нужного момента. 

![](https://habrastorage.org/webt/c1/zi/nw/c1zinwvadfzuttgntan4f5nukfu.png)

Для этого есть несколько способов. И когда вы выбираете consumer для Kafka, первое, что вы должны читать, это то, как он работает с хранением отступов. 

Самый базовый путь – это автоматическое сохранение. Но на самом деле он самый плохой.

Как он работает? Consumer фетчит какие-то данные из Kafka. Kafka периодически до него стучится – жив он или не жив. Если он жив, то у нас сохраняется соответствующий отступ. 

В чем здесь проблема? В том, что consumer данные зафетчил, Kafka проверила то, что он жив, сохранила отступ, а он не успел записать данные на диск и упал. Мы получили потерю данных. 

Если произошла другая ситуация, к примеру, consumer успел записать данные на диск и упал, Kafka пришла проверить, что он жив, а он мертв. Отступ не сохранила, мы получим дубли данных. Т. е. такой подход не обеспечит вообще никакую семантику доставки. 

![](https://habrastorage.org/webt/yd/1p/y1/yd1py1sxbbl5jbeidltdoujkwty.png)

Следующий способ – это сохранение вручную. Вот consumer, он фетчит данные из Kafka. И после скачивания какого-то батча, он говорит: «Kafka, я этот кусочек данных скачал, сохрани отступы». Вроде бы все хорошо, но обеспечит семантику только at least once. 

At least once – это такая семантика, когда мы данные не теряем, но могут быть дубли. 

Почему так может произойти? Точно такая же ситуация. Предположим, мы сохранили кусок данных, но, когда мы делали данные, кто-то дернул <font color ="red">kill</font> -9 и ваш consumer благополучно упал, не успел сообщить Kafka, что нужно было сохранить отступы. Получили дубли данных. 

На самом деле эта семантика не такая плохая. Большинство pipeline по доставке данных работают как раз в семантике at least once и с последующей дедупликацией. 

Но мы гонимся за пресловутым exactly once. Как его обеспечить? 

![](https://habrastorage.org/webt/x6/je/e3/x6jee3efhw5urlk0lbflnwbwfoo.png)

Единственный способ обеспечить exactly once – это сохранять и данные, и отступы вне Kafka. При этом сохранение данных и отступов должно быть атомарное. Потому что, если оно будет не атомарное, то получаем ту же самую проблему. Мы могли сохранить данные, а отступы потерялись. Мы повторно их выключили. 

Если у вас в качестве целевой базы данных выступает какая-то полноценная база с транзакциями, то нет никаких в этом проблем. В нашем случае – это HDFS. Там транзакций нет. 

И я вам расскажу решение от LinkedIn. Оно используется в Gobblin для семантики с exactly once при фетче в HDFS.

![](https://habrastorage.org/webt/sn/7f/qq/sn7fqqf7l98gygxl-ixdyxcmj0s.png)

Предположим, у нас отработал consumer и сохранил 2 файла с данными и файл с отступами в какую-то папку. Затем их надо переместить в какую-то production-директорию. <font color ="red">Муф</font> в Hadoop атомарный.

Мы разбиваем последовательность этих сохранений на набор атомарных операций. И consumer их пробует по очереди выполнять, т. е. 1, 2, 3.

Предположим, мы первый выполнили, все хорошо. А на втором мы упали. Тогда при следующем запуске consumer’а, он первым делом посмотрит в эту временную директорию, какие файлы еще не были перемещены. Также у него есть информация о том, что должно было перемещено и куда. И он пытается завершить этот процесс. 

И если у него это получится, то все хорошо. Мы закончили предыдущий фетч данных при уже следующем фетче. Если это не получается, мы можем откатить. Таким образом можно обеспечить exactly once при сохранении в базах данных без транзакций. 

![](https://habrastorage.org/webt/gt/ks/qo/gtksqod7lkvxgk9bbfjandaijd8.png)

И вроде бы все хорошо. И на самом деле это хорошо. 

- Producer получает ответ об успешной записи данных. 
- Kafka реплицирует данные и данные внутри себя не теряет. 
- Мы фетчим данные в HDFS с семантикой exactly once. Exactly once – это семантика, когда мы забираем данные без потерь и без дублей, т. е. точь в точь.

![](https://habrastorage.org/webt/rd/gh/ze/rdghzezdf0opjyi_7zgqhmozxzo.png)

Все отлично, но у нас начались проблемы. В чем они начали проявляться? 

Во-первых, они начали проявляться в запаздывании логов. Большая часть pipeline – это обработка дневных логов, т. е. за день данные мы собрали и запустили их обработку на Hadoop. 

Если данные за вчера доезжают, к примеру, сразу после полуночи (в 1-2 ночи, где зеленая отметка), то все хорошо. 

Если они доезжают по какой-нибудь причине с большим опозданием, например, после обеда или с опозданием на день, то все плохо. Мы не успеваем их обработать, мы не успеваем посчитать нужные метрики, аналитику и прочее. В общем, данные стали к нам приезжать с большим опозданием. 

Во-вторых, у нас появились дубли сообщений. Эта пресловутая семантика at least once, которой мы так дорожим, была нарушена. 

![](https://habrastorage.org/webt/nr/jm/d3/nrjmd3f0ywjejxsjvudww_84mpw.png)

- Кроме этого, мы заметили очень медленную работу consumers на микробатчах, когда минутный батч данных из Kafka у нас фетчился по 2-3 минуты.
- А также consumers стали падать с ошибками. Все ошибки были на мотив: я не могу достучаться до Kafka, я не могу получить важную мне информацию от Kafka. 

![](https://habrastorage.org/webt/ct/jy/wa/ctjywahlsej9xtlqbyjpdjbooda.png)

И причина была общей для этих двух проблем: Kafka стала очень медленно отвечать на простейшие запросы, стала отвечать по несколько минут. Точнее отдельные ноды Kafka стали очень медленно отвечать на эти запросы. 

И когда мы фетчим получасовым батчем информацию, то мы эти пару минут можем даже не заметить, потому что надо запуститься раз в полчаса. А если мы фетчим каждую минуту, то эти две минуты будут намного более заметней. 

![](https://habrastorage.org/webt/zp/xe/pa/zpxepat7itqwcntmtkwnjbmt-fc.png)

Первым делом мы посмотрели мониторинг. Что мы увидели? Мы увидели, что наша нагрузка на чтение из Kafka выросла в 4 раза. 

Нижняя красная линия – это пиковая нагрузка до этих событий. Верхняя красная линия – это после событий. Желтый график – это график на чтение. 

![](https://habrastorage.org/webt/gk/jf/ll/gkjfll3wdidopqzhik-e3icb-ne.png)

Но еще более интересную картину мы увидели, когда посмотрели на эту же информацию в разрезе нод. Здесь просто какая-то лапша. Ее даже сложно разобрать. Это чтение с нод. 

Но основное, что отсюда можно вынести, это то, что в отдельные моменты времени чтение происходит с двух-трех нод кластера из пяти машин. А также здесь отлично видно переизбрание лидеров, когда Kafka лихорадочно пытается переизбрать лидеров, чтобы сбалансировать нагрузку и у нее ничего не получается. 

![](https://habrastorage.org/webt/sy/fh/lb/syfhlbmh1-18lynjerngajekxgi.png)

В чем была причина? Причина такого неравномерного распределения по нодам была в двух вещах:

- Во-первых, Kafka неравномерно распределяла партиции по нодам. 
- Во-вторых, лидеров по нодам. 

У меня здесь изображен гротескный случай. Но на самом деле он крайне близок к истине, примерно так все и было. 

![](https://habrastorage.org/webt/lz/8k/yo/lz8kyod0albnvcbwn8mpnqlat18.png)

Очень важно понимать, что, когда у вас даже все хорошо, т. е. у вас не вырос трафик в одночасье в 4 раза, Kafka все равно может ошибаться. И это проблема. 

![](https://habrastorage.org/webt/cf/du/h8/cfduh8v3qjqxzxesm8oxrczf9cq.png)

Как мы ее решали? Более-менее автоматизированного способа решения мы не нашли. Единственное решение, которое мы нашли – это распределять руками. В Kafka можно скормить конфиг, в котором можно описать, на каких нодах должны лежать какие-то партиции и какие из них должны быть лидерами. И это позволит построить картинку ближе к идеалу. 

Понятное дело, все, что я говорю, это верно для самых толстых топиков, через которые у вас проходит наибольшее количество информации. 

![](https://habrastorage.org/webt/7o/2v/-a/7o2v-a11xcosspqx5nivvwk-xse.png)

Следующее решение вообще не очевидно. Повышаем репликацию. Кажется, ребята, у вас и так трафик на чтение увеличился в 4 раза, а вы хотите его еще увеличить чтением на синхронную репликацию. 

Сейчас попытаюсь объяснить, почему это помогает. Вот снова наша гротескная ситуация: 

![](https://habrastorage.org/webt/da/qb/a7/daqba77lawjjhe1cqsl4haiqpoy.png)

Весь трафик пришелся на первый брокер. И на нем все лидеры. Он красненький, ему очень плохо. 

Что делает Kafka в этом случае? По какому-то тайм-ауту брокер не ответил, Kafka решает: «Ему плохо, дай-ка я сниму нагрузку». И получаем следующее: 

![](https://habrastorage.org/webt/x6/i3/ou/x6i3oum516abbb5mmzub_ds1mgo.png)

Весь трафик переезжает на другой брокер. И уже плохо ему. 

Именно эту картину вы и видели на графике в разрезе по нодам, когда дико колбасило, нагрузка по нодам так и скакала. 

А если бы у нас была репликация в топике, к примеру, 3, то мы могли увидеть после этого процесса следующую картину: 

![](https://habrastorage.org/webt/cy/lr/me/cylrmednadbo5lxpqmghmpug4zi.png)

Мы помогаем Kafka не ошибаться при распределении нагрузки по нодам, т. е. чем больше реплик, тем ей сложнее ошибиться. И это хорошо. 

Все, что я рассказываю, это было для Kafka 0.8.2. Возможно, в новых релизах сделали ее чуть умнее. Я на это очень надеюсь. Но для старых версий это очень актуально. 

![](https://habrastorage.org/webt/r9/af/ms/r9afmsehudibgjfcsoff7r6gt_m.png)

Также у нас случились довольно классические проблемы, которые, наверное, бывают во всех сервисах обработки данных. 

- Мы уперлись в сеть. У нас часть нод в стойке оказались с 1 Gb uplink. Это примерно 125 мегабайт в секунду. На графике у нас на ноду приходилось более 150 мегабайт в секунду. И понятное дело, сеть была запружена. 
- Мы внезапно узнали, что Kafka не дружит с RAID 5. 

Почему так получилось? Это жизнь. Документация читается в последний момент. Если у вас RAID 5 и все работает хорошо, то у вас все хорошо. Но когда у вас на RAID 5 теряется какой-то блок данных и начинается восстановление, в этот момент Kafka резко просаживается. 

Для Kafka рекомендуется использовать RAID 1.0 с зеркалированием, т. е. вы какие-то кусочки файлов зеркалируете по нескольким дискам, тогда это дешевле восстановить.

![](https://habrastorage.org/webt/uv/67/78/uv6778pvl9ksgbeoqzpn0kum2bs.png)

Мы все это поправили. И что мы в итоге получили? 

- Самое главное, мы равномерно размазали всю нагрузку. 
- Разобрались с сетью. 
- Разобрались с дисками. 
- Kafka стала отвечать реально за нормальное время, т. е. вместо минут стали секунды. 
- Но нагрузка осталась такой же высокой. 

![](https://habrastorage.org/webt/an/d7/a6/and7a6ggorpxnz2276wkr_j_y9e.png)

В чем была проблема? 

Как уже нетрудно догадаться, проблема была в повторном скачивании данных. У нас был at least once. 

И корень этой проблемы заключался в следующем. Мы использовали батчевый consumer данных с Kafka <font color ="red">Camus</font>. Это предок Gobblin. И он сохраняет отступы для топиков в директорию под названием history. И отступы всех топиков попали в одну папку. 

Вот здесь приведен фикс, исправляющий проблему. И, по сути, все отступы попали в одну директорию. 

![](https://habrastorage.org/webt/by/tb/di/bytbdi6snb37k1k6evsiv-yx2ce.png)

При этом Camus сам был заточен под то, чтобы работать одним запуском сразу для нескольких топиков. Поэтому мы иногда как теряли отступы после успешного фетча, так иногда они успешно обновлялись. Из-за этого нам было крайне сложно понять, что проблема именно в этом месте. 

Мы пофиксили. Так было: 

![](https://habrastorage.org/webt/tt/k8/go/ttk8gowyy7oasihb1k6iou0yojk.png)

Так стало: 

![](https://habrastorage.org/webt/8u/6u/m3/8u6um3pmfyygpnw728wyp-kva_u.png)

Это нагрузка на Kafka в нормальном режиме работы. В 4 раза упал пиковый трафик. А вот эти периодические пики, которые вы видите, это запуск нашего consumer раз в полчаса. Т. е. у нас на Hadoop работала задача, когда они каждые полчаса просыпаются, фетчат новые данные и обратно засыпают. 

![](https://habrastorage.org/webt/ij/aj/gu/ijajgu1srdkjiib9n-eie3vzn1m.png)

Немного цифр. К чему мы сейчас пришли?

У нас сейчас Kafka состоит из тех же 5 серверов, каждый день в нее записывается порядка 3 млрд событий. Это более 5 терабайт информации. Пиковая нагрузка на запись порядка гигабита. 

Каждый день из Kafka вычитывается порядка 18 терабайт информации. И пиковая нагрузка порядка 5 гигабит. Это на 5 тачек. 

Здесь стоит отметить, что мы сейчас уже держим нагрузку большую, чем при которой у нас проблемы, ранее описанные, но при этом все хорошо. Растем дальше. 

![](https://habrastorage.org/webt/bf/cx/zq/bfcxzqj1tjbn6-ycofz0hjzfxus.png)

И пара слов про мониторинг. 

В первую очередь стоит мониторить узкие места:

- Сеть,
- Диски,
- Распределение нагрузки. 

Вы должны за этим следить, Kafka может ошибаться. Вы должны ей в этом помогать. 

Вот, к примеру, график распределения нагрузки по нодам, когда у вас все хорошо: 

![](https://habrastorage.org/webt/hv/vh/2x/hvvh2x5qbvkvbtblv5k6-f0gdao.png)

Здесь можно увидеть, что 4 ноды работают более-менее в унисон. Одна немного отстает, но это не так страшно. При этом на этом графике ближе к концу заметно, как Kafka сделала leader reelection. И баланс немножко изменился. 

![](https://habrastorage.org/webt/m0/vi/q1/m0viq1gvzxx5vcbhi-zfpx4fdxa.png)

Во-вторых, стоит мониторить переизбрание лидеров. Бесспорно, вы на это повлиять никак не можете. Данные вы все равно теряете, но круто иметь количественную метрику того, насколько часто и насколько много данных вы теряете. 

![](https://habrastorage.org/webt/bu/zn/f_/buznf_tgujlmrqwk72f9g4wfltu.png)

В-третьих, стоит мониторить рассинхронизацию партиций. В любой, наверное, системе, где есть синхронная репликация, на это стоит смотреть. Метрик для этого целое множество. Из тех, которые нравятся мне, это:

- Число несинхронных партиций из тех, которые должны быть insync.
- Максимальный лаг репликации. 

Это график максимального лага репликации с точностью до нод, насколько сообщений мы отстаем:

![](https://habrastorage.org/webt/8x/nj/tf/8xnjtfnudaaldbmj6xmfqggjbem.png) 

Что по этому графику видно? Все хорошо, лаг в пределах нуля с приличными пиками. И это нормальная ситуация. Так все хорошо. 

Начинаются проблемы, когда у вас лаг стабильно больше нуля, т. е., по сути, ваша система не успевает вовремя реплицировать всю поступающую в нее информацию. 

![](https://habrastorage.org/webt/ho/o7/-z/hoo7-z8cz0blxmrzcrm7cs9kymc.png)

И, в-четвертых, время ответа на простейшие запросы. Это, наверное, самая главная метрика. Сама по себе она мало что значит, если только вы не хотите с помощью Kafka делать около real time обработку данных, но она является очень важным симптомом того, что с вашей Kafka стало происходить что-то плохое. 

Здесь изображено время ответа на запросы consumer’а: 

![](https://habrastorage.org/webt/7p/yi/vr/7pyivrxqd35zdddoe63ribmu5p8.png)

И красной линией выделено отсечка в 10 секунд. Мы сейчас ее используем. Она эмпирическим образом найдена. Если Kafka отвечает меньше, чем за 10 секунд, наши обработки данных работают хорошо. Если Kafka отвечает больше, чем за 10 секунд, кричим: «Караул!».

И на этом графике есть два пика. По ним сразу видно, что голубенькой ноде очень плохо. Надо разбираться, почему она периодически начинает отвечать так долго. При этом плохо не только ей, но и другим нодам. 

![](https://habrastorage.org/webt/8f/rr/02/8frr02-wgnwrtbhrnf331o7uad0.png)

И в качестве итогов: 

- Если вы хотите обеспечить exactly once семантику доставки данных на Kafka, то это возможно. Но вы должны смотреть на все элементы pipeline. Вы должны знать, понимать, как это работает и внимательно контролировать producer, consumer и Kafka. 

  Когда вы имеете много поставщиков данных, то крайне сложно за всеми уследить, но в жизни обычно не все данные должны следовать семантике exactly once. А за теми, которые должны, нужно внимательно следить. 

- Нужно мониторить узкие места. В нашем случае при повышении нагрузки первое, что у нас вылилось, это сеть, диски и баланс нагрузки по нодам. 

- Также стоит мониторить много метрик. Самая главная – это время ответа Kafka на запросы, причем как продюсера, так и consumer. И стоит мониторить классические метрики типа: pool, memory. 

На этом все. Ваши вопросы?

![](https://habrastorage.org/webt/qo/k5/qh/qok5qhexypwiwycqglsusz1pbyy.png)

**Вопросы**

*Можно подробней, почему* *Kafka* *так внедрили? Что поменялось, когда вы внесли обработку* *offsets* *на* *consumer*?

Обработка отступов никак не связана с балансом нагрузки. Она связана с обеспечением семантики exactly once. Если вы сохраняете их не в Kafka, а вместе с данными, у вас появится шанс этот процесс сделать почти атомарным и получить нормальный exactly once. 

Про баланс. Мы глубоко в кишки Kafka не вдавались. Почему именно Kafka? Когда человек посмотрит на ситуацию, он примет другое более оптимальное решение. Почему алгоритмы не умеют ситуацию разруливать – мы не в курсе, работаем с тем, что есть. 

*Я правильно понял, что возрастание нагрузки на чтение на ноды, оно произошло после перехода на схему с* *offset* *…*

У нас сначала появилась Kafka. Исходно была схема сохранения offset вне Kafka. А спустя год мы словили эти проблемы. При миграции мы допустили ошибку в конфигах. И из-за этого consumer стал повторно вычитывать те же самые данные. Это вырастило нам нагрузку в 4 раза. 

*Если это была ошибка в конфиге и вы ее могли потом поправить, почему так сделали?*

С исправлением всех этих проблем у нас сейчас Kafka переваривает больше трафика на чтении и с ней все хорошо. 

*Ладно, просто не очевидно по графикам, что у вас происходило.* 

Понял.

*Здравствуйте! Спасибо за доклад! Меня интересует такой момент. Вы для того, чтобы обеспечить семантику* *exactly* *once*, *сделали что-то вроде транзакций в* *Hadoop*?

Да.

*Мне не совсем понятно, как вы откаты транзакций сделали в случае, если произошла какая-то ошибка?*

Сейчас отмотаю на эту штуку:

![](https://habrastorage.org/webt/sn/7f/qq/sn7fqqf7l98gygxl-ixdyxcmj0s.png)

На самом деле все довольно просто. Это не наше решение, это решение от LinkedIn. Мы используем готовый consumer. И там такое используется. 

Откат можно произвести следующим образом. Мы знаем, какие файлы и куда должны были быть перемещены. Если мы в итоге так и не смогли завершить, условно говоря, эту транзакцию, мы эти файлы просто удаляем, т. е. целевые. 

*В последствии вы же потом их заново перечитываете?*

Да-да.

*Т. е. это не совсем* *exactly* *once* *получается.* 

В итоге мы получим exactly once. Т. е. получается такая штука, что в рамках одного запуска consumer’а какие-то данные могут не доехать. Но учитывая, что он запускается регулярно, каждый следующий запуск фиксит проблему предыдущего. И суммарно у нас нет ни потерь, ни дублей. 

*Здравствуйте! Спасибо за доклад! Почему вы не храните информацию об* *offsets* *в* *ZooKeeper*?

Проблема именно в том, что мы не можем атомарно сохранить и данные, и отступы. Данная эта схема – это стандартная схема для хранения отступов в ZooKeeper. И большинство consumers используют именно ее. Но в этом случае, если вы данные уже успели сохранить, но у вас по пути в ZooKeeper что-то произошло, вы получили проблему.

*Тогда второй вопрос. Я правильно понимаю, что вы используете* *Kafka* *версии 0.8.2?*

Уже нет. Но проблемы с балансом мы наблюдаем до сих пор. У нас сейчас стоит 0.10.2. И те же самые наблюдаются проблемы с балансированием партиций по нодам и лидеров по нодам.

*Ожидание подтверждения от второй реплики, насколько ухудшает временные показатели?* *Вы замеряли это?*

Да, мы замеряли. У нас оно деградировало в разы. Но там было несколько миллисекунд, стали десятки миллисекунд, т. е. для нас это было допустимо. 

*Здравствуйте! Спасибо за доклад! В* *ZooKeeper* *у вас не получилось нормально и персистентно гарантированно записывать* *offsets*, *но получилось записывать их в* *HDFS*, *правильно?*

Да. Мы храним их в HDFS. 

*Но* *ZooKeeper* *у вас тоже кластеризованный?* 

ZooKeeper используется вот так:  

![](https://habrastorage.org/webt/x6/je/e3/x6jee3efhw5urlk0lbflnwbwfoo.png)

Он никак не связан с хранением отступов.

*Это внутрикавочный* *ZooKeeper*?

Да, он стоит на тех же самых нодах, что и брокеры, просто параллельно, т. е. на них размазаны еще instances ZooKeeper. 

*И еще такой вопрос. Как* *exactly* *once* *обеспечивается на уровне продюсеров?*

Это наша больная тема, потому что с нашей стороны преимущество в том, что мы сами даже что-то в Kafka пишем, но очень сложно уследить за каждым продюсером, как он допишет. В идеале это должно происходить примерно так: продюсер знает о каждом сообщении, которое он пишет в Kafka. И если у него не было полученного подтверждения, он пытается его переотправить. Это идеальная ситуация. Но мы не можем контролировать всех поставщиков данных, это много-много других проектов. 

*Какие-то другие варианты кроме* *Kafka* *рассматривали для такого общего интерфейса?*

Какие у нас были требования? Нам нужно было через нее каждый день передавать терабайт информации, при этом каким-то малом количеством железа. 

Также мы хотели иметь возможность вычитывать данные заново. На самом деле с Kafka одни и те же данные за последний интервал времени, например, за неделю, считает множество поставщиков. Мы каждый топик вычитываем по несколько раз. К примеру, offline pipeline и online pipeline обработки. Плюс этим еще занимаются и другие отделы. 

Нам хотелось не терять данные, пока они там. И при этом хотелось какой-нибудь популярный проект, у которого есть сообщество, которое активно развивается. И у нас больше выбора особо и не было в то время. Это было года два назад. 

*Здравствуйте! Спасибо за хороший доклад! Насколько критично для вас потеря нескольких сообщений? Например, за день 20 сообщений? И если это критично, то как вы мониторите соотношение отправленных и полученных сообщений?*

К нам каждый день приезжает 3 млрд сообщений. Если мы потеряем несколько десятков из них, ничего страшного не произойдет. Но есть часть данных, которые используются, к примеру, для точной аналитики, либо используются для счета денег. И там нам потеря 10 сообщений критично. 

К примеру, считаем клики для какого-то баннера за интервал в несколько часов. Их там будет 20 штук. У нас получалось 20 строк и среди них 5 были кликами на этот баннер. Итого у нас на 20 % просела итоговая аналитика, а это уже большая проблема. 

Все зависит от случая. И мы следим за точной доставкой только для нескольких топиков. 

*Ясно. Если не секрет, как вы это делаете? Какой механизм?*

У нас сейчас проходит процесс автоматизации всей этой деятельности. Основной способ такой: сходить в хранилище данных, если оно есть на стороне поставщика, получить нужную нам метрику и сравнить с тем, что есть у нас есть. Все, все довольно просто. 

Единственное, это можно сделать не всегда. К примеру, если поставщик вообще не хранит эти данные, он сразу пишет в Kafka и забывает про них, т. е. они ему особо не нужны, то это сделать нельзя никак. Единственный способ – это мониторить на уровне продюсера. 

*Спасибо за доклад! У меня вопрос по доработке, которую вы сделали, т. е. о хранении вместе с данными отступов. Это решение основано на еще каком-то решении от* *LinkedIn*?

Нет, это не наше решение. Я рассказал вам решение, которое используется в Camus или в Gobblin.

*Это закрыто, открыто?*

Это открытый исходный код, который лежит на GitHub. Его можно взять, посмотреть, как это работает. Никакой проблем. 

Кстати, сделаю небольшую рекламу. Если вы используете Kafka, то Camus и Gobblin – это лучшее решение для батча и фетча данных из Kafka. Мы проводили тестирование множества решений, лучше не нашли. 

Всем спасибо!
