Григорий Кошелев – А вы Кафку пробовали

 Apache Kafka — распределённый программный брокер сообщений, применяемый в обработке в реальном времени данных большого объёма. К отличительным особенностям Apache Kafka можно отнести: надёжность, масштабируемость и высокую производительность. В докладе разберём основные архитектурные особенности и сценарии использования Apache Kafka. Рассмотрим неочевидные моменты и грабли, которые мы собрали на пути Востока.

![](https://habrastorage.org/webt/vp/m7/ko/vpm7ko62n5acusry8shwqjnavfc.png)

Всем привет! Меня зовут Григорий! И сегодня мы поговорим про Kafka.

![](https://habrastorage.org/webt/t2/93/_1/t293_1j3vi26fd0zqu8l_ococ58.png)

План у нас будет такой:

- Вначале я расскажу для чего нужна нам Kafka, что мы с ней делаем.
- Потом потихоньку начнем разбираться, как она устроена, т. е. введение в Kafka. 
- Далее по архитектуре отдельных компонентов пробежимся. 
- И самая интересная часть доклада – это какие-то неочевидности. По-хорошему здесь надо было писать «грабли, боль и страдания», но оставим немного интриги. 
- А в конце подведем итоги. Сделаем выводы и поймем: надо ли с этим что-то делать и как вообще жить.

![](https://habrastorage.org/webt/hb/6z/v8/hb6zv82hvqcrmguadqeantcpio4.png)

Зачем нам Kafka? 

В какой-то момент у нас в компании появился проект [Vostok](https://github.com/vostok). 

Это тот инструмент, который позволяет масштабировать и разрабатывать микросервисную архитектуру с минимальными затратами. Т. е. он конкретно под проект [Hercules](https://github.com/vostok/hercules) заточен. Он нацелен на то, чтобы со всех приложений, со всех тысяч микросервисов уметь собирать логи, метрики, распределенные трассировки сетевого взаимодействия между сервисами. 

И также мы хотим универсальное какое-то решение получить, чтобы наши пользователи, наши проекты могли передавать бизнес-события через такую трубу. 

А также в нашей компании Kafka используется для поисковых и рекомендательных систем. 

![](https://habrastorage.org/webt/2f/gf/bu/2fgfbuy91inzoatmn20no8rkf1w.png)

Т. е. у нас есть много источников различных интересных данных, по которым можно строить интересные штуки, рекомендательные системы. Kafka где-то это все агрегирует. На другом конце находится Spark, туда заливаются данные, вычисления происходят. А также поисковые движки могут стоят на другом конце. 

Для первого проекта у нас используется Kafka версии 2 и выше, т. е. самую последнюю версию используем. 

Почему мы это можем делать? Потому что у нас там Java stack. 

![](https://habrastorage.org/webt/gx/jx/sq/gxjxsqkd9kcipghkkltoffl9jqy.png)

Для рекомендательных систем используется Kafka версии 0.11, потому что нет клиента ни для Python, ни для DotNet для Kafka версии 1 и выше. У нас в компании основной stack – это DotNet, поэтому ребятам очень тяжело. И поэтому они старую Kafka использует. 

В каждой версии Kafka есть свои баги, свои проблемы. И они либо перетекают из версии в версию, либо все-таки фиксятся. Может даже что-то меняться в архитектурном плане. И к этому надо быть готовым при апдейте своей версии. 

![](https://habrastorage.org/webt/ty/hq/jn/tyhqjns1kkqk_8zsgch53cthy9a.png)

Давайте разберемся, что такое Apache Kafka. 

Для тех, кто не поднимал сейчас руку, поясню, что у Kafka много компонентов. Первый из них – это producer. Producer – это тот, кто создает кучу сообщений и отправляет их куда-то. 

![](https://habrastorage.org/webt/ub/g0/ug/ubg0ugd0m-sqivf_onuk2ujfcbm.png)

На другом конце находится consumer. Они употребляют эти данные, что-то с ними делают. 

![](https://habrastorage.org/webt/nf/tu/gg/nftugg6gqwaiyjs6upzcdsrceh4.png)

Передача осуществляется через кластер. Там у меня было написано, что Apache Kafka – это распределенный брокер сообщений. Получается, что у нас есть в кластере куча брокеров. 

![](https://habrastorage.org/webt/t1/xr/o7/t1xro7pxrytxinxqkkpbimmzvoo.png)

У нас в кластере есть куча брокеров. 

![](https://habrastorage.org/webt/pj/x9/tb/pjx9tbzyafuw3dshtf_txd0f6w0.png)

Передача сообщений осуществляется через них. 

![](https://habrastorage.org/webt/te/o3/xb/teo3xbaqx_k7opfqei2fyslxuns.png)

И там брокер выступает таким звеном, который позволяет от producer к consumer не напрямую данные передавать, а через такой топик. Опытный слушатель может сказать, что это обычный Message Queue. Обычный, но не совсем. 

![](https://habrastorage.org/webt/uv/mt/wr/uvmtwrvobiujc82_73lwif9qhyq.png)

Там используется Publish-Subscribe, т. е. у нас просто продюсер пихает сообщение в топик, а consumers могут подписываться на них. И одни и те же сообщения могут читаться разными consumers. 

И важно, что для чтения используется poll-механика, т. е. брокер не должен говорить: «Вот, consumer, тебе новое сообщение. Забери его». Каждый consumer должен прийти к Kafka и сказать: «Есть что-то новое?». И она отдает данные. 

Такой подход лучше масштабируется, чем, когда сам брокер начинает вливать данные в consumers.

![](https://habrastorage.org/webt/su/bm/wa/submwauqfoju08catrou0ahgdbq.png)

Мы увидели 4 важные вещи. Это топик, брокер, producer и consumer. Давайте по ним и пойдем в таком порядке. 

![](https://habrastorage.org/webt/lc/lv/tv/lclvtvss9ze4lsby07cnf_mufpu.png)

Топик – это логическая единица, которая связывает между собой producers, consumers. И есть какое-то физическое хранение. Каждый такой топик – это множество партиций. 

В данном случае у нас топик с тремя партициями. В них записано сообщение. Что при этом важно? 

![](https://habrastorage.org/webt/xf/wy/3z/xfwy3zapgfg5bqbgfktv5w8tywq.png)

- Сообщения всегда пишутся в конец партиции. 

![](https://habrastorage.org/webt/gp/ne/dg/gpnedg_hcdyhlgxzmw23ejtkax0.png)

- Писать можно параллельно в несколько партиций одновременно. Они между собой никак не связаны, физически это разные вещи.

![](https://habrastorage.org/webt/p_/f3/jh/p_f3jhcfo6oze8bus5qinoufrzk.png)

При этом у каждого сообщения есть свой номер, свой offset. Каждая партиция начинается с номера 0. 

![](https://habrastorage.org/webt/xe/yn/d5/xeynd5qzs2yauubnaahyxvmqcee.png)И там они увеличиваются на единичку. Offset = 0

![](https://habrastorage.org/webt/j-/ii/tf/j-iitfxwubvkrr4utrrfqmw-rwu.png)

Все вроде бы просто. 

![](https://habrastorage.org/webt/_g/g7/wn/_gg7wnqgh451dmnynvtmmxndsmc.png)

Но партиция может стать очень большой. Она может начать весить десятки, сотни гигабайт. А видео одного такого файла не похоронить, поэтому каждая партиция делится на сегменты. 

Вот у нас такая длинная-длинная партиция. 

![](https://habrastorage.org/webt/ex/9f/76/ex9f761cnm1xskbb9jckzp7egi8.png)

Она делится на кусочки. 

![](https://habrastorage.org/webt/zi/_1/bh/zi_1bhusjxfss4de2j6sbgjgvbq.png)

Они примерно одинакового размера, т. е. 1 сегмент, 2 сегмент и т. д. 

![](https://habrastorage.org/webt/x7/bp/ox/x7bpoxxqid7uv6dgz6kjzm503ju.png)

И есть последний сегмент. 

![](https://habrastorage.org/webt/w_/e2/5r/w_e25rztejxibtppojdzwo1r2kw.png)

Последний сегмент еще по-другому называется активным сегментом. Это тот сегмент, в который можно писать данные. Благодаря этому сохраняется правило, что пишем всегда в конец партиции. Это логично. 

![](https://habrastorage.org/webt/mc/ks/hv/mckshvwj4-x04la0kbtbdefxety.png)

У нас каждый такой сегмент начинал с некоторого сообщения. 

Вот такой базовый offset, с которого начинается каждый из сегментов. 

![](https://habrastorage.org/webt/rz/sn/ap/rzsnapivie1eeaeoexbu65rpass.png)

Он состоит из четырех вещей. Про базовый offset уже понятно. 

![](https://habrastorage.org/webt/gf/im/jd/gfimjd1pqvdl67ekvw3griftzei.png)

Он используется для того, чтобы называть файлики, которые лежат в файловой системе. И также data, index, timeindex – это три файла. 

![](https://habrastorage.org/webt/sh/ob/ns/shobnsawvfswghh37uhtwybhzka.png)

Data – это наши данные, т. е. есть какой-то здоровый файл. И в нем лежат сообщения. Сообщения могут быть разного размера. Kafka все равно, складывайте, что хотите в нее. 

![](https://habrastorage.org/webt/kd/qp/sg/kdqpsge229mnrmsw8o9804ehfvy.png)

Что такое index? Если мы хотим найти какое-то сообщение по смещению, то надо уметь это делать быстро. Как раз index позволяет это делать. 

![](https://habrastorage.org/webt/ct/xe/5d/ctxe5d792bkbogcsluydh6v7r_w.png)

Он выглядит следующим образом. Каждая запись в index занимает в общей сложности 8 байт. Там два инта: relative offset, positon. 

Relative offset – это смещение от начала сегмента. 

Берем offset сообщения, вычитаем базовый offset, получаем вот это число. 

Сообщений в партиции может быть миллиард, поэтому int переполнится, а базовый offset маленький, у нас сегмент небольшой, поэтому он влезает в int. 

Тоже самое с position. Position – это просто физическое смещение данного сообщения в этом лог-файле. 

![](https://habrastorage.org/webt/up/4u/sp/up4uspjkjcg1dzjusgni97u1nsi.png)

Получается, что у нас растет relative offset на единичку, позиция смещается относительно размера предыдущих сообщений. 

![](https://habrastorage.org/webt/g2/2y/cl/g22ycll6yvax5ohnaytpsglffke.png)

Он потихоньку растет-растет. 

![](https://habrastorage.org/webt/yx/lb/5p/yxlb5p4rgcrvhbzgoeqebnzuj34.png)

И так можно будет потом искать по этому индексу. Мы знаем номер сообщения, который мы хотим найти. Быстро через базовый offset находим relative offset и в индексе смотрим его позицию, и можно забирать. 

Понятно, что там не все сообщения хранятся, а только какие-то выборочные. Там есть шаг, с котором их можно сохранять в индекс. Это все настраивается в Kafka. 

![](https://habrastorage.org/webt/3w/5o/5o/3w5o5ofw4ljhtcf9gfeq-gpaiza.png)

Похожим образом устроен timeindex. Для чего он нужен? Каждое сообщение в Kafka имеет какую-то метку времени. И неплохо бы уметь искать по этой ветке времени. Timeindex решает эту задачу. 

![](https://habrastorage.org/webt/v6/ls/nl/v6lsnljaa4v5f3hzo7n2z4va4h4.png)

Вернемся к кластеру, к брокерам. Кластер – это множество брокеров. 

Один из них отвечает за контроллер. 

![](https://habrastorage.org/webt/c3/td/mi/c3tdmivdhbpobugqqlgmf-hlqcq.png)

Он координирует работу кластера. О его назначении мы поговорим чуть попозже. 

![](https://habrastorage.org/webt/fp/ve/u6/fpveu6_gfolkpmheg94e9axg2ya.png)

У нас есть топик, который состоит из нескольких партиций. В данном случае мы создали топик на кластере из трех брокеров. И партиции распределились по кластеру. 

При этом Kafka надежная, у нее есть replication factor для каждого топика. И мы можем сказать, что он равен трем. 

![](https://habrastorage.org/webt/vy/mq/sa/vymqsacqthjfnlp_reths2_g5wi.png)

Это означает, что каждая партиция должна иметь три копии. У нас три копии, поэтому на каждом брокере будет по копии. 

Вот у нас 4 партиции на одном, на втором и на третьем. 

При этом Kafka позволяет добавлять новые партиции к топику. 

![](https://habrastorage.org/webt/re/p2/uj/rep2ujtlnlo-wodfyfgvyin1wkw.png)

Поэтому, если у нас данных станет много, можно увеличить единицу параллелизации.

Партиция – это единица параллелизации, поэтому можно добавить еще одну. И там producers стало больше, которые могут писать с той же эффективностью. 

Теперь поговорим о роли контроллера. Дело в том, что контроллер должен назначить лидера. Каждая партиция должна иметь своего лидера. 

![](https://habrastorage.org/webt/2h/ru/lz/2hrulz42siengjdhwk2y7wnwioc.png)

Лидер – это тот брокер, который отвечает за запись в конкретную партицию. Не может быть у одной партиции несколько лидеров. Иначе каждый из них что-то написал бы свое, а потом это никак нельзя было бы соединить в какую-то одну последовательную партицию. 

![](https://habrastorage.org/webt/yr/dr/bb/yrdrbbza19dyi2ahmejiudhe5nk.png)

И каждый брокер может стать лидером у некоторых партиций. 

![](https://habrastorage.org/webt/gz/vc/oc/gzvcochiye6vaed-suph_mlriey.png)

В данном случае получилось так, что у нас на одном брокере лидерство двух партиций. На одном всего одна. 

Kafka старается это равномерно распределять по кластеру и балансировать, чтобы не получилось так, что один брокер пыхтит и в себя все пишет-пишет, а остальные отдыхают. Чтобы такого не было, она равномерно пытается распределить. 

![](https://habrastorage.org/webt/cj/xk/_u/cjxk_uylpbmo_cik3qnrmu5fqmq.png)

Что происходит дальше? Данные, которые пишутся в лидера, должны быть распределены по репликам. Те, кто в себя тащат данные из лидера, называются follower. Они следуют за ним и потихоньку подкачивают к себе данные. 

![](https://habrastorage.org/webt/js/cr/d4/jscrd4-pdeqoidsp0x5spuncuc0.png)

После того, как они сохранили в себе все данные, они становятся крутыми, они становятся in sync replica. Это реплика, которая синхронизирована с лидером. 

![](https://habrastorage.org/webt/wt/8x/wd/wt8xwdm75gtgeapug2iaoafkdi0.png)

И в идеале у нас весь кластер должен быть синхронизирован, т. е. все реплики должны быть в списке in sync replica. 

Понятно, что какая-то из реплик может выпасть. Если это follower, то ничего страшного? Почему? Потому что follower ходит в лидер за данными. И подумаешь, если один из них перестанет ходить. 



![](https://habrastorage.org/webt/w2/uu/ij/w2uuij2wt7bg2b3ls88r9c4czsu.png)

А что произойдет, если сам лидер пропадает? 

У нас был лидер в брокер 3 на партиции 2, а потом пропал. В этот момент мы данные писать не можем, потому что лидера нет.

![](https://habrastorage.org/webt/o3/j6/ph/o3j6phntbx5vr6em7m_nohgkioi.png)

 Kafka не теряется, она выбирает нового лидера. И за это отвечает контроллер. Все, отлично. 

![](https://habrastorage.org/webt/r8/vv/fu/r8vvfumalk2y_07fb4-ajk08zni.png)

Теперь с него можно реплицировать данные по другим брокерам. Но в партиции старый лидер может ожить. 

![](https://habrastorage.org/webt/bp/kz/vs/bpkzvsl_6-pte6qosfiifgs8dus.png)

Ожил, у него все хорошо, но он успел отстать по данным, потому что лидерство сменилось, появились новые данные. 

![](https://habrastorage.org/webt/gl/oj/ej/glojejr1fujhdhd_yjrbqvxoonm.png)

И поэтому он вынужден уже реплицироваться с нового лидера. Он это сделал. Кластер восстановился. И все хорошо. 

![](https://habrastorage.org/webt/ic/_b/zi/ic_bziozp1pbhk3mlpabhokb-h4.png)

И потом в Kafka случается магия, которая – раз и возвращает лидера обратно. 

![](https://habrastorage.org/webt/yc/cx/lp/yccxlptwojwmgsgscygylzhdauo.png)

Для чего это сделано? Если мы последовательно будем перезапускать ноды, мы можем взять и согнать лидерство на одного брокера. Это не очень хорошо. Он будет отвечать за всю запись, а остальные брокеры будут отдыхать. И чтобы этого не было Kafka периодически умеет это дело перебалансировать. И у нее есть куча настроек, которые это дело регламентируют. 

![](https://habrastorage.org/webt/vf/-g/rp/vf-grpefuljfhn5yg3vgbgjkm5e.png)

Теперь пойдем к продюсеру, т. е. поговорим о том, как данные пишутся. 

Начнем с сообщения. Сообщение можно представлять, что это пара ключ, значение. 

![](https://habrastorage.org/webt/oj/ei/du/ojeidus0mcdhhaha_oh3anbag8y.png)

При этом для Kafka и ключ, и значение – это просто массив байтов, в который можно писать все, что угодно. Продюсер должен знать, что он туда сериализует. И consumer должен знать, что он туда десериализует. 

![](https://habrastorage.org/webt/vi/9f/xu/vi9fxu5nyhjoucoofyf5mpwkzrg.png)

Зачем нужен ключ? Ключ используется для определения номера партиции, куда положить данные. Используется [MurmurHash](https://ru.wikipedia.org/wiki/MurmurHash2) в том случае, если ключ есть. 

![](https://habrastorage.org/webt/rx/er/hz/rxerhzrihazfqpmeq8h_vc7ip64.png)

Если ключа нет, то используется round robin, когда продюсер перебирает партиции по кругу, т. е. дошел до конца, начинает с нулевой и т.д. 

И при этом важный point: куда положить данные решает продюсер. Т. е. не брокер как-то там сам определяет и сохраняет, а именно продюсер. Продюсер должен всегда писать в лидера. Соответственно, он пишет в конкретного брокера, в конкретную партицию. Это важно.

![](https://habrastorage.org/webt/ke/wz/qd/kewzqduocrxn6bt9ommkj5x7doc.png)

Про ключ я уже сказал, что Kafka это интерпретирует как массив байтов и так же сохраняет в лог, как и другую метаинформацию. Другая метаинформация – это offset, timestamp и т. д. 

![](https://habrastorage.org/webt/hv/zg/fa/hvzgfarfz4qxquejyxr3ipmzlqk.png)

У нас есть одна партиция. И у нее три реплики. Они синхронизированы. 

Одна из них является лидером. В каждой по 9 записей. 

![](https://habrastorage.org/webt/qb/lo/n2/qblon2vban8p4sslscy_loztjzq.png)

Теперь посмотрим, как работает продюсер. Есть такая штука в Kafka, как acknowledgement, т. е. подтверждение записи. Продюсер должен убедиться, что данные записались 



![](https://habrastorage.org/webt/ea/kl/ht/eaklhtbkvltok8loh1qgp4nnwms.png)

Нулевой уровень предоставляет нулевые гарантии. Как это работает? 

![](https://habrastorage.org/webt/m-/sc/eq/m-sceq1qz_iobyz493gxz-elxta.png)

Продюсер начинает писать сообщение в одной из партиции. Он передал все данные и говорит, что у него все хорошо. 

![](https://habrastorage.org/webt/td/ch/jt/tdchjts3i5acmisfcappb43ze1k.png)

Ему не надо никакого подтверждения. 

При этом данные могли записаться в Kafka, а могли не записаться. Там могла быть какая-то проблема. И лидер в реплике 0 мог не сохранить данные. И тут уже может быть потеря данных. Это нужно понимать. 

![](https://habrastorage.org/webt/xq/c-/ar/xqc-arwtswus3sbp-irxxx0qyfc.png)

Поднимаем уровень. Уровень гарантии стал 1. 

![](https://habrastorage.org/webt/lm/q2/94/lmq2948untc4q_dtmafwbytof0y.png)

Что изменилось? Теперь продюсер пишет данные. Он передал все данные в Kafka. Kafka пишет. Он ждет. 

![](https://habrastorage.org/webt/js/t4/xn/jst4xncyzh9krnazzq9ifh6fzpe.png)

Запись произошла успешно на брокера. Брокер вернул подтверждение, что сохранил. 

![](https://habrastorage.org/webt/2n/9p/it/2n9pitrga236ftjhi-n6cp7ifnu.png)

После этого продюсер считает, что все хорошо. 

Какой следующий шаг? 

![](https://habrastorage.org/webt/vm/sy/vi/vmsyvipnoyodraqd2y4qjkbi58c.png)

Followers приходят к лидеру и говорят: «У тебя есть новые данные?». Он говорит: «Да». И они это забирают. А могли и не забрать. Это важно. Об этом мы, может быть, позже подробней поговорим. 

![](https://habrastorage.org/webt/sd/1j/j1/sd1jj1tbwfiitp0uqnniufxkdj8.png)

И, наконец, максимальный уровень. Уровень all, когда идем на все. 

![](https://habrastorage.org/webt/tc/kg/fj/tckgfj3mg2adn_tajv8kw93i2ng.png)

Как это работает? У нас продюсер пишет данные в лидера. 

![](https://habrastorage.org/webt/zr/jc/dg/zrjcdgzuek8xwjbnpfzjet19x9s.png)

Лидер сохранил, но ничего пока не говорит продюсеру. 

![](https://habrastorage.org/webt/ob/r-/_d/obr-_dgwbmc7y82ajzitadyffpw.png)

Дальше followers фоном приходят за данными. 

![](https://habrastorage.org/webt/wc/0k/dz/wc0kdza1zjucvpmzhutcg9plqbc.png)

И начинают их фетчить с лидера, пишут к себе. Записал первый, записал второй. 

![](https://habrastorage.org/webt/ff/ru/9k/ffru9kbx6g9zvyto-tsvkaex9r4.png)

Брокер, который лидер, понимает, что данные записаны на всех и говорит продюсеру, что данные записались. 

![](https://habrastorage.org/webt/tv/bm/de/tvbmdeztf0oaol8dotpwieuvlz8.png)

После этого продюсер считает, что все хорошо. 

И тут появляется важная настройка появляется. Это min.insync.replicas. 

![](https://habrastorage.org/webt/yi/ce/sy/yicesyc39kkngazcujprszzvrfs.png)

Сейчас это равно 3, поэтому лидер уведомляет продюсера, что данные записались не меньше, чем в эти 3 реплики, т. е. в себе + еще 2. 

Эту настройку можно понизить, тогда можно дожидаться не всех, а, допустим, `все-1`.

![](https://habrastorage.org/webt/mx/wh/jp/mxwhjpfs3kbqxhcmx4ewzq92w3g.png)

Теперь посмотрим, как работает consumer. У нас есть несколько партиций. Пока consumer будет читать из одной партиции. Он еще ни разу не читал, он читает с самого начала. Вот он прочитал какое-то количество сообщений. 

![](https://habrastorage.org/webt/5s/jz/ml/5sjzml1lufuely1wirfnflhtf0u.png)

Потом перешел к следующему моменту, и так постепенно дочитывает. 

![](https://habrastorage.org/webt/fw/yn/ca/fwyncawvzyoaszol3lvmg5igba8.png)

![](https://habrastorage.org/webt/3s/55/g5/3s55g5oti5ouva95hawwa-yucbw.png)



![](https://habrastorage.org/webt/gn/c9/ti/gnc9tiwjvsciubx7yjtqvhayzqa.png)

![](https://habrastorage.org/webt/5f/_t/jl/5f_tjlaibos9ktppf7r79j2cnsq.png)



Также он может читать из нескольких партиций одновременно. 

![](https://habrastorage.org/webt/jb/xo/gg/jbxoggslhvxppiqhjc_chdmqdpy.png)

Как это выглядит? 

![](https://habrastorage.org/webt/mq/qm/vc/mqqmvcra2nwmytdruemr1cj3nqm.png)

Он на каком-то месте остановился и начинает забирать данные. 

![](https://habrastorage.org/webt/nh/_k/rq/nh_krqm736hbvvgwcxoeeis04m8.png)

Он забрал данные. 

![](https://habrastorage.org/webt/mh/ef/jw/mhefjwlzsi7ocsxc4nkpfzfxoxu.png)

Пошел дальше-дальше и дошел до конца. 

![](https://habrastorage.org/webt/1m/fp/fp/1mfpfpvb0qfwwvwii613dpvlydc.png)

![](https://habrastorage.org/webt/tn/v5/he/tnv5hepdyfs42sbqtng6smzizgc.png)

![](https://habrastorage.org/webt/3_/6a/gm/3_6agm4qxskrjdv7ruzadxhavqc.png)

![](https://habrastorage.org/webt/8q/pf/0u/8qpf0uvwv7g1y9y6u3neg-hqpaa.png)

А в version polled сообщений в Kafka еще нет. И он будет продолжать делать это до тех пор, пока данные новые не появятся, и он их не получит. 

Но есть маленькая тонкость. Когда вот это он делал, он запоминал место, с которого надо читать дальше. Он мог и перезапуститься, мы могли прибить процесс. И эта информация потеряется. Это понимали и те, кто делали Kafka. И поэтому добавили такую фичу, как commit offset. 

![](https://habrastorage.org/webt/x8/m3/wl/x8m3wlazxqbvoezriz7_d9sx5m8.png)

Что это значит? 

![](https://habrastorage.org/webt/pz/kx/yt/pzkxytdlspsk0q_vvebzb1cfkmm.png)

У нас есть consumer, он прочитал несколько сообщений. 

![](https://habrastorage.org/webt/jn/zb/rc/jnzbrcwkni1xhoxwzx9ldfehgri.png)

И то значение, с которого надо начинать дальше, он коммитит. Он говорит Kafka: «Я дочитал до вот этого и с этого места я хочу потом продолжить». Соответственно, он потом продолжает и читает. 

![](https://habrastorage.org/webt/b-/ho/9q/b-ho9qyn05fd62wt-ttrvyi3ev8.png)

Потом у него могло произойти что-то и он отвалился. 

![](https://habrastorage.org/webt/ve/o5/yy/veo5yyev88okguxihy2pqiklnho.png)

И при этом он не успел закоммитить данные. Т. е. вот эти все сообщения считаются не закоммитченными. 

![](https://habrastorage.org/webt/5v/qa/an/5vqaan78klxnwdwxjn5j5yak4qs.png)

И когда consumer вернется в работу, он начнет ровно с того момента, на который в последний раз закоммитил. 

![](https://habrastorage.org/webt/iu/q4/qu/iuq4quuutm5lkzhki5bqr_u0o7u.png)

И с него благополучно начнет читать. 

![](https://habrastorage.org/webt/xg/ba/lp/xgbalp9_ba5drlulvsgbxunpiyy.png)

Кажется, что все логично. 

Мы в какой-то момент начинаем писать много данных и нам надо уметь масштабироваться. 

![](https://habrastorage.org/webt/oc/g5/c_/ocg5c_vj6gp7cj7hj9hups6ddto.png)

У нас есть consumer, который начинает уже не справляться с чтением всех данных.

![](https://habrastorage.org/webt/3z/ou/la/3zoulavvkihrvmmqspmwiwsi8vq.png)

Тогда мы поднимаем еще один. И как бы мы могли это сделать? Мы могли бы сделать так, что один consumer читает из первых двух партиций, второй consumer читает из другой партиции. Но этим очень трудно управлять ручками, поэтому в Kafka предусмотрена вот такая штука. 

![](https://habrastorage.org/webt/hp/ro/t1/hprot1o3e05t_wjgjvaaykpf1ty.png)

Можно взять и объединить набор consumers в группу. И у них там появляется общий идентификатор. 

![](https://habrastorage.org/webt/2r/jy/0b/2rjy0ba8cfvvs7z8u-afxqrhsjg.png)

И когда они работают, они между ними автоматически расбалансируют все партиции и начнут читать. 

![](https://habrastorage.org/webt/lj/or/zf/ljorzfl-egysxhmbkuin5qwori0.png)

У них там был какой-то offset, с которого они должны начать. 

![](https://habrastorage.org/webt/m2/ku/3t/m2ku3t5qzsj7dw-ndznr83s7t7w.png)

Первый почитал, второй начал читать. 

![](https://habrastorage.org/webt/yz/bx/3l/yzbx3lgwwj1cgd3fvabni_enwxe.png)

И второй – бах и упал, и не докоммитил что-то. 

![](https://habrastorage.org/webt/ur/bn/tg/urbntgwbwvv1iahndmucjuswg_i.png)

Что произойдет? Во-первых, надо начать снова с первого сообщения. 

![](https://habrastorage.org/webt/ay/fu/sb/ayfusb2eeaxchoewc2ln7qvf-ns.png)

Произойдет перебалансировка в этой группе. 

![](https://habrastorage.org/webt/oz/nd/sw/ozndsw1uug9vbrsulhufjn5m-qo.png)

Единственный живой consumer в данном случае подхватит эту партицию. 

![](https://habrastorage.org/webt/a8/uv/vd/a8uvvdg9hciblkrs9f4zsefvz6g.png)

И уже сам начнет читать данные. 

![](https://habrastorage.org/webt/pe/ak/vc/peakvc950rhf44ns645unofhqva.png)

И так же коммитить. 

![](https://habrastorage.org/webt/60/if/7l/60if7l20pvsyp1ejx03dwvytx0y.png)

А потом consumer ожил, все у него хорошо. Он снова присоединяется к группе. 

![](https://habrastorage.org/webt/qe/d1/q6/qed1q6eno0bnpm7ycvxtdljgte0.png)

И получает какую-то партицию, с которой он может читать данные. 

Кажется, что все очень просто, все хорошо работает. Там очень клевая архитектура, все очень быстро. 

И когда мы все это начинаем изучать с нашими коллегами, то происходят вот такие диалоги. 

![](https://habrastorage.org/webt/ow/sj/d5/owsjd5m9a9wdzo9toltthej1rc4.png)

Я нахожусь справа. 

![](https://habrastorage.org/webt/k2/tm/kk/k2tmkkdjqa__sppuwutbwik_5wa.png)

Приходят коллеги и говорят: «У нас новый проект, хотим PostgreSQL выбрать». 

![](https://habrastorage.org/webt/oz/zw/fg/ozzwfgwgb83al7j_yzmtontgyla.png)

А ты им говоришь: «Ребята, вы чего? Kafka». 

![](https://habrastorage.org/webt/bl/tw/ra/bltwraxp0wfn6yskvkmshn2vjzg.png)

Они оправдываются, что у них RPS маленький. И ты им снова про Kafka говоришь.

![](https://habrastorage.org/webt/oh/0m/ir/oh0mir_f-lpzci1tpfplq9s01z4.png)

Еще какие-то аргументы приводят. 

![](https://habrastorage.org/webt/rq/ud/qb/rqudqbdkqshipc1ebw-sufm2nwk.png)

![](https://habrastorage.org/webt/iq/qo/np/iqqonpfbtg65xy_q-bokpp1agsa.png)

Но ты их уже не слышишь, у тебя в голове Kafka, у тебя уже все хорошо. 

![](https://habrastorage.org/webt/5t/50/w-/5t50w-i2xd0376awet-lt-rd96o.png)

Вот так оно и работает.

![](https://habrastorage.org/webt/76/rg/n4/76rgn4ahr0wrsdu94ejpqtz1-oq.png)

Но потом мы начинаем все эксплуатировать и сталкиваемся с реальностью.

И поговорим о том, что мы пережили за год эксплуатации. 

![](https://habrastorage.org/webt/sk/u8/iq/sku8iqsvvelgb7femiubigqyacm.png)

Но честно скажу, что это очень-очень краткое изложение. 

![](https://habrastorage.org/webt/_d/ct/jn/_dctjnezuxm27mocrc3szzmas3m.png)

Я постарался выбрать показательные примеры того, с чем приходится сталкиваться разработчикам и не только при работе с Kafka. 

![](https://habrastorage.org/webt/ju/ll/vy/jullvyolkcefk_wrcjtzibyckgm.png)

Давайте начнем с настроек. Как разломать кластер? Его можно разломать разными способами. Я предложу один. 



![](https://habrastorage.org/webt/ub/0x/lx/ub0xlx4nlqw7pjo5w6qkk-svu48.png)

Есть настройка в Kafka. Называется log.dirs. Как можно догадаться из названия, это папки, где лежат данные. 

![](https://habrastorage.org/webt/sm/dd/dj/smdddjjno_uopl7kebcbgkj3tmm.png)

У нас есть два брокера. У каждого из них по реплике, по какой-то партиции. 

![](https://habrastorage.org/webt/f-/gj/oy/f-gjoyax9-em4ldidfjxyfgs2_k.png)Один – не очень, он не успевает за лидером. Лидер в этот момент был нами, например, остановлен, перезагружен. 

![](https://habrastorage.org/webt/av/ck/2u/avck2uzyxq2diiahfdfr5ufdu-q.png)

Что произойдет? 

![](https://habrastorage.org/webt/7s/de/zh/7sdezhivj7dbgptyjte1eu45d6o.png)

А реплика 1 ожила, у нее все хорошо. GC прошел долгий. 

И вот какая ситуация. У лидера данных было больше, чем у последователя. И в Kafka может произойти такая штука, как грязные перевыборы. Вроде бы ему нельзя быть лидером, потому что данные потеряются. 

![](https://habrastorage.org/webt/bn/t-/ge/bnt-geijld9841bjznvkcdfvvlq.png)

И есть вот такая настройка - unclean.leader.election.enable=false. Она по умолчанию такая с 0.11 версии. 

[KIP-106 - Change Default unclean.leader.election.enabled from True to False](https://cwiki.apache.org/confluence/display/KAFKA/KIP-106+-+Change+Default+unclean.leader.election.enabled+from+True+to+False) (0.11)

![](https://habrastorage.org/webt/pv/t5/5i/pvt55idvf3ht_0pqxvsuklj45sy.png)

Теперь всё. Теперь партиция не доступна на запись. Надо ждать. Чего мы ждем? Мы ждем, когда вернется старый лидер. 

А мы же эту настройку поменяли. И сказали, что теперь у тебя данные лежат в другом месте. Это что означает? Когда брокер поднимается, у него партиция пустая. И теперь кластер видит вот такую ситуацию. У реплики 1 какие-то данные есть, у реплики 0 их нет. 

Но как мы помним, мы ждали возвращение реплики 0. 

![](https://habrastorage.org/webt/4j/0s/ed/4j0sedmyhcdiduw7o8qililnlqc.png)

И поэтому реплика 1 говорит, что я так не могу больше. И брокер упал. 

А теперь представьте, что таких партиций было много на самом первом, который был перезагружен с новым каталогом. И получается, что все остальные брокеры упали, один остался. 

![](https://habrastorage.org/webt/zy/3g/ag/zy3gagups6-glsrbymboxvgevt4.png)

https://issues.apache.org/jira/browse/KAFKA-3410

В Kafka заведен не один баг по этой теме. И кажется, что он исправлен в версии 1.1. 

По крайней мере в этой ситуации вроде бы брокеры уже не падают. Но это не точно. 

![](https://habrastorage.org/webt/z-/cj/ys/z-cjysuo8lso8lkefuhfsib2jsi.png)

Поехали дальше. Теперь мы поговорим про настройки по умолчанию. У нас по умолчанию default.replication.factor = 1. 

И понятно, почему. Потому что, когда мы только установили Kafka и начинаем ее пробовать, мы поднимаем на своей dev-тачке всего одну ноду, поэтому, если бы default.replication.factor был каким-то другим, то мы не смогли бы ничего себе создать. Это логично. 

Хорошо, всегда при создании топика будем указывать replication.factor, который мы хотим. 

![](https://habrastorage.org/webt/sz/f3/d4/szf3d4sfsec9cansocvr4cd-rhw.png)

Но есть второй подводный камень – это опция auto.create.topics.enable = true. 

Это по умолчанию такая настройка. Что она означает? Если мы создаем топик со своими настройками, то все отлично, но если кто-то пытается писать к нам в несуществующий топик сообщение, то Kafka думает: «О, что-то новенькое». И создает нам топик. И создает топик с настройками по умолчанию, т. е. с replication.factor =1. И мы увидели у себя в production, что часть топиков с нормальным настройками replication.factor, а у некоторых топиков почему-то replication.factor = 1. И оказалось, что кто-то просто не дожидался создания и уже начинал писать данных. И из-за этого оно вот так ломалось. 

Поэтому первым делом, когда вы развернули кластер, меняйте вот эти две настройки.

![](https://habrastorage.org/webt/fh/64/pc/fh64pcim8bb5_yxsvoer_znos2g.png)

Еще две истории с настройками. Я думаю, все согласны, что настройки у брокера, у consumer должны быть. Но они должны не просто быть, а еще должны быть согласованы. 

Давайте посмотрим, что нам разработчики Kafka подложили в качестве дефолтных настроек. 

![](https://habrastorage.org/webt/01/cc/2t/01cc2tops6q33js8_sg1c-ypseg.png)

Есть такая настройка, как message.max.bytes. На брокере она равна 1 000 012 байт. 

Что она означает? Грубо говоря, мы с продюсера начинаем писать данные. И это максимальный размер сообщения с данными, который можно запихнуть в Kafka. 

![](https://habrastorage.org/webt/kf/4o/d3/kf4od3jjqhbzd1ofqh9wutjwo50.png)

Хорошо, на стороне брокера есть похожая настройка, только она равна мегабайту в другой системе, т. е. не в  <font color ="red">C</font>. 

![](https://habrastorage.org/webt/lp/zs/rp/lpzsrpo0jobp93ya84qvuxmftmq.png)

Мегабайт равен настоящему мегабайту, которому мы с вами привыкли. Для сравнения у consumer она точно такая же, как у продюсера. Дело в том, что consumer и продюсер, видимо, одни люди писали. 

![](https://habrastorage.org/webt/7x/bn/8o/7xbn8oatxluajgtu2wq2nzva93k.png)

А брокер кто-то другой делал. 

Клиент Kafka на Java написан, а брокер написан на Scala. 

К чему это может привести? Если у нас на стороне продюсера есть большое-большое сообщение, то, естественно, мы это сообщение получили только в production. Оно чуть больше, чем один миллион байт, но меньше одного мегабайта. Т. е. с нашей стороны пуля вылетела, а брокер говорит, что это не вариант. И не сохраняет. Дефолтные настройки вот такую штуку дают. 

С этими настройками была еще одна забавная история. 

![](https://habrastorage.org/webt/my/0k/bt/my0kbtbr0li_ruhfdc2zrahjtxy.png)

Т. е. тут было большое сообщение, но мы в коде ограничение понизили. На стороне нашего кода мы не передаем большие сообщения. Соответственно, когда в Kafka упаковывается вроде бы должно быть все нормально. 

Но в Kafka есть еще одна такая настройка, как batch.size. 

![](https://habrastorage.org/webt/q6/lo/qf/q6loqfw4pu_a0rg2z-yynmm7rgk.png)

Она измеряется не в штуках, она измеряется в байтах. 16 килобайт – это пачка, которую Kafka-клиент может собрать перед тем, как отправить в брокер. Т. е. там не сразу первое же сообщение пуляет в Kafka, он набирает потихоньку, чтобы меньше сетевых запросов было. Кажется, логично. Все здорово. 

Мы протестировали на своих данных. И оказалось, что вот это сообщение надо умножить на 10. Т. е. для нас 160 килобайт выдает нормальный performance. 

Хорошо, все протестировали, все замечательно. 

А потом мы катили это на production. И помнили, что это значение надо умножить на 10. И оказалось, что у нас уже 160 килобайт было, а мы еще на 10 умножить. И в production уехала эта настройка в чуть больше 1,5 мегабайт. 

Что мы получили? Если вы думаете, что все развалилось, то нет. Все было следующим образом. Мы выкатываем, все хорошо работает. Никаких ошибок нет. У нас нагрузка потихоньку растет-растет и растет. И потом мы пересекаем какой-то невидимый порог и все разваливается. Производительность падает практически в ноль.

Что происходит? 

![](https://habrastorage.org/webt/4h/vz/ax/4hvzaxcay-nfaudynn5aliue-hu.png)

Дело в том, что в Kafka есть KIP.

KIP (Kafka Improvement Proposals) – это такие предложения по тому, как улучшить Kafka, т. е. сделать ее прикольней. 

И такой KIP в версии 0.11 выпущен. Kafka producer, если получает от брокера информацию о том, что какой-то большой пакет, он его разбивает и снова перенаправляет Kafka. 

И когда нагрузка маленькая, то не успевают копиться пачки. Мы их отправляем, и они пролетают быстро. А когда нагрузка возрастает, пачки начинают расти. И мы отправляем пачку, которая чуть меньше одного мегабайта, потому что у продюсера такая настройка, но это больше миллиона байта, который может принять брокер. 

Соответственно, брокер присылает ошибку. В логах ничего нет. Сам клиент решает переразбить. Он переразбивает данные и переотправляет. 

И мы раз большую пачку отправили. Брокер отвечает: «Не пойдет». Мы разбили и еще отправили. Получается, что у нас сетевая активность очень сильно выросла. И мы по этой части тухли неожиданно.

![](https://habrastorage.org/webt/8s/xl/ou/8sxloum6utcodpwo2aerljfzzcm.png) 

Давайте перейдем к API, перейдем к блокирующему send.

Почему я об этом говорю? Дело в том, что в Kafka есть крутой асинхронный API, т. е. все, что бы мы не делали с Kafka, она возвращает нам фьючу. И код дальше продолжает выполняться. Но, как выяснилось, не всегда. 

![](https://habrastorage.org/webt/sq/4y/h9/sq4yh9bdsaai0xeuttpuvhsmg_k.png)

Если метаданные не доступы, то продюсер send блокируется, т. е. он нам даже фьючу не вернет, а просто остановится. 

Метаданные – это информация о том, какой брокер на каких топиках является лидером и у какой партиции. Это информация, которая нужна продюсеру, чтобы понять в какой брокер записать данные, т. е. в какую конкретно партицию. 

И как раз вот эта метаданная периодически обновляется. В кластере лидерство меняется, поэтому его надо периодически обновлять. 

Заблокироваться он может вплоть до 60 секунд. 

![](https://habrastorage.org/webt/co/3a/yf/co3ayfw8crx-yq-xmnhnympk8hi.png)

Такое значение по умолчанию. Если каким-то с кластером были проблемы, то мы даже фьючу не сможем получить. И свой код, который написан с той точки зрения, что у нас от Kafka все асинхронно, просто встает на достаточно большой промежуток времени. 

Кто считает, что так недолжно быть? Все верно, есть люди, которые считают, что это не нормально. 

![](https://habrastorage.org/webt/f5/6s/9c/f56s9crwcps6v7lktnfdloesucm.png)

И есть люди, которые сказали: «Давайте сделаем нормально». И этот KIP в статусе discuss. Это означает, что там какие-то обсуждения идут. И неизвестно, когда эта штука появится в Kafka и когда не появится. 

Но живем дальше, жизнь на этом не заканчивается. 

![](https://habrastorage.org/webt/59/yw/ru/59ywru5iii0ekeiipgy0xaddsss.png)

Давайте посмотрим, как на другой стороне. У нас был продюсер, а сейчас посмотрим, как на стороне consumer. 

Я говорил, что у нас poll-механика, т. е. consumer приходит и просит у Kafka данные. Это означает, что типичный consumer выглядит примерно таким образом: 

![](https://habrastorage.org/webt/ec/ws/e6/ecwse6mnboj7lincg8hivze6uxs.png)

Есть такой бесконечный цикл, в котором мы берем и долбит Kafka. Получаем какие-то данные и дальше с ними что-то делаем. Все неинтересное, я в коммент убрал. Все интересное здесь на слайде. 

Из consumer.poll вылетает ConsumerRecords. 

Там есть Key и Event. Это уже какие-то типы, в которые должны данные десериализоваться. 

А что если кто-нибудь взял и подкинул нам в Kafka плохое сообщение, которое нельзя десериализовать? 

![](https://habrastorage.org/webt/vb/rl/7m/vbrl7mql9ohwrdlnggda7jmgh2e.png)

Сразу скажу, что там будет все плохо. Будет бесконечная десериализация, потому что он не может десериализовать, он падает. Мы снова запускаемся, он снова падает и т. д. 

Почему? Вот это код внутри fetcher, который находится в stack trace перед тем, как вернуть результат poll. 

Я прочитаю эпичную фразу, которая здесь написана в качестве message к exception: произошла ошибка десериализации либо ключа, либо значения для такой-то партиции с таким-то offset. И если хотите, вы можете как-то поискать следующее сообщение за этим и продолжить. Внизу – это метод, которым можно делать. 

![](https://habrastorage.org/webt/wg/v2/gb/wgv2gbpc9vw-j6ajgldza1iv97a.png)

Это партиция, которая там. И offset, который offset. 

И надо взять, перехватить этот exception, распарсить его, вытащить оттуда partition, который является объектом, а не просто номером. Так же, видимо, offset надо распарсить. 

Ок, допустим, мы это сделали. 

Давайте посмотрим, что они предлагают делать. 

![](https://habrastorage.org/webt/3d/y3/zz/3dy3zzmvv2nb0ddvdkb8eeygsxs.png)

У нас есть партиция, в ней данные. 

![](https://habrastorage.org/webt/dw/cd/vw/dwcdvwcdpbwd5tnk1exsn4ry938.png)

Мы начинали с первого и читали пять. 

![](https://habrastorage.org/webt/5d/yc/01/5dyc013kfmxymj2pnko5wl-nylq.png)

И четвертое оказалось плохое. 

![](https://habrastorage.org/webt/cp/ui/4f/cpui4fytnjx3q5v2n-ouuceb7oo.png)

Они говорят, чтобы перейти на следующее. Это, видимо, пятое. 

![](https://habrastorage.org/webt/uc/_a/ei/uc_aeizunxhico5q-5ywqbjgdio.png)

Ладно. А что будет с этими первыми тремя? 

![](https://habrastorage.org/webt/9s/sd/u8/9ssdu85pmztxdwe-thgfish_9kk.png)

Все, их не надо уже читать? Мы же перешли на конец. Просто пропустили их. Такое себе решение. 

Кто считает, что это не очень решение? 

И мы тоже решили, что это не очень хорошее решение. 

![](https://habrastorage.org/webt/jh/ax/p5/jhaxp52xdjm1tmddhrvhlnwvu3g.png)

И решили сделать свой десериализатор, который не будет кидать exception, потому что он не будет ломаться никогда. Он просто вернет null, если ничего не получилось. 

Теперь посмотрим, как в коде это будет выглядеть. 

![](https://habrastorage.org/webt/45/ml/ns/45mlnsxhbodmqqqvzyyjr5rstqu.png)

Добавляется всего одна строчка. 

![](https://habrastorage.org/webt/i3/fh/v1/i3fhv1blfcol1skuvhkf6dqzxqm.png)

И все работает. Там, конечно, надо метрики поставить, чтобы посчитать, сколько таких плохих сообщений оказалось, но это уже отдельная история. 

Это не последний случай с тем, как мы напоролись в consumer. 

![](https://habrastorage.org/webt/et/fk/ql/etfkqlsarnxh6le8purszkeskkm.png)

Нечестное чтение. Мы хотим читать по три сообщения. У нас три партиции. 

![](https://habrastorage.org/webt/dc/qc/tl/dcqctly86fbig9bhyv8d5xldqku.png)

Я не говорил, как данные залетают. На самом деле там все предопределено. 

![](https://habrastorage.org/webt/8d/s7/uc/8ds7uc6ffohaxl_xlgfaj8gag68.png)

Вот у нас три сообщения прочитались. 

![](https://habrastorage.org/webt/em/iy/gq/emiygqanm6u4cyieaa9p9flssyo.png)

О, боже, смотрите, он прочитал из того же самого места, из той же самой партиции. 

![](https://habrastorage.org/webt/ws/px/lx/wspxlxmk9c3g2eufank0e8w2_ye.png)

Потом оттуда же, потом перешел сюда и т. д. 

![](https://habrastorage.org/webt/pc/ng/e5/pcnge5vj11bvv6cvb8uohueogdi.png)

Как он это делает? 

![](https://habrastorage.org/webt/uu/bt/8y/uubt8yjlj61vo95lo_s6anyhpge.png)

Там есть такой KIP. 

![](https://habrastorage.org/webt/b6/fg/se/b6fgsew44wcdxqiisogs1ozbvnw.png)

В документации этого, естественно, нет. Надо идти в KIP читать. 

![](https://habrastorage.org/webt/b6/fg/se/b6fgsew44wcdxqiisogs1ozbvnw.png)

А у нас жадный round robin, который работает следующим образом. Мы хотим читать consumer’ом из нескольких партиций. Будем читать из первой. Пока данные есть, мы из нее забираем. Потом переключимся на вторую и так до конца, пока все данные не вычитаем. И к этому нас никто не готовил. 

И мы начали писать в Google: «Как читать нормально?». Нашли вот такой KIP – честное чтение между партициями. 

![](https://habrastorage.org/webt/zj/nc/qz/zjncqz1amawttcf3tuw7g0uu-fs.png)

Но оно в статусе discuss. Как я уже говорил, это означает, что это появится, может быть, когда-нибудь. Можно, кстати, законтрибьютить. Мы пока еще не пробовали. Мы пока только баги начали отправлять туда. 

**![](https://habrastorage.org/webt/to/hk/tl/tohktl4jip3fafwu89bt68mncme.png)**

Давайте немножко на отвлеченные темы поговорим. На тему DevOps. Ограничение на размер топика. 

Логично в какой-то момент сказать: «Мы пишем много данных в топик. А если у нас диск закончится? Давайте попробуем ограничить». Нашли настройку в документации. Она говорит, что у меня безлимит. 

![](https://habrastorage.org/webt/z8/qs/vg/z8qsvgate7pcn-hv6x1ag6odhcu.png)

Ладно, идем дальше. Настройка на уровне топика есть – это retention.bytes. 

![](https://habrastorage.org/webt/ao/b1/ac/aob1acn0flyo7fangbksyehglyg.png)

Retention.bytes переопределяет дефолтную настройку, т. е. log.retention.bytes – это дефолтная на уровне брокера, а на уровне топика можно переопределить. 

Мы обрадовались, но оказалось, что она per partition, т. е. на каждую партицию. 

![](https://habrastorage.org/webt/zm/uc/9u/zmuc9u5cdzetmztov5j4du2t2nc.png)

У нас есть топик из трех партиций. Мы его ограничили – 1 гигабайт партиция. 3 гигабайта – вроде нормально. А никто не запрещал добавлять партиции к топику. Сколько угодно можно добавить. В общем, это печаль. 

Это настолько печаль, что мы решили новые диски добавлять. 

![](https://habrastorage.org/webt/th/p6/sv/thp6sv6gxe_rv-hi5a97_ldg7qm.png)

И столкнулись с тем, что, когда добавляешь новые диски, Kafka не переносит туда партиции.

![](https://habrastorage.org/webt/xl/pw/yt/xlpwytmczp0cawrjj961eq1p1lg.png)

Слава богу, там есть KIP, который разрешает это делать скриптами. 

![](https://habrastorage.org/webt/jo/4k/im/jo4kimtu-ix8lpr36m5ryg2waia.png)

Мы подумали, что ладно, жить можно. 

Но! Как известно, всегда есть «Но». 

![](https://habrastorage.org/webt/7a/ka/ts/7akatsyg7n0iqy_7zut_8v2kiw0.png)

С Kafka всегда есть какое-нибудь «Но». Равномерное распределение партиций по количеству. Если у нас у нас в каком-то месте мало партиций, то он туда будет докидывать. Ему плевать, сколько данных у нас реально там в байтах. 

И если у нас какие-то топики пишут много данных, то какие-то диски переполнились, а на каких-то может быть ничего нет. Например, раз в час какое-то сообщение падает. 

Мы были не единственными, кого это возмутило. И мы пошли смотреть, что там по этому поводу есть. 

![](https://habrastorage.org/webt/os/yg/7m/osyg7mxs6vif2-slr7ceqf1u5go.png)

Нашли такой KIP, который должен как-то эту политику поменять, но он discuss, т. е. неизвестно, когда появится. 

Мы решили, что будем добавлять новых брокеров. 

![](https://habrastorage.org/webt/ct/ze/m5/ctzem59x69g8clf6i88wzdfz4z8.png)

Думаете, это улучшило ситуацию? Вы уже не верите и правильно делаете. Там нет распределения партиций по новым брокерам. 

![](https://habrastorage.org/webt/i3/zr/rv/i3zrrvthxyu5hkwfnde8ancec1m.png)

Но это можно сделать скриптами. Там есть bash-скрипты, которыми можно сделать partition reassignment. 

![](https://habrastorage.org/webt/9y/1w/6i/9y1w6ie7gbm_ptimlikojff292o.png)

![](https://habrastorage.org/webt/5a/bv/ql/5abvql4yzi5kajvqoafiorbbwwi.png)

Когда я говорю «руками», это означает, что надо создать руками большой JSON, который нужно впихнуть в bash-скрипт, который там что-то сделает. 

Он выглядит следующим образом:

![](https://habrastorage.org/webt/wm/hj/nl/wmhjnl0zian07zqudygvjn2qes0.png) Там можно перечислить партиции, с которыми надо что-то поделать. 

![](https://habrastorage.org/webt/pr/k3/ik/prk3ikxdr8z8cdsijpsjpgtcius.png)

![](https://habrastorage.org/webt/6e/9i/o6/6e9io6nljhhkeqj11gh7gmwpfvg.png)

У партиций есть топик. У нее есть номер. И реплики, на которые надо, чтобы она разъехалась.

![](https://habrastorage.org/webt/7k/j5/mn/7kj5mnh5wdfyc-sw60bfkiydv10.png)

Но есть «Но». 

![](https://habrastorage.org/webt/ld/o-/kq/ldo-kqurkzfdbscedodb-e5d1s0.png)

Вот этот первые массивы реплик он будет preferred leader. Помните, я рассказывал, что, когда у нас лидерство меняется, оно потом – раз и возвращается к истинному королю? Это как раз тот самый случай. 

А там тысяча партиций, для каждой из который нужно написать кусочек JSON. И если мы неаккуратно заполнил эту штуку, там может получится ситуация, когда лидером вот этот первый является. И когда мы в него начинаем вваливать данные, он разваливается. Когда один всего пишет, он не масштабируется. 

![](https://habrastorage.org/webt/uw/s4/9h/uws49hzel1tgwhhszd1oh5o8h_a.png)

Какие выводы мы из этого сделали? Какие уроки мы извлекли?

- Очень внимательно относиться к настройкам. Особенно к настройкам по умолчанию. Надо все тестировать. 

![](https://habrastorage.org/webt/7t/wf/3b/7twf3b1cb-yhhd6kclul6xlc0ta.png)

- Особенности клиентского API. Я аккуратно назвал это особенностями, но некоторые мои коллеги считают это недоработками. Наверное, я с ними согласен. 

![](https://habrastorage.org/webt/nn/z9/gq/nnz9gqsxihqfgd9p9fax5cihepa.png)

- Большое количество рутины в задачах, которые должны решать DevOps 

![](https://habrastorage.org/webt/ny/q4/j3/nyq4j3boalm53_lozuj01zeq6x8.png)

- И самое страшное то, что в документации об этом не пишут. В KIP это можно найти. Наверное, за год эксплуатации я таких документов не одну сотню перечитал, чтобы понять, что происходит. 

![](https://habrastorage.org/webt/ed/dd/5e/eddd5e8b--idoa6ni2cmlc7ydtu.png)

- Но! Kafka при этом – это лучшее, что сейчас есть. 

![](https://habrastorage.org/webt/xg/q2/ve/xgq2vefm_vnjnlutwh_h2seaz1o.png)

И она классная. 

![](https://habrastorage.org/webt/oi/wo/4a/oiwo4a8bkkzz3kfru-veunujebo.png)

Но, мы трезво стали смотреть на вещи. Коллеги приходят с новыми проектами, с новыми технологиями, говорят, что у нас Highloag, BigData...

![](https://habrastorage.org/webt/kr/ro/3a/krro3aqyeshwmalpgqbk6kvwdoq.png)

![](https://habrastorage.org/webt/53/9b/lc/539blcewf0km2bdwl_tvid6d8e0.png)

![](https://habrastorage.org/webt/cm/bz/o5/cmbzo5x55t0los9u-ve-lyccvrq.png)

![](https://habrastorage.org/webt/a1/vl/nz/a1vlnziw8p5ztx7nji2a0e8hqk4.png)

![](https://habrastorage.org/webt/st/tt/vm/stttvmfk4ldrxo3ds0shoyaemfs.png)

![](https://habrastorage.org/webt/kj/qf/ht/kjqfhtctsx-irqkmqhf9n5rmt_m.png)

![](https://habrastorage.org/webt/t4/kr/51/t4kr51p7prccpo52en8b2ialvf4.png)

![](https://habrastorage.org/webt/ds/aa/9o/dsaa9ozgxv4qcahp2p7744c9zie.png)

![](https://habrastorage.org/webt/dz/4a/7z/dz4a7zk50huvuqin5v1oc0gdk8c.png)

![](https://habrastorage.org/webt/qo/v3/er/qov3er87njnyff07vl51kdmrlxy.png)

![](https://habrastorage.org/webt/ee/bw/dp/eebwdp6rzsn7d0cblardtsjaqyk.png)

![](https://habrastorage.org/webt/zo/dk/e0/zodke05x-ckrnbrk4h78ve8hfcs.png)

![](https://habrastorage.org/webt/ae/oj/ka/aeojkadezwumqfhro55ifmdlwnc.png)

![](https://habrastorage.org/webt/f3/uh/2x/f3uh2xvtvero4lul9b8bfkcdfjo.png)

![](https://habrastorage.org/webt/tq/nc/66/tqnc66vr9ynpnky1dichjcniriy.png)

![](https://habrastorage.org/webt/ul/no/7c/ulno7ckbz7_pz08qg2zruurp28k.png)

![](https://habrastorage.org/webt/ic/sr/hd/icsrhde25io9rjwyhjrp3_nrtis.png)

![](https://habrastorage.org/webt/db/to/7r/dbto7ri1vznqeuvx21mo0el7lie.png)

![](https://habrastorage.org/webt/en/yt/fm/enytfmre3rv5ilba3va_a9_wznq.png)

![](https://habrastorage.org/webt/hs/0c/ir/hs0cirz4w4cyox2kmbb4lf7rjdi.png)

![](https://habrastorage.org/webt/5f/cg/sg/5fcgsgk4ehww_skeswtld0gwefu.png)

![](https://habrastorage.org/webt/rw/8s/rg/rw8srgjvmgfmnt0kgy3rzjkiipg.png)

![](https://habrastorage.org/webt/n1/0_/_6/n10__6gaoocyr6wqnk68ym6u1pa.png)

![](https://habrastorage.org/webt/8l/4d/wy/8l4dwytnbiic0ioawaeex5ztysq.png)

![](https://habrastorage.org/webt/r0/14/x6/r014x6usdrpubepwwqme9nb1slc.png)

![](https://habrastorage.org/webt/e1/hz/nc/e1hzncwegoqgklj8eplltdoa9qy.png)

![](https://habrastorage.org/webt/-p/_n/xd/-p_nxdyx8wfqakucuzslg8h81-k.png)

![](https://habrastorage.org/webt/eu/gv/et/eugvet5dx_if14i0npudwcvnqrk.png)

![](https://habrastorage.org/webt/qn/g2/m8/qng2m8nsrnr_it7gzt5ddyoqx-s.png)

![](https://habrastorage.org/webt/um/lm/zt/umlmztmcimbgp4syft5596kp5ru.png)

![](https://habrastorage.org/webt/mr/mq/4j/mrmq4jdem2idlcfynxgmfsgnzeo.png)

![](https://habrastorage.org/webt/dq/ob/-l/dqob-l2louquhjd6edahvjvh3sc.png)

![](https://habrastorage.org/webt/om/pz/sw/ompzswmmc58lnmhzdpwl2dm7dcu.png)

![](https://habrastorage.org/webt/fg/kc/9e/fgkc9ehr-gaira-oib6zyrmyhl0.png)

![](https://habrastorage.org/webt/nc/ko/pk/nckopke37rxcrcgfpmcu62on0ta.png)

![](https://habrastorage.org/webt/ct/ta/im/cttaimo0-9_ceggudww3bnozivk.png)

![](https://habrastorage.org/webt/um/xq/wk/umxqwk3q5ht0ntqkpfm64pwkcey.png)

![](https://habrastorage.org/webt/sg/qf/at/sgqfatrgmosx6w9top_rizh51ye.png)

![](https://habrastorage.org/webt/o2/2t/lv/o22tlvjem3bph7p68-ohxw4lbuc.png)

![](https://habrastorage.org/webt/fh/dc/b_/fhdcb_kzqxqhyr8atoe9rtyhfgy.png)

Спасибо!

**Вопросы**

*Как правильно нарастить количество брокеров и партиций для того, чтобы переварить объем нагрузки, который растет? Т. е. мне сначала добавить партиций, а потом брокер подрубить? Или сначала подрубить брокер, а потом пойти и сказать, что теперь ты – топик, где у тебя было 3 партиции, надо 4, и он сообразит?*

Я понял вопрос. Есть хорошее правило: нормально делай – нормально будет. Что это означает? Сразу нужно понять, сколько брокеров сделать. И сразу ставишь. После этого ты знаешь, сколько тебе нужно партиций. И ты их сразу делаешь. И тогда это работает нормально. В противном случае нужно гонять вот эти partition reassignments. И есть такие тонкости, когда ты говоришь кластеру, что надо перераспределить по мелочам. И он отвечает, что сейчас сделаем. И ты сидишь, а bash-скрипт ничего не возвращает. Проходит 5 минут, 10 минут. У тебя этот процесс просто повис. 

И там есть специальная инструкция, как сделать так, чтобы этот процесс прервать. Так что здесь с точки эксплуатации пока что не очень здорово. 

Но! В Kafka есть очень хороший вектор. Там есть клиент, который позволяет продюсеров делать, consumers делать. И так же там есть админ-клиент, который позволяет управлять Kafka. Мы можем написать очередной свой велосипед, который умеет все это делать, но уже не bash-скриптами, а уже из какого-то нашего кода, который умеет мониторить состояние кластера и распознавать, когда и какие диски заканчиваются. И то же самое это выполняет через админ-клиент. Это true way, как мне кажется, но мы пока еще до этого не дошли. Мы пока только понимаем, что это надо сделать. 

*Спасибо за доклад! Получается, что в* *partitions* *всегда пишутся разные сообщения или они могут дублироваться для одного топика? Не может быть такого, что в двух* *partitions* *одно и то же сообщение?*

Все зависит оттого, как продюсер эти данные пишет. Если к продюсеру пришло сообщение, которое надо записать, он запишет ровно в ту партицию, в которую ему указали. Если у этого сообщения был указан ключ, то оно попадет ровно в ту партицию, в которую должно было попасть. Все сообщения, у которых одинаковые ключи (не важно отличается ли у них value или одинаковое), попадут все в одну партицию. На этом построен механизм compaction. Kafka умеет не только во время протухания удалять данные или по размеру, она еще смотрит, что у нее, например, в партиции 10 сообщений с одним ключом и оставляет последнее. Она так умеет делать. Это можно отключить. Это используется, когда мы агрегируем данные, которые нужны всегда актуальные. В этом случае это очень хорошо работает.

*У меня еще второй вопрос есть. Есть какие-то* *best* *practices* *по созданию* *partitions*, *т. е. мне нужно это завязывать на количество* *consumers* *или* *consumer* *groups?*

Это хороший вопрос. Я, может быть, вскользь упоминал, что партиция – это единица параллелизации. Что это означает? Это означает, что мы можем параллельно писать в топик ровно по количеству партиций. И читать мы тоже можем параллельно, только не более, чем количество партиций. Это означает, что в consumer group нет смысла делать больше читателей, чем партиций. Но если у нас какой-то из consumers падает, то свободный находится в группе, но ему ничего не досталось. Он автоматически получит высвободившуюся партицию при перебалансировке внутри группы. Поэтому можно с запасом делать. Это иногда имеет смысл. 

*Здравствуйте! Спасибо за доклад! Используется старая версия, а планируется ли выход новый версии? И второй вопрос еще есть. Известны ли какие-то баги в старой версии?*

Багов очень много. Они известные, но я почти уверен, что не все. По поводу новой версии отвечу. Я буквально вчера зашел посмотреть, какие релизы были у клиента. Допустим, у Dot Net клиента. Он сейчас находится в стадии – вот-вот и релиз-кандидат. 

*Спасибо за доклад! Хотел уточнить, как готовите* *Kafka*, *т. е. в виде отдельных виртуальных машин или в кластере* *Kubernetes*? *Если это кластер* *Kubernetes*, *то предпочтительно* какие *helm* *charts* *используете? И сталкивались с проблемами*  <font color ="red">*failed*</font> *offset* *commit*?

Начнем с первого вопроса по поводу Kubernetes. У нас Kubernetes используется только для сервисов, т. е. мы Kafka в Kubernetes не тащим. Она у нас на реальном железе крутится. Т. е. про то, как Kafka готовить в Kubernetes, я вам ничего не скажу. Насчет failed commit offset, то это типичная штука, она всегда происходит, когда у нас один из consumers прилег. Consumer, который уже хотел закоммитить данные, почти готов, но произошла перебалансировка. Он пытается коммитить, а ему говорят: «Ты уже не отвечает за эту партицию» и данные не коммитятся. Это часто происходит. И поэтому есть частые грабли. Время перебалансировки маленькое. И если у нас  <font color ="red">Гц</font> длинный на consumer, то он может затупить. Перебалансировка произошла, он ожил, пытается коммитить, но у него ничего не получается. 

Это решается следующим образом. Надо увеличить время на перебалансировку. Оно еще имеет побочное значение, когда мы начинаем нового consumer поднимать в нескольких репликах. Как обычно там происходит? Сначала одна реплика поднялась, через секунду вторая, еще через секунду третья. И если бы мы перебалансировку делали на каждое такое действие, то мы бы вообще не начинали ничего читать, пока все consumers не поднимутся. Часть consumers уже успела подняться. Перебалансировка произошла. Партиции между ними распределились. И он берет паузу. Он ждет, пока наберутся новые consumers. Там есть определенный тайм-аут. И только после этого произойдет перебалансировка на уже большой группе. И это прикольно. 

*Спасибо за доклад! Как реализуете доставку сообщений нескольким* *consumers*, *т. е. не записал, считал, сообщение отбрасывается, а вычищается только после того, как событие получил каждый* *consumer*?

Задачи, чтобы каждый consumer получил, у нас такой жесткой нет. Но данные мы храним в течение нескольких дней. Это означает, что если consumer не удосужился за 3 дня прочитать сообщение, то и не надо его ждать. У нас такая логика.

*Здравствуйте! У меня простой вопрос. Можете привести пример* *user* *case, как вы используете* *Kafka* *у себя и для каких задач? Я не очень понял про* *Postgres, о том, что* *Kafka* *лучше, чем* *Postgres. Они же совсем для разных задач, я не очень понял про это.* 

Там специально было подобрано наиболее абсурдная вещь. Я хотел показать абсурдную ситуацию в том плане, что, когда ты знакомишься с Kafka, с ее архитектурными вещами, то видишь, насколько они классно сделаны и просто влюбляешься в технологию. Технология, действительно, крутая. В этом был point. Это не про то, что Postgres надо заменять на Kafka. Нет, конечно, нет. 

Для чего мы используем Kafka? У нас куча микросервисов, запущена тысяча экземпляров. Из них мы собираем логи, а в перспективе есть еще задача по трассировке, т. е. хотим информацию сетевых взаимодействий между микросервисами агрегировать через Kafka. И также метрики, приложения, т. е. количество запросов, ошибок и прочее. 

 
