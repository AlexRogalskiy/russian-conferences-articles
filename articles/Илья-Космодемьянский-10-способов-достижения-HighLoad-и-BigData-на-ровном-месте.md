**10 способов достижения HighLoad'а и BigData на ровном месте⁄Илья Космодемьянский**

*Коллеги, в нашем следующем докладе Илья Космодемьянский расскажет про 10 типичных грабель, на которые наступают обычно разработчики, когда пытаются свой проект из низконагруженного переделать под* *HighLoad**.*

Да, об этом доклад и будет. Но я бы не сказал, что, когда пытаются низконагруженный переделать под высоконагруженный. Иногда получается из низконагруженного внезапно сделать высоконагруженный. И далеко не всегда в том виде, в котором хочется. 

Есть типичные ошибки в работе с хранилищем. И я эти ошибки не выдумываю специально. Поскольку мы много работаем с удаленной поддержкой баз данных, то мы коллекционируем зачастую одни и те же ошибки от клиентов. И составляем из них некий рейтинг. Об этих вещах я и буду сегодня рассказывать. Какие-то вещи будут абсолютно общие для всех хранилищ, но пару специфических вещей для PostgreSQL я, естественно, упомяну, поскольку мы работаем в основном с ним. И PostgreSQL традиционного много применяется в вебе.

С базами данных бывает две главные проблемы

В качестве эпиграфа можно привести следующую вещь: подобно как в электронике бывают проблемы двух типов, когда где-то есть контакт, который лишний, либо нет контакта там, где нужно, с базой данных та же самая история. Чего только люди не хранят в базе данных: от картинок до совершенно удивительных вещей, которые, может быть, там хранить и не стоило. А бывает и наоборот, когда не хранят в базе данных те вещи, которые стоило бы хранить, потому что для каких-то вещей применяется один кэш, для других вещей применяется другой кэш, а, может быть, лучше было бы, если это хранилось в базе. 

\1. Масштабирвоание

Давайте по вещам кратенько проедемся. Одну из первейших проблем я не наблюдаю только в редких случаях и разве, что с Oracle и с DB2 эта проблема редко случается, потому что там каждая отмасштабированная нода стоит адских денег. Это все же ограничивает полет фантазии и в вебе особо не применяется. А с бесплатными вещами – только в путь. И при этом совершенно необязательно, что это с PostgreSQL. С MySQL это еще больше характерно. И с NovaSQL тем более. Люди любят масштабироваться. Потому что масштабирование – это хорошо, об этом написали в книжках. Просто потому что хорошо. А почему хорошо? Мало кто может объяснить. И тут возникает довольно много разных проблем. 

На протяжении последних десяти лет я постоянно слышу, что все должно быть scale-out. И ни в коем случае не должно быть scale up, потому что scale up – это очень плохо. 

За прошедшее время, с тех пор, как это было придумано, компьютеры стали несколько мощнее. Задачи тоже выросли. Но задачи выросли не до такой степени, насколько мощные компьютеры. 

Есть люди, которые по объективным причинам имеют дело с BigData. Это физики, которые из адронного коллайдера что-то достают. Это физики, которые звезды изучают. Это биологи, которые гены исследуют. Это люди, которые пишут компьютерные игры и плохо разрабатывают игровые сценарии. У них зачастую может быть элементов больше, чем атомов во Вселенной, и они считают, что это должно работать. 

Но объективных задач, где действительно нужно много данных и в одну машину они никогда не вместятся, их довольно мало. Поэтому, возможно, что масштабироваться с самого начала не так уж и хорошо. 

Типичный случай. Мы решили, что нашим веб-сайтом будет пользоваться много народа. Обычно, когда маркетолог с учредителями строят планы, какая у них будет нагрузка, то нормальный программист может это делить на 100 сразу, потому что вряд ли оно так будет. Но если оно будет, то вы должны быть готовы что-нибудь с этим сделать. И, естественно, люди задумываются сразу о масштабирование. И я бы даже сказал, что о преждевременном масштабировании. 

Типичная история. Давайте у нас будет миллион активных пользователей на сайте. Мы по дате создания пользователя их расшарим. Почему по дате создания пользователя? Потому что нам показалось, что нам так нужно. На самом деле таких случаев там может быть много разных, но это один из них. 

В результате через некоторое время мы получаем кучу проблем, потому что пользователи у нас вполне независимые. И, например, чтобы собрать банальную задачу для реляционной базы, если она на одной машине как френдлента, нам нужны какие-то специальные сервисы и т. д. И практически на любое между пользовательское взаимодействие: будь то чат или что-то еще, у нас получается так, что неизбежно users живут на разных машинах. И мы приехали. У нас резко усложняется вся логика, сразу резко усложняется поддержка и т. д. 

Еще в довольно давние времена мне доводилось в одном проекте считать, что если мы поставим 10 машин с PostgreSQL (тогда еще не очень быстрым по ряду вещей), то эти машины должны быть такой-то мощности, чтобы выдержать запланированную нагрузку, такой-то стоимости. Сравнили, посмотрели. Но купить oracle-интерпрайзные лицензии и поставить на дисковый массив со шкафом большим – оказалось дешевле по деньге. Такой случай вполне может быть. 

Сейчас еще проще с этим делом, потому что базы данных стали работать лучше, притом все: и Oracle, и MySQL, и PostgreSQL за последние 10 лет сделали очень большой прогресс. 

Но дальше возникает еще большая проблема. Внезапно выясняется, что у нас из этих 100 машин, на которых мы это дело распилили, активны только несколько, потому что пользователи свежезарегистрированные в силу того паттерна, как они живут у нас на сайте, они активны не равномерно. Т. е. те пользователи, которые зарегистрировались давно и на каких-то машинах осели, они ничего не делают, и машина стоит. 

Наступает еще одна проблема. Если машина стоит, у нее не прогрет кэш. База данных быстрее работает, когда она отдает данные с кэша, а не с диска. И если туда раз в час приходит запрос – достать данные о каком-то пользователе, то в какой-то далеко не прекрасный момент выясняется, что для того, чтобы это сделать, запрос работает в несколько раз медленнее, потому что непрогретый кэш и все плохо работает. Налицо проблема, что мы поставили 100 машин, сделали соответствующие изменения в инфраструктуре и получили неработоспособную систему, которую нам немедленно надо переделать. 

И этот случай гораздо шире. Сначала распилили по одному ключу на шарды, потом поняли, что на самом деле нужно было сделать совсем по-другому. Стали переделывать. Переделали, выяснили, что надо еще раз по-другому, потому что веб – это moving target и условия меняются быстрее, чем мы напроектировали. 

Поэтому мой здесь типичный совет – это сначала вырасти до ресурсов одной машины, на нормальной реляционной базе и посмотреть до куда мы влезаем такими темпами. Потом посчитать, сколько стоят проапдейтить эту машину и сколько мы еще можем на модном, более быстром железе жить. И после того, как мы это посчитаем, принимать решение о масштабировании. И этот подход спас не один проект, потому что преждевременное масштабирование отнимает кучу ресурсов команды, кучу денег. И в конечном итоге получается только дороже и хуже.

\2. Бизнес хочет хранить данные за все время ©

Второй момент, который у меня тоже в рейтинге достаточно высоко – это кейсы BigData на пустом месте. 

Бизнес хочет иметь данные за все время. Типичная история, когда мы считаем какую-то статистику, собираем активность пользователей за какой-то интервал времени. 

И на самом деле нам нужны от нее в основном только агрегаты, потому что свежие данные – это последняя неделя, последний день, последний час. Все остальное – это предрассчитанные агрегаты. И пересчитать эти агрегаты нужно раз в год. В иных случая я видел, что их вообще никогда не пересчитывали, но хранили несколько терабайт для того, чтобы они просто там лежали. Это типичный пример плохой BigData.

И объяснять бизнесу эту проблему – это задача программиста, потому что боязнь бизнеса потерять какие-то данные и потом не посчитать аналитику совершенно очевидная. Данные – это деньги, все просто. 

Но вместо того, чтобы держать там 4 TB вы можете сделать много разных интересных вещей. Вы можете данные правильно архивировать. И даже сырые данные можете хранить на архивной машине, чтобы если понадобится, можно было пересчитать по ним агрегаты, которые вы используете для какой-то статистики. И в результате те горячие данные, которые к вам только что приехали, они будет вместо 4 TB занимать, например, 100 GB. И с этим работать будет на порядок проще.

Более того, во многих базах данных, например, в том же PostgreSQL, есть много автоматических средств, которые позволяют это делать. Например, PL/Proxy позволяет на удаленную машину унести архивированные данные и при необходимости подтянуть данные оттуда, чтобы какую-то выборку выдать. Методик, как это сделать, весьма много. И то или иное партиционированние на архив и горячие данные сильно помогает в такой ситуации. Потому что эксплуатировать базу данных, отличающую по размерам в тысячу раз в большую сторону, когда реально большая часть этих данных вам не нужна, это всегда боль. И делать так не надо. 

\3. EAV упрощает проектирование

Очень любят использовать универсальные подходы. Т. е. entity-attribute-value – это бич для DBA, потому что программисту так проектировать интересней. Потому что у нас же все универсально, нам не нужно менять структуру таблиц, когда нам надо что-то новое добавить. Мы можем добавить новый тип атрибута, прописать новый атрибут. И ему новый value туда засунуть. 

Это хорошо, удобно, но! У вас это все разрастается до трех или четырех таблиц огромного размера. И все ваши данные живут в них. И вы их всегда join’ите. И при этом как-то оптимизировать эти JOINs очень сложно, потому что данные у вас будет, скорее всего, лежать в каком-нибудь типе текст в такой ситуации, потому что вам будет сложно их положить в другой тип. И, например, эффективность индексирования таких данных будет на порядок меньше, чем если вы их будете держать в отдельных реляционных таблицах.

И представляете, какого объема документацию нужно держать, чтобы разобраться какой атрибут что значит и т. д.? Все-таки реляционная схема до некоторой степени сама себя документирует. Особенное, если к ней есть какое-то разумное описание. В EAV вы не разберетесь просто так без хорошей документации ни при каких условиях. 

В результате обычно то, что представляет собой EAV, называется ядром. Когда оно достигает такого состояния, когда становится полностью неработоспособным, оно переименовывается в гордое наименование «ядро». И это все начинает обвешиваться разными представлениями, где хранятся денормализованные данные, чтобы к ним иметь хоть какой-то быстрый доступ. 

Но в результате от изначально стоящего в заголовке упрощение проектирования не остается вообще ничего. Это работает медленно, плохо. И любое изменение в этой схеме приходится прокидывать в кучу разрозненных, совершенно независимых от себя и зачастую написанных разными людьми, вещей. И в результате мы эффективно не достигаем той цели, которая у нас была заявлена. 

\4. ORM упрощает разработку 

Тоже самое с ORM. Я не хочу тут вдаваться в долгий холивар – хорошо ли это или плохо иметь ORM, но как DBA я ORM категорически не люблю. И я понимаю стремление что-то быстро сделать на ORM. Но по факту дальше начинается какое-то странное явление. Мы понимаем, что ORM существенно тормозит базу данных. 

Мы начинаем некоторые запросы переделывать на plain SQL. Еще «лучше» попытаться выдрессировать ORM писать хорошие запросы. Это страшное дело. Я как-то видел человека, который мужественно боролся с запросом, сгенерированным ORM, зная, как его написать правильно. У него уже был запрос, который он написал правильно. И он пытался выдрессировать ORM, чтобы тот сгенерировал запрос такого же вида, чтобы он нормально заработал. Но бессмысленность этого становится совершенно очевидной. 

Более того, можно прочитать отдельный доклад на тему «А давайте мы угадаем по логам SQL запросов из какого ORM к ним ходили». 

Вот это из какого? Многие ORM этим грешат. Но если я добавляю, что этот in может быть какого-то огромного размера, то, я думаю, что это Django. Факт в том, что по многим признакам можно ORM угадать. А эта вещь очень плохая. Почему? Потому что этот список может быть каким угодно. Оптимизатор вообще не знает, что с ним сделать. Если это заменить на JOIN, то это будет работать еще куда не шло, скорее всего, это можно как-то оптимизировать. С этим in не сделать ничего без очень грязных хаков. И в ORM так всё, все запросы делаются такими. И надо понимать, что в какой-то момент рано или поздно это станет неудобно и плохо. И плохо с точки зрения производительности. 

\5. Главное зло в PostgreSQL – autovacuum

В PostgreSQL есть такая штука, как автовакуум. Когда PostgreSQL делает insert, он делает insert. А когда он делает update, он делает insert + delete. При этом delete – это тоже не delete, а просто убирание из области видимости tuple.

И получается очень фрагментированная таблица, т. е. у нас может быть в табличке реально актуальных значений 100 000, а она может весить адские гигабайты просто потому, что в ней еще много миллионов неактуальных tuple, которые нам не видны. Они просто балластом ездят и превращают нашу базу данных в типичный пример BigData.

Если там ничего не настроено, если там работает все плохо, то первая же реакция у людей, которые это эксплуатируют – это давайте выключим автовакуум, потому что это самый долгоработающий процесс, который всему мешает и невозможно добавить индекс, нельзя ddl выполнить, если автовакуум по таблице идет. И поступает предложение его выключить.

Но заканчивается это очень плохо. И тут можно много об этом рассказывать. Вот эта ссылочка на рекомендации, как бороться с автовакуумом и что делать. Главное, что не надо отключать его, потому что результаты получаются совершенно чудовищные. У вас BigData получается на ровном месте больше, чем с time-series данными, которые вам не нужны и занимают место в базе. 

\6. JOIN – это зло, они медленные

У меня даже есть наблюдения, откуда это у людей в голове. У людей, которые давно работают с MySQL, у них часто бывает идея, что JOIN – это зло. 

Что я тут могу сказать? JOIN – это добро. Почему реляционная модель настолько успешна? Кто из вас слышал, что реляционная модель – это отсталая вещь? Я думаю, что все слышали. И все ждут, что сейчас придет … SQL, и всех победит. Но так не происходит. Почему? Потому что у нас данные удобно хранить одним способом в виде блоков на диске. А доставать их удобно другим способом, желательно более высокоуровневым. И если у нас данные достаются высокоуровневым способом, то реляционная модель очень удобна для того, чтобы делать оптимизацию.

И если мы используем реляционную модель, то надо использовать ее на всю мощь. Что является альтернативой тому, что мы сделаем JOIN? Очень все просто. Мы вытягиваем в наш любимый язык программирования на наше сервер-приложение данные из двух-трех табличек. Т. е. фактически они живут в приложении и занимают место, при этом эти таблички могут быть большими. Нам надо получить в итоге 10 строчек, а так мы вытягиваем огромные простыни. И дальше мы занимаемся JOIN’ом вручную. Т. е. мы сначала начинаем ходить циклом. Потом понимаем, что это медленно. Начинаем какой-нибудь алгоритм хэширования использовать. И база данных будет плохой, потому что оптимизатор собирает информацию о многих параметрах для того, чтобы выбрать алгоритм JOIN.

В нашем языке программирования мы это все будем писать заново сами. И зачем это делать? Но кто ни разу не видел, чтобы кто-то делал так? Все, хотя бы раз такое встречали. Это говорит о том, что в народе эта мысль популярна. Поэтому используйте ту мощь реляционной алгебры, которая вам дана. SQL – это высокоуровневый хороший язык. Если вам нужно достать 10 строчек, предварительно пройдя по очень большим таблицам, то JOIN хорошо вам поможет в отличие от делания этого вручную и не предназначенными для этого средствами. 

\7. Давайте изобретем Slony

Еще одна больная совершенно тема – это изобретение многочисленных вариантов репликации. Почему Slony? Потому что в PostgreSQL есть не очень любимая многими система репликации Slony, которая очень старая. Она была еще до появления встроенной репликации. Это trigger base репликация, которая тяжела в обслуживании.

И тем не менее, несмотря на то, что она есть много лет, она неплохо работает, если ее выдрессировать. Но люди регулярно пытаются изобрести что-нибудь свое. В PostgreSQL есть, как минимум, три широко распространенных методов репликации. Это shipping log Hot Standby, это Slony, это Londiste PgQ, которые имеют свои плюсы, свои минусы. Я бы в 99 % случаях не советовал бы использовать ничего, кроме встроенного shipping log. Но тем не менее народ пытается изобрести что-нибудь свое для своих каких-то целей.

Всегда, когда я видел такие изобретенные штуки, они всегда работали как-то не так. Потому что вы не учтете всех проблем, которые у вас возникнут. Репликация – это обработка распределенных транзакций. Обработка распределенных транзакций – это всегда тяжело. Там никакого волшебства нет. И, соответственно, если по каким-то причинам встроенная репликация не предоставляет вам какого-то функционала, как, например, мульти-мастера, то это означает что-то. Почему-то это так не работает. И если вы будете пытаться это изобрести самостоятельно, то с большой вероятностью будете ходить по граблям. И многие так делают. Если есть какая-то технология в базе данных, то нужно подумать, почему она так используется и использовать ее.

Что касается репликаций, то тут есть еще один немаловажный момент. Встроенная репликация не работает на высоком уровне, на уровне SQL. Т. е. вы, когда пишете свою репликацию, наверное, общаетесь с уровнем таблиц, триггеров, хранимых процедур. Это, как правило, медленно и чревато конфликтами. Залезть куда-то глубже – это уже гораздо сложнее. Т. е. перепилить там базу данных, чтобы у нее была другая репликация – это задача очень серьезная. Скорее всего, вы должны понимать, что делаете, если вы можете залезть так глубоко. 

А встроенная репликация – это репликация к транзакционным логам. Пишется информация об изменениях, но не в виде SQL statement, а в виде того, какая информация нам нужна, чтобы вернуть страничку 8 килобайтную в предыдущее состояние или, наоборот, сделать redo до нового состояния. И вот этот лог едет на slave, и там применяется. И это будет на порядок эффективней, и на порядок надежнее, чем любой другой метод репликации. И он уже сделан. Он уже встроен. Его можно брать и пользоваться. 

8 У меня в тесте все работает

Типичный ответ программиста админу, который говорит о том, что процессы взаимодействия эксплуатации и разработки в конторе фундаментально поломаны. 

Все знают, что нужно использовать explain. Далеко не все используют, но все знают, что надо использовать. Т. е. надо посмотреть – оптимально ли работает запрос, не забыли ли мы там индекс и т. д. 

Но часто бывает так, что человек посмотрел это у себя в разработческой среде. И самый простой вариант, что на production будет в 1 000 больше данных. И будет выбран другой план, и соответствующим образом будет происходить работа. И уже будет медленно, понадобится индекс, понадобится другой индекс, понадобится что-то еще и т. д. Тут может быть много проблем. 

Более того, база данных – это очень сложная система. Там workload зависит от кучи разных странных параметров. Например, у вас проект шаред базу с другим проектом. Т. е. они работают там вместе, а у вас деволоперской среде есть только ваш проект. От того проекта порождается какая-то нагрузка, потому что все сотрудники всех офисов России утром за чашкой кофе заходят на ваш сайт, например. А ваш проект, который совершенно с этим никак не связан, но вы испытываете какие-то удивительные тормоза, потому что база сильно тормозит в эти утренние часы. 

Здесь очень важно разработчикам иметь какой-то доступ на production, либо отлаженные процедуры работы с DBA. Один из самых худших моментов, когда админы просто стараются не пускать бестолковых программистов никуда. Это неправильно, т. е. ведущие разработчики, которые знают проект и не навредят, должны иметь либо доступ на чтение, либо какую-то информацию из мониторинга должны иметь все. 

Если у вас стоит мониторинг, который показывает в PostgreSQL по pg_stat_statements, какие запросы занимают какие-то ресурсы, то эту картинку должны иметь программисты, чтобы после каждой выкатки могли посмотреть на свой запрос, т. е. как он себя ведет, что там происходит. И должны уметь это дело интерпретировать.

Если админ туда не пускает разработчиков никаким образом, то рано или поздно закончится такими вещами, что у меня все в тесте работает. Потому что ответственности за свой код, за свой проект в такой ситуации разработчик не несет, а он должен уметь посмотреть на production, как это все происходит, потому что разница там может быть весьма существенной.

\9. Be smart, as a java-developer!

Ни в коем случае не хочу обидеть java-программистов, … . Программисты – люди любознательные. Им интересно попробовать что-нибудь новое, что-нибудь интересное и воспользоваться принятыми в своем языке подходами. 

Есть у нас java. В java есть потоки. И известно, что c java … можно сделать так, чтобы все работало быстрее, чем без них. Но у нас есть база данных. Мы в базу данных можем нагрузить много данных. Кладем их туда в несколько потоков. Нам кажется, что медленно. Мы распараллеливаемся побольше. 

И при этом разработчиков в данной ситуации не волнует, что происходит. А происходит следующее, что в базе данных у нас 10 воркеров, потому что у нас такое количество ядер на машине, что с учетом служебных воркеров, нам только на 10 их остается. И в эти 10 воркеров приходят вот эти все потоки. 

Что дальше происходит? Естественно, эти потоки начинают тупо драться между собой. И в данной ситуации – это не самая хорошая идея. Поэтому программисту нужно представлять себе весь stack в больших и лучших деталях, потому что если такие вещи делать, то получается очень странно. 

Второй пункт – это случай вполне себе из жизни. Мы обычно делаем следующее. У нас есть мониторинг медленных запросов. Т. е. топ медленных запросов, почему они медленно работают. Смотрим в отчете запросы в топе. Как обычно DBA-дежурный проверяет, что такая проблема есть? Он берет этот запрос в транзакции, потому что запрос может заменять какие-то данные. У PostgreSQL все транзакционно, соответственно, можно сказать: begin explain analyze и rollback, чтобы ничего не испортить. Гоняет этот запрос – все работает. Притом работает миллисекунды. Смотрит в отчет – регулярно и стабильно очень долго. 

Оказалось, что ребята решили попробовать ко-рутины. И, соответственно, из скрипта это дело запускается через ко-рутины. Они дерутся между собой. И в результате запрос работает медленно. И не по тому, что запрос плохой, а потому что на стороне Python’а дерется и данные с него медленно отправляются, и данные в него медленно приходят. Там все ждет. 

И таких вещей может быть много, поэтому в эти вещи надо внимательно смотреть и не пытаться применять свою технологию на всю катушку, не зная достоверно, что технология, с которой вы взаимодействуете, а именно база данных в данной ситуации, это дело поддерживает или не поддерживает. 

\10. Приятные мелочи

Что касается 10-го пункта, то это не один единственный пункт, а это сразу много пунктов. Это такие вещи, которые постоянно всем всегда объясняешь.

 У нас есть запрос, он возвращает сколько-то строк. Например, много строк, миллион. А мы – веб-сайт, например. Мы работаем с веб-мордой. Как вы думаете, часто ли на веб-морду приходится выводить миллион строк в каком-то виде? На самом деле часто. Как вы думаете, сколько людей прочитало этот миллион строк? Ни один. 

Если вы разбираете свои медленные запросы, например, и видите по explain, что запрос возвращает такое количество строк и это не ETL, не выгрузка чего-то куда-то, т. е., например, ночью в аналитику и т. д., то задумайтесь – это кто-то может прочитать? Это верный признак, что у вас явная ошибка. Это такой запрос, на который надо посмотреть и понять, что, может быть, там данных стало больше, либо кто-то его с ошибкой написал. Т. е. это сразу большая проблема. И самое главное, что для веб-морды такой запрос совершенно бесполезен. Это ошибка, ее надо убирать. 

Та же самая история со счетчиками count, которую я рассказываю на каждой конференции, где я что-то говорю о том, как правильно работать из веба с базами данных. И в очень многих случаях я вижу это дело в отчетах о медленных запросах.

Очень любят пользователям показывать счетчики. При этом у нас высоконагруженный сайт. И на морде этого сайта эти счетчики тикают быстро в конкурентной среде. Если они тикают не быстро, то их можно показывать очень просто. У PostgreSQL есть данные от анализатора статистики планировщика. И можно написать процедуру, которая будет возвращать эти данные, которые будут обновляться раз в какое-то время. И они будут достаточные, потому что они будут приблизительные, и все хорошо. 

Но люди делают следующее. Они реально этот счетчик выводят. У вас будет информация о том, что сегодня зарегистрировалось 261 526 пользователей, а когда вы обновите страничку через секунду, у вас окажется, что эта цифра изменилась на 15 000. Какой толк от того, чтобы иметь такую точную цифру?

При этом count – это тяжелый запрос. Чтобы сделать count по всей табличке – это всегда … count, потому что проверяется версия каждой странички. Count – не бывает легким ни в одной базе данных по этой причине.

Соответственно, вы сделали 20 счетчиков на главной странице. И эта вещь будет отдаваться несколько секунд, если у вас под этим делом лежат большие таблицы. 

Что здесь делать? Как это ускорить? Никак. Только убрать такие счетчики. Потому что, если у вас страница высоконагруженного сайта только на SQL-запросы тратит несколько секунд или даже хотя бы полсекунды, то это не веб. Пользователю такой веб не нужен. Пользователь привык, что компьютер тормозит, да и то более-менее отвыкает в нынешние времена, а веб для пользователя должен работать мгновенно. Потому что он загружает страничку долго, пользователь пошел на другой сайт и сделал заказ на нем, например. Поэтому эти вещи нужно всегда выкашивать.

И традиционный совет в таком докладе про базы данных – это know your data. Вы должны знать, как работают ваши данные. Как, например, Том Кайт, который является вице-президентом Oracle по консалтингу и очень много знает про то, как Oracle работает, знает про SQL-запросы и т. д. Он может провести дискуссию про оптимизацию запросов, которыми предлагают оптимизировать прямо сейчас. Его спросили: «Какими инструментами пользуетесь для того, чтобы так хорошо оптимизировать SQL-запросы?». Он сказал: «Тут очень все просто. Я закрываю глаза и пытаюсь представить, как это работает». Т. е., грубо говоря, полезно знать, какие у вас данные, какие у вас запросы и нужно думать, когда вы с ними работаете. Это основной и классный секрет успеха работы с базами данных вообще и с PostgreSQL в частности, и с любой технологией, с которой вы имеете дело.

На этом у меня все. И остается разумное количество времени под вопросы. И я с удовольствием на них отвечу. 

Вопросы

*Спасибо за доклад! Было сказано, что автовакуум не стоит отключать. А если база данных* *open* *done* *only**, тогда имеет смысл отключать и не использовать?*

Не имеет. Надо все равно держать. Там по ссылке это написано. Несколько есть моментов. Во-первых, помимо автовакуума как такого это demon занимается analyze, он обновляет статистику. И, соответственно, если у вас opened происходит, у вас меняется количество записей в табличках. И оптимизатору нужно знать об этом, чтобы выбирать оптимальные планы. Во-вторых, у вас есть pg_catalog, который обновляется, т. е. это внутренние всякие таблички. И если по нему отключить автовакуум, то результат может быть очень удивительным. У вас табличка gp_class станет в несколько гигабайтов и работать все это будет очень печально. Т. е. этого не нужно делать хотя бы даже из-за этого.

*Т. е. лучше не трогать?*

Да, лучше настроить разумно и работать. В ряде случаев, если какие-то таблички, которые обновляются не очень часто, можно по ним индивидуально указывать, чтобы автовакуум работал не так интенсивно. Но на практике я так делать не советовал, потому что, если табличка вырастет до больших значений. И у нее на 90 %-ах апдейтов стоит автовакуум, то этот автовакуум будет работать долго и будет серьезно мешать performance системы, несмотря на то, что он будет срабатывать редко. Это нехорошая практика.

*Спасибо. И еще вопрос по поводу больших выборок. Там же можно лимит поставить.* 

Лимит можно поставить, но, как правило, если возвращается миллион, то лимит не поставлен. Но с лимитом, чтобы получить те данные, которые вы хотите, вам надо понять, как они у вас отсортированы, какую часть вы получаете. Тут надо конкретной реализацией заниматься.

*Илья, спасибо за доклад! Меня зовут Тимур. Такой вопрос. Вы сказали, что бизнес хочет хранить любые данные всегда и за весь промежуток времени. У нас тоже примерно такие же ситуации. И как в таком случае лучше подходить к проблеме архивирования данных какой-то большой таблицы огромных логов, которые невозможно куда-то деть, они когда-то могут понадобиться, но на самом деле нам нужна только последняя часть? А вот этот большой хвост, который остается, его надо куда-то деть, заархивировать каким-то образом, но при этом сохранить, чтобы можно было с ним работать. Вы немножко про* *PL**/**Proxy* *сказали. Интересно, что это за инструмент. Подходит ли он для этого? Или какие-то другие инструменты?*

Есть достаточно много методов. Во-первых, простой способ – держать какую-то машину, может быть, попроще, просто с большим объемом, которой не нужна особо производительность, а должно быть места там много. И можно туда выносить этот хвост холодный. Дальше уже вопрос о том, какими методами выносить. 

Простейший способ – это дампить эту табличку. И этот дамп выносит на тот хост. И, соответственно, его там разворачивать. Пусть оно там будет, это можно будет оттуда достать в любой момент. 

Если нужно более оперативный доступ, то можно написать хранимую процедуру на языке PL/Proxy. Это один из методов. Может быть, он не самый лучший и не для всего подходящий, но тем не менее он работающий. 

Что это за технология? У вас есть две хранимые процедуры. Одна на вашем хосте, на котором у вас активные живут. И это процедура на языке PL/Proxy, которая занимается только тем, что вызывает на удаленной машине с архивом хранимую процедуру на языке PL/pgSQL, которая обладает точно такой же сигнатурой и возвращает этот результат по удаленному вызову. Довольно быстро работает, хорошая технология. Skype’м в свое время разработанная, когда они еще были не под Microsoft’ом.

И в такой ситуации вы можете повесить вьюху Union на запрос из этой таблички, которая у вас с горячими данными на активном сервере и с результатом, выданном из хранимой процедуры, которая PL/Proxy. И дальше, если вы обращаетесь к этой вьюхе и вам нужны только горячие данные, вы обращаетесь только к ней. Вы только эти данные поднимаете. PL/Proxy разбирается, куда надо сходить. Смотрит, что там нулевой результат и в Union приезжает нулевой результат. Это одна из методик, как это можно сделать.

*Илья, спасибо за доклад. Меня зовут Владимир. Хотел вернуться в самое начало. Вы говорили, что нужно не торопиться масштабироваться. Но нужно как-то предусмотреть масштабирование заранее. Какие подходы стоит учесть, что можете посоветовать?*

Первый совет – не масштабироваться заранее. А готовясь к старту понимать, сколько данных будет. Понятное дело, что никакие нагрузочные тестирования не гарантируют воспроизведение ситуации, как оно пойдет в бою. Но в любом случае, первое, что нужно иметь – это хороший мониторинг. Вы должны в идеале иметь график прироста ваших данных и реакции базы данных на это. Вы имеете график и смотрите тренд. Если вы видите, что у вас маркетинг начал активно продавать какие-то услуги и у вас пошел рост, то вы заметете это на графике и тогда примете решение достаточно обоснованное: что вам проще и дешевле. Например, купить несколько новых быстрых SSD и доставить оперативной памяти, либо это слишком дорого и нужно поставить пару дешевых машин, чтобы разгрузить на какие-то определенные задачи. 

Я бы сказал, что все-таки нулевая задача – это понять – надо ли вам масштабироваться или нет. А дальше уже исходя из этого думать. PostgreSQL на разумно стоящем сервере для базы данных обойдется, условно говоря, в 30 000 долларов. Это разумная цена по нынешним временам. Он может выдерживать, не замечая, очень большую нагрузку. И далеко не во всех проектах такая нагрузка возникает. 

Скорее, имеет смысл масштабировать какие-то вещи по workload. Например, если у вас есть веб, на нем много OLTP-запросов и они идут на мастер. Если нужно какую-то аналитику прогнать, то тут, скорее всего, будет не очень хорошо, потому что на базах данных, особенно версионных, не очень хорошо живут одновременно длинные и короткие запросы по массе причин того, как устроены транзакции. В этой ситуации вам лучше масштабироваться тем, что реплику поставив и запросы на чтение туда отправив. Т. е. масштабироваться не по размазыванию данных по разным местам, а исходя из задач. Если у вас запросы длиной больше нескольких миллисекунд, то они идут на реплику. Если запросы короткие и пригодные для OLTP, они живут на мастере. Скорее, в эту сторону я советую думать.

*Подскажите, насчет предыдущего вопроса про вынос логов, кроме* *PL**/**Proxy* *еще можно же вроде использовать* *F**…* *reaper**? Вы им не пользовались?*

Его можно использовать. Но там больше вещей есть на свой страх и риск. Т. е PL/Proxy не до конца умеет транзакции в том смысле, что он умеет только автокоммит. Но это транзакционный вызов RPS, притом он может быть вызван очень далеко. А F…reaper – это вещь, во-первых, совершенно непредсказуемая для optimizer, а, во-вторых, не всегда хорошо работающая с транзакциями, потому что неизвестно, кто его написал. F…reaper к PostgreSQL – вообще довольно странная вещь, потому что там на одной стороне исполняется запрос с одним планом, на другой с другим планом. И это может быть предельно не оптимально. Т. е. его, в-принципе, можно использовать для этой цели, но я бы рекомендовал более простые решения, а его использовал бы только, если очень хочется. 

*А с* *PL**/**Proxy* *тоже так не получится, что там другой план будет?*

Да, естественно, получится. С ним по опыту проблем меньше бывает на эту тему. 

*Вам встречались решения типа вынести архивную табличку на отдельный диск, положить диск в* *shared* *storages* *с какой-нибудь* *OCFS**2 и с этим* *OCFS**2 отдельный сервер на* *read* *only**, чтобы работал?*

По какому-то NFS ходить?

*Грубо говоря, да.*

Это не самая лучшая идея, потому что, если вдруг кто-нибудь дернет этот table space по каким-то причинам, то это резко увеличивает всю latency, потому что вот это все добро начинает лезть в шаредные буфера.

*Я имею в виду с другого сервера.* 

В смысле table space на этом сервере дергать с другого сервера?

*Да.*

Этого делать ни в коем случае не надо даже на read only, потому что там вся метаинформация в table spaces, pages относится к этому серверу. И это не будет работать.

*Но* *Oracle* *как-то так делает же.*

У Oracle есть рак. И это большая разница. 

*А что все-таки делать с* *ORM**? Вы вообще отказываетесь от* *ORM,* *как только начинаете писать проект или вы перед выпуском в* *production* *все переписываете? Т. е. какая у вас политика относительно него?*

Я ничего не пишу, я только последствия ликвидирую в основном. И среди наших клиентов много, кто делает по-разному. Есть люди, которые ORM пользуются и живут с ним, у них есть процессы, которые отвечают за оптимизацию ORM и они считают, что так хорошо. И это совершенно нормально. С моей персональной точки зрения ORM пользоваться не очень удобно, потому что SQL – это вещь удобная, а на java конструирование запросов – выглядит немного диковато. Но на вкус и цвет все фломастеры разные. Есть куча людей, которые, наоборот, любят это делать. 

Но опыт подсказывает, что проектов, которые начинаются целиком на ORM очень много. Проектов, которые успешно эксплуатируются многие годы и при этом целиком на ORM – я не встречал вообще ни одного. Обычно народ начинает с того, что какие-то критичные запросы начинает выдергивать из ORM и писать их руками. И, наверно, 90 % проектов, начавшихся с ORM, живут по такой схеме. При этом проектов, которые с ORM переписали, я тоже видел сильно меньше, потому что обычно это большие затраты. И во всех случаях ситуация с производительностью у них изменилась просто на порядки. Это серьезно приносит много плюсов, но нужно нанимать программистов, которые знают SQL и умеет его писать. И, соответственно, тратить время на какие-то более медленные вещи в разработке поначалу. 

ORM возник как идея от вендорской разработки, когда люди разрабатывают какую-то систему, ставят ее заказчику и им нужно сильно сократить время до подписания акта. А дальше – то, что его будет сложно поддерживать, это только бонус. За это поддержке деньги платят. А когда мы в вебе живем – это совершенно другая история. Нам нужно делать по-другому, потому что у нас нет точки подписания акта, как правило. Мы сами потом будем тратить время на эти вещи. 


 


 


 

