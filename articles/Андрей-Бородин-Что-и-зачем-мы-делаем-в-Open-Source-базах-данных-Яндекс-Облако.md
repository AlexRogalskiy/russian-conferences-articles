**Что и зачем мы делаем в Open Source базах данных — Андрей Бородин**

![](https://habrastorage.org/webt/gx/yo/st/gxyostrzwelso3rfsxyzdepj6sw.jpeg)

Hello world! Меня зовут Андрей Бородин. И я в Яндекс.Облаке занимаюсь тем, что развиваю открытые реляционные базы данных в интересах Яндекс.Облака и клиентов Яндекс.Облака. 

![](https://habrastorage.org/webt/vk/cb/lb/vkcblbyznf5ntggva20v_awbhvy.jpeg)

В этом докладе мы поговорим о том, какие проблемы возникают у открытых баз данных в масштабе. Почему это важно? Потому что маленькие-маленькие проблемки, которые, как комарики, потом становятся слонами. Они становятся большими, когда у вас много кластеров. 

Но это не главное. Бывают невероятные вещи. Вещи, которые происходят в одном на миллион случае. И в облачном окружении вы должны быть к этому готовы, потому что невероятные вещи становятся весьма вероятными, когда что-то существует в масштабе. 

Но! В чем плюс открытых баз данных? В том, что у вас есть теоретическая возможность с любой проблемой бороться. У вас есть исходный код, у вас есть знания программирования. Совмещаем и работает. 

![](https://habrastorage.org/webt/4s/c-/b4/4sc-b4wdndxmqyuymybn5ic6zhg.jpeg)

Какие подходы есть в работе над открытым программным обеспечением?

- Самый понятный подход – это использование программного обеспечения. Если вы используете протоколы, если вы используете стандарты, если вы используете форматы, если вы пишите запросы на программном обеспечении из open source, то вы уже его поддерживаете. 

- Вы делаете его экосистему больше. Вы делаете вероятность раннего обнаружения бага больше. Вы повышаете надежность этой системы. Вы повышаете доступность разработчиков на рынке. Вы улучшаете это программное обеспечение. Вы уже контрибьютор, если вы просто сделали up getting style и что-нибудь там поковыряли. 

- Другой понятный подход – это спонсирование открытого программного обеспечения. Например, всем известная программа Google Summer of Code, когда Google платит большому количеству студентов со всего мира понятные деньги за то, чтобы они развивали открытые программные проекты, которые удовлетворяют определенным требованиям по лицензии. 

- Это очень интересный подход, потому что он позволяет развивать программное обеспечение, не смещая фокус с сообщества. Google, как технологический гигант, не говорит о том, что мы хотим вот эту фичи, хотим починить вот этот баг и вот там нужно копать. Google говорит: «Делайте то, что вы делаете. Просто продолжайте работать так, как работали и все будет хорошо».

- Следующий подход участия в open source – это соучастие. Когда у вас есть проблема в открытом программном обеспечении и есть разработчики, то ваши разработчики начинают проблемы решать. Они начинают делать вашу инфраструктуру более эффективной, ваши программы более быстрыми и надежными. 


![](https://habrastorage.org/webt/aa/dk/j3/aadkj3wo-ra-eqvmvygpuk-2rnm.jpeg)

Один из наиболее известных яндексовых проектов в области СПО – это ClickHouse. Это база данных, которая родилась как ответ на вызовы, стоящие перед Яндекс.Метрикой. 

И как база данных она была сделана в open source для того, чтобы создавать экосистему и совместно с другими разработчиками (не только внутри Яндекса) ее развивать. И сейчас это большой проект, в котором участвует множество различных компаний. 

![](https://habrastorage.org/webt/xk/ko/tu/xkkotumqxtzlqzt3o1tq5z048sk.jpeg)

В Яндекс.Облаке мы сделали ClickHouse поверх Yandex Object Storage, т. е. поверх облачного хранилища. 

![](https://habrastorage.org/webt/bx/l8/wc/bxl8wc4atzwcw8nmbnkln6m8fee.jpeg)

Почему это важно именно в облаке? Потому что любая база данных работает в этом треугольнике, в этой пирамиде, в этой иерархии типов памяти. У вас есть быстрые, но маленькие регистры и дешевые большие, но медленные SSD-диски, жесткие диски и какие-то еще блочные устройства. И если вы эффективны сверху пирамиды, то у вас быстрая база данных. если вы эффективны снизу этой пирамиды, то у вас масштабированная база данных. И в этом плане добавление еще одного слоя снизу – это логичный подход по увеличению масштабируемости база данных. 

![](https://habrastorage.org/webt/k0/ff/0f/k0ff0ftmicbzxqrea4-gowlyhme.jpeg)

Как его можно было сделать? Это важный момент в этом докладе. 

- Мы могли реализовать ClickHouse over MDS. MDS – это внутренний яндексовый интерфейс облачного хранилища. Он более сложный, чем распространенный протокол S3, но он больше подходит для блочного устройства. Он лучше подходит для записи данных. Он требует больше программирования. Программисты будут программировать, это даже хорошо, интересно. 

- S3 – более распространенный подход, который делает более простой интерфейс ценой меньшей адаптации под некоторые типы нагрузок. 


Естественно, желая предоставить функциональность всей экосистеме ClickHouse и сделать ту задачу, которая нужна внутри Яндекс.Облака, мы решили сделать так, чтобы эффект от нее получило все сообщество ClickHouse. Мы реализовали ClickHouse over S3, а не ClickHouse over MDS. А это большое количество работы. 

![](https://habrastorage.org/webt/qy/lz/ri/qylzrieyj67be12orjwq4atdchc.jpeg)

Ссылки:

https://github.com/ClickHouse/ClickHouse/pull/7946 "Filesystem abstraction layer”
https://github.com/ClickHouse/ClickHouse/pull/8011 "AWS SDK S3 integration”
https://github.com/ClickHouse/ClickHouse/pull/8649 "Base implementation of IDisk interafce for S3”
https://github.com/ClickHouse/ClickHouse/pull/8356 "Integration of log storage engines with IDisk interface"
https://github.com/ClickHouse/ClickHouse/pull/8862 "Log engine support for S3 and SeekableReadBuffer"
https://github.com/ClickHouse/ClickHouse/pull/9128 "Storage Stripe Log S3 support"
https://github.com/ClickHouse/ClickHouse/pull/9415 "Storage MergeTree initial support for S3"
https://github.com/ClickHouse/ClickHouse/pull/9646 "MergeTree full support for S3"
https://github.com/ClickHouse/ClickHouse/pull/10126 "Support ReplicatedMergeTree over S3"
https://github.com/ClickHouse/ClickHouse/pull/11134 "Add default credentials and custom headers for s3 storage"
https://github.com/ClickHouse/ClickHouse/pull/10576 "S3 with dynamic proxy configuration"
https://github.com/ClickHouse/ClickHouse/pull/10744 "S3 with proxy resolver"

Это список pull request для реализации виртуальной файловой системы в ClickHouse. Это большое количество pull requests. 

![](https://habrastorage.org/webt/w4/is/qr/w4isqruv58rg6u6ssp9lktckooo.jpeg)

Ссылки:

https://github.com/ClickHouse/ClickHouse/pull/9760 "DiskS3 hardlinks optimal implementation"
https://github.com/ClickHouse/ClickHouse/pull/11522 "S3 HTTP client - Avoid copying response stream into memory"
https://github.com/ClickHouse/ClickHouse/pull/11561 "Avoid copying whole response stream into memory in S3 HTTP
client"
https://github.com/ClickHouse/ClickHouse/pull/13076 "Ability to cache mark and index files for S3 disk"
https://github.com/ClickHouse/ClickHouse/pull/13459 "Move parts from DiskLocal to DiskS3 in parallel"

Но на этом работа не закончилась. После того, как фича была сделана, потребовалось еще какое-то количество работы для того, чтобы оптимизировать эту функциональность. 

![](https://habrastorage.org/webt/jk/mo/9o/jkmo9o6t77v03oo5vd6onuhqdls.jpeg)

Ссылки:

https://github.com/ClickHouse/ClickHouse/pull/12638 "Add SelectedRows and SelectedBytes events"
https://github.com/ClickHouse/ClickHouse/pull/12464 "Add profiling events from S3 request to system.events"
https://github.com/ClickHouse/ClickHouse/pull/13028 "Add QueryTimeMicroseconds, SelectQueryTimeMicroseconds and InsertQueryTimeMicroseconds"

И потом потребовалось сделать ее диагностируемой, наладить мониторинги и сделать ее управляемой. 

И все это было сделано так, чтобы все сообщество, вся экосистема ClickHouse результат этой работы получила. 

![](https://habrastorage.org/webt/iw/kg/-v/iwkg-vmb0i6hq9otzmncgbclfas.jpeg)

Давайте перейдем к транзакционным базам данным, к OLTP базам данным, которые лично мне ближе. 

![](https://habrastorage.org/webt/ou/vo/pn/ouvopnorpoizvtmfil_t3ahoheq.jpeg)

Вот это подразделение разработки СУБД с открытым кодом. Вот эти ребята делают уличную магию по улучшению транзакционных открытых баз данных. 

![](https://habrastorage.org/webt/xo/dl/qc/xodlqcn_56fmzuxkpm1nfkabwjk.jpeg)

Один из проектов, на примере которого можно рассказать, о том, как и что мы делаем, это Пулер соединений в Postgres.

Postgres – это процессная база данных. Это означает, что в базу данных должно входить как можно меньше сетевых соединений, которые работают с транзакциями. 

С другой стороны, в облачном окружении типичная ситуация, когда в один кластер пришло сразу тысяча соединений. И задача пулера соединения тысячу соединений упаковать в небольшое количество серверных соединений. 

![](https://habrastorage.org/webt/ky/cy/tz/kycytzuuinaoeclumrycu_wd17q.jpeg)

Можно сказать, что пулер соединения – это телефонист, который перекладывает байтики так, чтобы они эффективно доехали до базы данных. 

К сожалению, нет хорошего русского слова для обозначения пулера соединений. Иногда его называют multiplexer соединений. Если вы знаете, как назвать connection pooler, то обязательно мне скажите, я буду очень рад говорить на правильном русском техническом языке. 

![](https://habrastorage.org/webt/zm/9h/vt/zm9hvtubacw5xpsmog-rt4omoq4.jpeg)

https://pgconf.ru/2017/92899

Мы исследовали пулеры соединений, которые подходили для управляемого postgres'ного кластера. И лучше всего нам подходил PgBouncer. Но мы сталкивались с рядом проблем в PgBouncer. Много лет назад Володя Бородин делал доклады о том, что мы используем PgBouncer, нам все нравится, но есть нюансы, есть над чем работать. 

![](https://habrastorage.org/webt/tc/94/lk/tc94lk4h7chrfcillasllvtgg_y.jpeg)

https://pgconf.ru/media/2017/04/03/20170316H1_V.Borodin.pdf

И мы работали. Те проблемы, с которыми мы сталкивались, мы чинили, мы патчили Bouncer, старались относить pull request в upstream. Но с фундаментальной однопоточностью сложно было работать. 

Нам пришлось собирать каскады с патченых Bouncers. Когда у нас множество однопоточных Bouncers, то на верхнем слое переводят соединения во внутренний слой Bouncers. Это плохо управляемая, сложная в построении и в масштабировании туда и обратно система. 

![](https://habrastorage.org/webt/rf/5i/-y/rf5i-ys1p1nxf_nte8krq9etdiq.jpeg)

Мы пришли к тому, что мы создали свой пулер соединений, который называется Odyssey. Мы написали его с нуля. 

![](https://habrastorage.org/webt/o_/lx/sq/o_lxsquzi6pvecfbhcf6g4lyu2o.jpeg)

https://www.pgcon.org/2019/schedule/events/1312.en.html

В 2019-ом году на конференции PgCon я представлял этот пулер сообществу разработчиков. Сейчас у нас чуть меньше 2 000 звездочек на GitHub, т. е. проект живет, проект популярен. 

И если вы создаете кластер Postgres в Яндекс.Облаке, то это будет кластер со встроенным Odyssey, который перенастраивается при масштабировании кластера туда или обратно. 

![](https://habrastorage.org/webt/tk/4a/dk/tk4adkjrjwp6l41h-est_-ot7he.jpeg)

Чему мы научились в этом проекте? Запуск конкурирующего проекта – это всегда агрессивный шаг, это крайняя мера, когда мы говорим, что есть проблемы, которые не решаются достаточно быстро, не решаются в те промежутки времени, которые нас устроили бы. Но это эффективная мера. 

PgBouncer начал развиваться быстрее. 

И сейчас появились другие проекты. Например, pgagroal, которые развивается разработчиками Red Hat. Они преследуют похожие цели и реализуют похожие идеи, но, конечно, со своей спецификой, которая ближе разработчикам pgagroal. 

![](https://habrastorage.org/webt/uv/10/c_/uv10c_7ivf4g13ljatenq_ft5k0.jpeg)

Еще один случай работы с postgres’ным сообществом – это восстановление на точку во времени. Это восстановление после сбоя, это восстановление из бэкапа. 

![](https://habrastorage.org/webt/gx/cy/zs/gxcyzs5knwacjfmv69ljkh4dqnw.jpeg)

Бэкапилок много и они все разные. У почти каждого вендора Postgres есть свое бэкапное решение. 

Если взять все системы резервного копирования, составить матрицу фич и в шутку посчитать определитель в этой матрице, то он будет нулевой. Что это означает? Что, если взять какую-то конкретную бэкапилку, то ее нельзя собрать из кусков всех остальных. Она уникальна в своей реализации, она уникальна в своем назначении, она уникальна в идеях, которые заложены в нее. И все они специфичны. 

![](https://habrastorage.org/webt/pz/gp/g0/pzgpg0vvzlnrycd_2xvla49apnk.jpeg)

https://www.citusdata.com/blog/2017/08/18/introducing-wal-g-faster-restores-for-postgres/

Когда мы занимались этим вопросом, компания CitusData запустила проект WAL-G. Это система резервного копирования, которая была сделана с прицелом на облачное окружение. Сейчас CitusData – это уже часть Microsoft. А в тот момент, идеи, которые были заложены на начальных релизах WAL-G, нам очень понравились. И мы начали контрибьютить в этот проект. 

![](https://habrastorage.org/webt/ya/gu/ve/yaguvexedvoifvc6z3py4t4zv7w.jpeg)

https://github.com/wal-g/wal-g/graphs/contributors

Сейчас в этом проекте много десятков разработчиков, но в топ-10 контрибьюторов WAL-G входят 6 яндексоидов. Мы много своих идей принесли в туда. И, конечно, сами их реализовали, сами их протестировали, сами их выкатили в production, сами их используем, сами придумываем, куда двигаться дальше, при этом взаимодействуем с большим сообществом WAL-G.

![](https://habrastorage.org/webt/c1/p_/fs/c1p_fs50jru0ym6hqbiolgv88pa.jpeg)

И с нашей точки зрения, сейчас эта система резервного копирования, в том числе учитывая наши усилия, стала оптимальной для облачного окружения. Это лучшее, чем стоит бэкапить Postgres в облаке. 

Что это значит? Мы продвигали достаточно большую идею: резервное копирование должно быть безопасным, дешевым при эксплуатации и максимально быстрым при восстановлении. 

Почему оно должно быть дешевым при эксплуатации? Когда ничего не сломалось, вы не должны знать, что у вас есть резервные копии. Все работаете нормально, вы тратите как можно меньше центрального процессора, вы используете как можно меньше ресурсов вашего диска и шлете как можно меньше байтиков в сеть, чтобы не мешать полезной нагрузке ваших ценных сервисов. 

А когда все сломалось, например, админ дропнул данные, что-то пошло не так, и вам нужно срочно вернуться в прошлое, вы восстанавливаетесь на все деньги, потому что вы хотите ваши данные вернуть быстро и в целости. 

И мы эту простую идею продвинули. И, как нам кажется, нам удалось ее реализовать.

![](https://habrastorage.org/webt/2b/mk/mx/2bmkmxxhyahhlnldoq0r-_fldv8.jpeg)

Но это не все. Мы хотели еще одну маленькую вещь. Мы хотели много разных баз данных. Не все наши клиенты пользуются Postgres. Некоторые пользуются MySQL, MongoDB. В сообществе другие разработчики поддержали FoundationDB. И это список постоянно расширяется. 

Идея того, что база данных эксплуатируется в управляемом окружении в облаке, сообществу нравится. И разработчики поддерживают свои базы данных, которые единообразно вместе с Postgres нашей системой резервного копирования можно копировать. 

![](https://habrastorage.org/webt/e2/h0/6n/e2h06nt_pqd-hcsjur6_nwf7t1g.jpeg)

Чему мы научились в этой истории? Наш продукт, как подразделение развития, это не строчки коды, это не операторы, это не файлы. Наш продукт это не pull requests. Это идеи, которые мы транслируем сообществу. Это технологическая экспертиза и движение технологий в сторону облачного окружения. 

![](https://habrastorage.org/webt/mg/pe/ju/mgpejuhurvke57c1zovzjpvdoyk.jpeg)

Есть такая база данных, как Postgres. Мне ядро Postgres нравится больше всего. Я много времени трачу на то, чтобы развивать ядро Postgres совместно с сообществом. 

![](https://habrastorage.org/webt/vq/wf/kl/vqwfklb7ipnot2bqjnefvn7xvc0.jpeg)

Но тут надо сказать, что у Яндекс.Облака есть внутренняя инсталляция управляемых баз данных. И началась она давным-давно в Яндекс.Почте. Та экспертиза, которая привела сейчас к управляемому Postgres, она была накоплена тогда, когда почта захотела заехать в Postgres. 

У почты очень похожие с облаком требования. Ей нужно, чтобы вы могли отмасштабироваться на неожиданный экспоненциальный рост в любой точке ваших данных. И у почты уже была нагрузка с какими-то сотнями миллионов ящиков огромного количества пользователей, которые постоянно делают много запросов. 

И это был довольно серьезный вызов для команды, которая занималась развитием Postgres. Тогда все проблемы, с которыми мы сталкивались, сообщались сообществу. И эти проблемы исправлялись, причем исправлялись сообществом местами даже на уровне платной поддержки некоторых других баз данных и даже лучше. Т. е. отправить письмо в PgSQL hacker и получить ответ можно в течение 40 минут. Платный саппорт в каких-то базах данных может подумать, что есть более приоритетные вещи, чем ваш баг. 

Сейчас внутренняя инсталляция Postgres – это какие-то петабайты данных. Это какие-то миллионы запросов в секунду. Это тысячи кластеров. Она очень масштабная. 

Но есть нюанс. Она живет не на модных сетевых дисках, а на достаточно простом железе. И там есть тестовое окружение специально для интересных новых вещей. 

![](https://habrastorage.org/webt/ne/ah/q6/neahq6jd70kyfj3yqho3wnrtjr4.jpeg)

И в определенный момент в тестовом окружении мы получили такое сообщение, которое свидетельствует о том, что нарушены внутренние инварианты индексов баз данных. 

Инвариант – это какое-то соотношение, которое мы ожидаем, что будет выполняться всегда. 

Очень критичная ситуация для нас. Она свидетельствует о том, что какие-то данные, возможно, потеряны. А потеря данных – это что-то прямо катастрофическое. 

Общая идея, которой мы следуем в управляемых базах данных в том, что даже прилагая усилия, потерять данные будет сложно. Даже если вы их целенаправленно удалите, вам понадобится еще игнорировать их отсутствие в течение длительного промежутка времени. Сохранность данных – это религия, которой мы следуем довольно усердно. 

А тут возникает ситуация, которая говорит о том, что может быть ситуация, к которой мы можем быть не готовы. И мы начали готовиться к этой ситуации. 

![](https://habrastorage.org/webt/0a/lb/_m/0alb_mja0bnpg5wih6tp8xrrdte.jpeg)

https://commitfest.postgresql.org/23/2171/

Первое, что мы сделали, мы погрепали логи с этих тысяч кластеров. Нашли, какие из кластеров располагаются на дисках с проблемными прошивками, которые теряли обновление страниц данных. Разметили всю кодовую данных Postgres. И те сообщения, которые свидетельствуют о нарушениях внутренних инвариантов, мы разметили кодом, который предназначен для обнаружения коррупции данных. 

Этот патч был практически принят сообществом без особого обсуждения, потому что в каждом конкретном случае было очевидно, что что-то плохое случилось, нужно сообщить в лог об этом. 

![](https://habrastorage.org/webt/s4/rg/dv/s4rgdvjejwglieddfofob7ohoue.jpeg)

После этого мы пришли к тому, что у нас есть мониторинг, который сканирует логи. И в случае подозрительных сообщений будит дежурного, и дежурный чинит. 

Но! Сканирование логов – это дешевая операция на одном кластере и катастрофически дорогая для тысячи кластеров. 

Мы написали расширение, которое называется [Logerrors](https://github.com/munakoiso/logerrors). Оно создает представление о базе данных, в котором можно дешево и быстро поселектить статистику по прошедшим ошибкам. И если нужно будить дежурного, то мы узнаем об этом, не сканируя гигабайтные файлы, а извлекая несколько байт из хеш-таблицы. 

Это расширение было принято, например, в репозитарии для [CentOS](https://yum.postgresql.org/13/redhat/rhel-7-x86_64/logerrors_13-1.1-1.rhel7.x86_64.rpm). Если вы хотите его использовать, вы можете себе его поставить. Конечно, это open source.

![](https://habrastorage.org/webt/r0/l5/n9/r0l5n9oa6i9q0gc4g6c7mzw9slg.jpeg)

https://www.postgresql.org/message-id/flat/DA9B33AC-53CB-4643-96D4-7A0BBC037FA1@yandex-team.ru

Но это не все. Мы начали использовать Amcheck - расширение, которое создано сообществом, для поиска нарушений инвариантов в индексах. 

И мы выяснили, что если эксплуатировать его в масштабе, то там есть баги. Мы начали их чинить. Наши исправления были приняты. 

![](https://habrastorage.org/webt/1y/vw/gn/1yvwgnnvghmi7vm4cu2frvjewlu.jpeg)

https://www.postgresql.org/message-id/flat/59D0DA6B-1652-4D44-B0EF-A582D5824F83@yandex-team.ru

Мы обнаружили, что это расширение не умеет анализировать GiST & GIT индексы. Мы сделали их поддержку. Но это поддержка пока обсуждается сообществом, потому что это относительно новая функциональность и там есть много деталей. 

![](https://habrastorage.org/webt/v3/i_/4i/v3i_4itp4db8g8yzpo2jgl6q8o0.jpeg)

https://commitfest.postgresql.org/29/2667/

И еще мы обнаружили, что в случае проверок индексов на нарушение на лидере репликации, на мастере все работает хорошо, а на репликах, на follower поиск коррупций не такой эффективный. Не все инварианты проверяются. И один инвариант нас беспокоил очень сильно. И мы полтора года общались с сообществом для того, чтобы включить эту проверку на репликах. 

Писали код, который должен следовать всем can… протоколам. Обсуждали довольно долго с Питером Гейганом из Crunchy Data этот патч. Ему пришлось немножко модифицировать существующее B-дерево в Postgres для того, чтобы этот патч принять. Он был принят. И сейчас проверка индексов на репликах тоже стала достаточно эффективной для того, чтобы обнаруживать те нарушения, с которыми мы сталкивались. Т. е. это те нарушения, которые могут быть вызваны ошибками в прошивке дисков, багами в Postgres, багами в ядре Linux, железными проблемами. Довольно обширный список источников проблем, к которым мы готовились. 

![](https://habrastorage.org/webt/gz/zt/d_/gzztd_613jw-odamopufkqsss3m.jpeg)

https://www.postgresql.org/message-id/flat/38AF687F-8F6B-48B4-AB9E-A60CFD6CC261%40enterprisedb.com#0e86a12c01d967bac04a9bf83cd337cb

Но кроме индексов есть такая часть, как heap, т. е. то место, где данные хранятся. И там не так много инвариантов, которые можно было бы проверять. 

У нас есть расширение, которые называется Heapcheck. Мы начали его развивать. И параллельно вместе с нами компания EnterpriseDB тоже стала писать модуль, который они назвали точно так же Heapcheck. Только мы его назвали PgHeapcheck, а они просто Heapcheck. Он у них с аналогичными функциями, немножко с другой сигнатурой, но с теми же идеями. Они чуть-чуть местами получше их реализовали. И раньше выложили в open source.

И теперь мы занимаемся тем, что развиваем их расширение, потому что это уже не их расширение, а расширение сообщества. И в будущем – это часть ядра, которая будет поставляться всем для того, чтобы можно было о будущих проблемах знать заранее. 

![](https://habrastorage.org/webt/bc/mb/ou/bcmboumzwdkfzgumhqajjczdzze.jpeg)

https://www.postgresql.org/message-id/flat/fe9b3722df94f7bdb08768f50ee8fe59%40postgrespro.ru

Местами мы даже пришли к тому, что у нас есть ложноположительные срабатывания у наших мониторингов. Например, система 1С. При использовании базы данных Postgres иногда записывает в нее такие данные, которые сама считать сможет, но pg_dump считать их не сможет. 

Эта ситуация выглядела для нашей системы обнаружения проблем как коррупция. Дежурного разбудили. Дежурный посмотрел, что происходит. Через некоторое время пришел клиент и сказал, что у меня есть проблемы. Дежурный объяснил, в чем проблема. Но проблема в ядре Postgres. 

Я нашел обсуждение этой особенности. И написал, что мы столкнулись с этой особенностью и было неприятно, человек ночью просыпался для того, чтобы разобраться с тем, что это такое. 

![](https://habrastorage.org/webt/wn/r0/qs/wnr0qs-6--bo8nqgwfjwzffw414.jpeg)

https://www.postgresql.org/message-id/flat/fe9b3722df94f7bdb08768f50ee8fe59%40postgrespro.ru

Сообщество ответило: «О, действительно, надо чинить». 

У меня есть простая аналогия. Если вы ходите в ботинке, в который попала песчинка, то, в принципе, можно идти дальше – ничего страшного. Если вы продаете ботинки тысячам человек, то давайте делать ботинки без песка совсем. А если кто-то из пользователей ваших ботинок собрался бежать марафон, то вы хотите сделать очень хорошие ботинки, а потом уже отмасштабировать на всех ваших пользователей. И такие неожиданные пользователи всегда в облачном окружении находятся. Всегда находятся пользователи, которые эксплуатируют кластер каким-то оригинальным способом. К этому надо всегда готовиться. 

![](https://habrastorage.org/webt/_h/ez/qi/_hezqimsnhud1obaiqqd94s5xku.jpeg)

Чему мы здесь научились? Мы научились простой вещи: самое главное объяснить сообществу, что проблема есть. Если сообщество осознало проблему, то возникает естественная конкуренция за то, чтобы проблему решить. Потому что важную проблему все хотят решить. Все вендоры, все хакеры понимают, что сами могут на эти грабли наступить, поэтому хотят их устранить. 

Если вы работаете над какой-то проблемой, но она кроме вас никого не беспокоит, но вы работаете над ней системно и ее в конечном итоге посчитали проблемой, то ваш pull request обязательно будет принят. Ваш патч будет принят, ваши доработки или даже запросы на доработки будут рассмотрены сообществом. В конце концов мы делаем базу данных лучше друг для друга. 

![](https://habrastorage.org/webt/ca/3t/pl/ca3tplkwbsab0rqjukx_tpchevu.jpeg)

Интересная база данных – Greenplum. Это сильно параллельная база данных, основанная на кодовой базе Postgres, которая мне хороша знакома. 

![](https://habrastorage.org/webt/ju/2j/w7/ju2jw7msjgxo-lx0w8b1kvbdpwo.jpeg)

https://greenplum.org/greenplum-database-tables-compression/

И в Greenplum есть интересная функциональность – это append optimized таблицы. Это такие таблицы, в которых можно быстро дописывать. Они могут быть либо столбцовыми, либо строчными. 

Но там не было кластеризации, т. е. не было функциональности, когда можно упорядочить данные, расположенные в таблице, в соответствии с порядком, который есть в каком-то из индексов. 

Ко мне пришли ребята из такси и говорят: «Андрей, ты же знаешь Postgres. И тут почти то же самое. Переключение на 20 минут. Берешь и делаешь». Я подумал, что да, я же знаю Postgres, переключение на 20 минут – надо этим заняться. 

![](https://habrastorage.org/webt/su/n6/n8/sun6n8nwx7tgzefgzauqfc9xvhe.jpeg)

https://github.com/greenplum-db/gpdb/commit/179feb77a034c2547021d675082aae0911be40f7

**Но нет, это было не 20 минут, я писал это несколько месяцев. На конференции PgConf.Russia я подошел к … и спросил**: «Есть какие-то проблемы с этим? Почему нет кластеризации append optimized таблицы?». Он говорит: «Берешь данные. Сортируешь, перекладываешь. Это просто работа». Я: «О, да, это надо просто взять и сделать». Он говорит: «Да, нужны свободные руки, которые это сделают». Я подумал, что точно надо этим заняться. 

И через несколько месяцев я отправил pull request, который реализовывал эту функциональность. Этот pull request посмотрели в Pivotal совместно с сообществом. Конечно, там были баги. 

![](https://habrastorage.org/webt/6f/su/mt/6fsumt8od2ajqza4l6e3kv9b6zy.jpeg)

https://github.com/greenplum-db/gpdb/issues/10150

Но самое интересное, что при мерже этого pull request в самом Greenplum нашли баги. Мы обнаружили, что heap-таблицы при кластеризации иногда нарушают транзакционность. И это вещь, которую надо чинить. И она в том месте, которое я только что трогал. И естественная реакция моя была – хорошо, давайте я этим тоже займусь. 

![](https://habrastorage.org/webt/35/bg/vf/35bgvfkrczmilbqqtcu1jbvz26i.jpeg)

https://github.com/greenplum-db/gpdb/pull/10290

Я починил этот баг. Отправил pull request фиксам. Его помержили. 

![](https://habrastorage.org/webt/am/gd/4l/amgd4l93e4uho-5-_u5err0e1vg.jpeg)

https://github.com/greenplum-db/gpdb-postgres-merge/pull/53

После чего выяснилось, что эту функциональность нужно получить еще в версии Greenplum для PostgreSQL 12. Т. е. приключения на 20 минут продолжаются новыми интересными приключениями. Было интересно потрогать актуальное развитие, где сообщество пилит новые и самые важные фичи. Это помержили. 

![](https://habrastorage.org/webt/kj/3u/dv/kj3udv1prmggrqhwzeqkicrgdgs.jpeg)

https://github.com/greenplum-db/gpdb/pull/10565

Но этим все не закончилось. После всего выяснилось, что нужно и документацию на это все написать. 

Я начал писать документацию. К счастью, пришли документаторы из Pivotal. Для них английский язык родной. Они помогли мне с документацией. По сути, сами переписали то, что я предлагал на настоящий английский. 

И на этом, казалось бы, приключение закончилось. И знаете, что произошло потом? Ко мне пришли ребята из такси и говорят: «Тут два приключения еще есть, каждое на 10 минут». И что мне надо было им сказать? Я сказал, что сейчас доклад на scale сделаю, потом посмотрим ваши приключения, потому что это интересная работа. 

![](https://habrastorage.org/webt/ff/37/ur/ff37urlrosd_-7rczbjz9k8mkti.jpeg)

Чему мы научились в этом случае? Тому, что работа с open source – это всегда работа с конкретным человеком, это всегда работа с сообществом. Потому что в каждой отдельной стадии я работал с каким-то разработчиком, с каким-то тестировщиком, с каким-то хакером, с каким-то документаром, с каким-то архитектором. Я работал не с Greenplum, я работал с людьми вокруг Greenplum.

Но! Есть еще один важным момент – это просто работа. Т. е. приходишь, выпиваешь кофе, пишешь код. Работают всякие простые инварианты. Нормально делай – нормально будет! И это довольно интересная работа. На эту работу есть запрос со стороны клиентов Яндекс.Облака, пользователей наших кластеров как внутри Яндекса, так и снаружи. И, я думаю, что количество проектов, в которых мы участвуем, будет увеличиваться и глубина нашего вовлечения тоже будет увеличиваться. 

На этом все. Давайте переходить к вопросам. 

![](https://habrastorage.org/webt/pf/2h/9k/pf2h9kbvtiyzmvu0vr9asfdh1xe.jpeg)

Сессия вопросов

Привет! У нас очередная сессия вопросов и ответов. И в студии Андрей Бородин. Это человек, который только что вам рассказывал про вклад Яндекс.Облака и Яндекса в open source. У нас сейчас доклад не совсем про Облако, но в то же самое время на таких технологиях мы и базируемся. Не будь того, что сделали вы внутри Яндекса, не было сервиса в Яндекс.Облаке, поэтому спасибо тебе от меня лично. И первый вопрос из трансляции: «На чем написан каждый из проектов, который ты упоминал?».

*Система резервного копирования в WAL-G написана на Go. Это один из наиболее новых проектов, над которым мы работали. Ему буквально всего 3 года. А база данных зачастую – это про надежность. И это означает, что базы данных довольно старые и они написаны обычно на C. Проект Postgres начинался лет 30 назад. Тогда правильным выбором был C89. И на нем Postgres написан. Более современные базы такие, как ClickHouse уже пишут на C++ обычно. Вся системная разработка базируется вокруг C и C++.*

Вопрос от нашего финансового менеджера, который отвечает в Облаке за расходы: «Зачем Облако тратит деньги на поддержку open source?». 

*Тут есть простой ответ для финансового менеджера. Мы это делаем для того чтобы делать наши сервисы лучше. В каком плане мы можем сделать лучше? Мы можем сделать эффективнее, быстрее, сделать что-то более масштабируемым. Но для нас эта история в первую очередь про надежность. Например, в системе резервного копирования мы ревьюним 100 % патчи, которые к нему применяются. Мы знаем, что это за код. И мы спокойнее выкатываем новые версии в production. Т. е. в первую очередь это про уверенность, про готовность к развитию и про надежность*

Еще вопрос: «Отличается ли требования внешних пользователей, которые живут в Яндекс.Облаке, от внутренних пользователей, которые живут во внутреннем Облаке?».

*Профиль нагрузок, конечно, разный. Но с точки зрения моего подразделения, все особенные и интересные случаи создаются на нестандартной нагрузке. Разработчики с фантазией, разработчики, делающие неожиданные вещи, они встречаются равно вероятно как внутри, так и снаружи. В этом плане у нас все примерно одинаково. И, наверное, единственная важная особенность внутри яндексовой эксплуатации баз данных будет то, что внутри Яндекса у нас есть учение. В какой-то момент какая-то зона доступности полностью уходит в тень, и все сервисы Яндекса должны как-то продолжить функционировать, несмотря на это. Вот это небольшое отличие. Но оно создает много исследовательской разработки на стыке базы данных и сетевого стека. В остальном внешние и внутренние инсталляции создают одинаковые запросы на фичи и схожие запросы по улучшению надежности и производительности.*

Следующий вопрос: «Как лично ты относишься к тому, что многое из того, что ты делаешь, используется другими Облаками?». Мы не будем называть конкретные, но многие проекты, которые сделали в Яндекс.Облаке, используются в чужих облаках. 

*Это же классно. Во-первых, это признак того, что мы сделали что-то правильное. И это чешет эго. И мы больше уверены в том, что приняли правильное решение. С другой стороны, это надежда, что в будущем это будет нам приносить новые идеи, новые запросы со стороны сторонних пользователей. Большинство issues на GitHub создаются отдельными сисадминами, отдельными DBA, отдельными архитекторами, отдельными инженерами, но иногда приходят люди со систематизированным опытом и говорят, что в 30 % определенных случаях у нас есть вот такая-то проблема и давайте подумаем, как ее решить. Вот это то, чего мы ждем больше всего. Мы ждем, что у нас будет обмен опытом с другими облачными платформами.*

Ты рассказывал много про марафон. Я знаю, что ты пробежал марафон в Москве. Как результат? Обогнал ребят из Postgres Pro?

*Нет, Олег Бартунов бегает очень быстро. Он на час раньше меня финишировал. В целом я доволен тем, что я добежал. Для меня просто финиш был достижением. В целом это удивительно, что в postgres’овом сообществе столько бегунов. Мне кажется, что есть какая-то зависимость между аэробными видами спорта и стремлением к системному программированию.*

Ты хочешь сказать, что в ClickHouse нет бегунов?

*Я точно знаю, что они там есть. ClickHouse – это тоже база данных. Кстати, мне сейчас Олег пишет: «Идем бегать после доклада?». Это отличная идея.*

Еще вопрос из трансляции от Никиты: «Почему ты баг в Greenplum правил сам, а не отдал junior’ам?». Правда, тут не очень пояснено, что за баг и в каком сервисе, но, наверное, имеется в виду тот, о котором ты рассказывал. 

*Да, в принципе, можно было отдать кому-то. Просто это был код, который я только что поменял. И было естественным продолжить тут же его сделать. В принципе, идея разделять экспертизу с командой – это хорошая идея. Мы обязательно будем разделять задачи по Greenplum между всеми участниками нашего подразделения.*

Раз речь зашла про junior’ов, такой вопрос. Человек решил создать первый коммит в Postgres. Что ему надо сделать, чтобы сделать первый коммит?

*Это интересный вопрос: «С чего начать?». Обычно начать с чего-то в ядре достаточно сложно. В Postgres, например, есть список to do list. Но на самом деле это лист того, что пытались сделать, но не получилось. Это сложные вещи. И обычно можно найти какие-то утилиты в экосистеме, какие-то расширения, которые можно улучшить, которые привлекают меньше внимания разработчиков ядра. И, соответственно, там есть больше точек для роста. На программе Google Summer of code каждый год postgres’ное сообщество выставляет много различных тем, которыми можно было бы заняться. В этом году у нас, кажется, было три студента. Один даже в WAL-G писал по темам, которые важны для Яндекса. В Greenplum все проще, чем в сообществе Postgres, потому что хакеры Greenplum очень хорошо относятся pull request’ам и начинают ревьювить прямо сразу. Отправить патч в Postgres – это какая-то история на месяцы, а в Greenplum придут через день и посмотрят, что ты сделал. Другое дело, что в Greenplum нужно решать актуальные проблемы. Greenplum эксплуатируется не так широко, поэтому найти свою проблему довольно сложно. А в первую очередь надо решать, конечно, проблемы.*


