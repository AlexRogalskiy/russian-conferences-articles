

**Грехи оптимизации производительности. Александр Валялкин, VictoriaMetrics**

Доклад посвящен теме оптимизации производительности, но не совсем оптимизации производительности, а грехам оптимизации производительности. Что это такое – расскажу попозже. Сначала расскажу немного о себе. 

Hello

- 
- Я – Александр Валялкин. Мой ник valyala.  

- 
- Я автор библиотек на Go таких, как Fasthttp, Fastjson, Quicktemplate, т. е. это библиотеки, которые оптимизированы по скорости работ программ.  

- 
- Я увлекаюсь оптимизацией производительности.

- 
- И в данный момент я работаю над собственной timeseries базой данных VictoriaMetrics.  

When do we need performance optimizations?

Когда нам нужна оптимизация производительности? Есть несколько кейсов, когда это нужно:

- 
- Когда ваша программа работает медленно.  

- 
- Когда ваша программа требует очень много hardware ресурсов, чтобы она нормально работала, и вы хотите уменьшить свои расходы на эти ресурсы.

- 
- И третий кейс – это benchmark games. Мы любим оптимизировать наши программы, чтобы они показывали хорошие результаты в benchmarks. И эти результаты необязательно будут соответствовать хорошим результатам в production.

Performance optimization sins

Понятно, что оптимизация производительности не всегда приводит к общему улучшению вашей программы. Она не всегда бесплатна. И цена может быть слишком высока для того, чтобы продолжать заниматься производительностью. 

И в этом докладе я покажу примеры из реальных проектов из Fasthttp, fastjson, VictoriaMetrics. И несколько примеров будет из стандартной Go библиотеки. И последний пример вообще не будет касаться Go. Это про процессоры. 

Fasthttp

Начнем с Fasthttp. Fasthttp – это альтернатива стандартного пакета nethttp, который предоставляет сервер и клиент, и который оптимизирован под скорости работ. 

В Fasthttp мы рассмотрим такие примеры, как: 

- 
- Буферизация ответа.

- 
- Отложенный парсинг HTTP headers.

- 
- Использование slices вместо maps для хранения key, value значений.  

- 
- Переиспользование объекта requestCtx.

- 
- И кэширование DNS записи.  

Fasthttp response buffering

Начнем c response buffering. Вот так выглядит стандартный HTTP протокол flow на Go. Это псевдокод. Это не код HTTP, просто в качестве примера показано. 

Мы получаем connection функцию. Эта функция читает из этого connection http request. Проверяет закрыт он или нет. Если закрыт, то вы входим из функции. Если не закрыт, то процессим этот полученный request и отправляем response в connection. И это делаем в цикле, т. к. connection у нас может быть keep-alive, к которому могут приходить много запросов. 

Чего в этом коде плохого? То, что он на каждый request, на каждый response должен читать и писать из connection, т. е. вызывать системный вызов. 

Чтобы это исправить, мы делаем вот так. Это функция, которая оптимизирована на HTTP pipelining requests.

HTTP pipelining – это когда мы в HTTP connection на сервер отправляем несколько запросов, не дожидаясь ответа на предыдущие запросы.

Вот как выглядит оптимизированная функция. Мы заворачиваем connections в buffers. Это bufio.NewReader, bufio.NewWriter. И потом в цикле читаем request из буферизированного connection. Снова проверяем: если закрыт, то флашим все, что у нас есть в Writer buffer в connection. И возвращаемся. 

Тут тоже самое, что и было на предыдущем слайде. Мы обрабатываем connection и записываем response уже не в connection, а в buffer connection.

И после этого вот эта основная штука, это оптимизация для pipelining connection. Что она делает? Она проверяет – есть ли у нас еще в буфере на чтение какие-то данные, где у нас запросы приходят. И если этот буфер пустой, значит, надо зафлашить responses, которые накопились в буфере на запись.

Что нам такой код дает? Как я уже говорил, он уменьшает количество системных вызовов, необходимых на чтение и запись данных. Буфер по умолчанию равен 4 килобайту. Т. е. при первом чтении из connection из C, который завернут в NewReader, reader пытается считать до 4-х килобайт данных и поместить их в буфер. 

И в 4 килобайтах данных может находиться не один запрос, а много запросов. И таким образом мы с помощью одного системного вызова, получаем сразу много запросов и обрабатываем в этом цикле. 

Fasthttp response buffering benefits

И тоже самое касается в записи response. При записи в буферизованный netr connection данные сразу не пишутся в этот connection. Они записываются в буфер до 4 килобайт. И для того чтобы данные попали в connection, и попали дальше уже на клиент, нужно вызывать функцию «flush». Вот она здесь. Таким образом мы экономим количество системных вызовов, необходимых на отправку ответа клиенту. Это преимущество данной штуки. 

Fasthttp response buffering sins

Но в ней есть и недостатки. 

Первый грех в том, что HTTP pipelining подвержен head of line blocking. По-русски это означает, что если мы отправили несколько запросов в HTTP connection и если первый запрос требует много времени на выполнение, то все остальные запросы после него будут ждать выполнения первого запроса, даже если все остальные запросы могут выполниться намного быстрее. И таким образом это увеличивает наш latency на последующие запросы. 

Второй недостаток. На самом деле HTTP pipelining в реальной жизни мало, где используется. Например, все современные браузеры, хотя и поддерживают HTTP pipelining, но он у них всегда отключен. Почему? Потому что большинство серверов HTTP, которые были популярны в начале и в середине 2000-ых годов, не поддерживали HTTP pipelining, либо имели ошибки в реализации HTTP pipelining. Поэтому так исторически сложилось, что все браузеры поотключали HTTP pipelining. И сейчас вышел HTTP 2.0 протокол. И вместо того, чтобы возобновить HTTP, все браузеры просто включили HTTP 2.0.

И самый главный недостаток оптимизации под HTTP pipelining – это то, что я его сделал для того, чтобы … в Techempower benchmarks в два раза. Techempower benchmark – это benchmark, который проверяет скорость разных HTTP серверов на разных типах запросов. И там есть один тип запросов, который называется plaintext. Это когда отправляется 16 pipelining requests и ожидаются ответы от сервера. И для данного benchmark’а plaintext’а HTTP pipelining оптимизация позволила увеличить скорость HTTP библиотеки сервера в 2 раза. 

И недавно эту штуку поломали. И за нее пришлось заводить баг. Можете потом по нему пройтись, посмотреть, почитать, как ее потом починили. 

Deferring HTTP headers parsing

Следующий пример – это отложенный парсинг HTTP headers.

Fasthttp не парсит сразу HTTP headers в запросах. Он ищет маркер окончания HTTP headers. Это перевод каретки. И складывает нераспарсенный headers в byte slice локально. И этот byte slice парсится только при первом обращении к HTTP headers.

Вот пример кода header на Fasthttp, при котором HTTP header вообще не будет распарсен. Этот код пишет: «Hello, world!» в response и не читает header.

А вот код, который читает header. И, соответственно, мы на этой строчке при чтении header «X-Forwarded-For», Fasthttp сервер на этой строчке парсит HTTP header.

Deferring HTTP headers parsing benefits

Какие преимущества от такой оптимизации? Понятно, что HTTP сервер будет работать быстро, когда нет обращений к HTTP headers каждого запроса.

Deferring HTTP headers parsing sins

Недостатки этой схемы – это то, что в реальной жизни HTTP headers обычно читают эти заголовки. И эта оптимизация показывает свое преимущество в том же benchmark, где код выглядит вот так, как здесь. Просто отправляется ответ и все. 

Slices instead of maps for (key -> value)

Использование slices вместо maps. Fasthttp использует slices key -> value вместо стандартных go’шных maps для того, чтобы хранить список аргументов, т. е. список заголовков, query args, список cookies. 

И рассмотрим, как это выглядит. Вот у нас есть структура key -> value с двумя полями byte slices. И мы … slice map. Вот так выглядит добавление элемента в slice map.

Что мы здесь делаем? Мы помним, что slice map – это у нас byte slice. Поэтому мы сохраняем его во временную переменную kvs и проверяем – есть ли в этом byte slice еще место для дополнительной записи. А если … выше, чем его длина, то значит, есть у нас место. Мы увеличиваем длину на единичку. И на этой строчке мы экономим выделение памяти. Если у нас нет дополнительного элемента в slice, то мы его добавляем. И потом обращаемся к этому элементу. И в этом элементе записываем ключ-значение в byte slice kv.value. И тут используем такую запись [:0], что означает, что мы переиспользуем память, которая уже выделена под эти kv.value, если они раньше были где-то выделены. 

Такой код позволяет переиспользовать как элементы slice map, так и kv.value byte slice в каждом slice map. И, соответственно, это позволяет экономить выделение памяти.  

Вот так реализовано чтение данных из slice map. Мы тут просто проходимся по всем элементам slice map’а и сверяем ключ. И если ключ совпадает с нужным нам, то возвращаем значение. Ничего сложного.

Slices instead of maps for (key -> value) benefits

Какие преимущества у этого метода? В обычных условиях те элементы, в которых используются slice maps, это заголовки, query args и cookies. Обычно их количество небольшое, не превышает пару десятков. И для такого количества slice maps работают быстрее, чем стандартная map в Go.

Также, как я показал на предыдущем слайде slice maps позволяют переиспользовать память в отличии от обычных maps. Т. е. мы можем переиспользовать те же самые slice maps много раз и таким образом экономить на выделении памяти. 

И еще одна фича, которая бывает иногда необходима, это сохранение исходного порядка элементов, которые были у нас получены в запросах. Т. е. если у нас были получены query args в определенном порядке, то в slice map они будут храниться в этом же порядке в отличии от map, где этот порядок будет случайным при чтении из этой map.

Slices instead of maps for (key -> value) sins

Но у этого метода есть недостатки. Если у вас количество этих query args или заголовков слишком большое, то, как вы видели из слайда вот этого, чтение данных из slice map – это O(N) операция, т. е. для того чтобы прочитать все данные, необходимо пройтись по всем элементам slice. И это может быть очень медленно, если вы читаете из slice map, который содержит тысячу или миллион элементов. 

Второй недостаток в том, что slice map подвержен фрагментации. Это означает, что если в slice map сохраните какое-то большое значение, например, мегабайт, то после этого этот slice map всегда будет занимать этот мегабайт, даже если вы будете в него записывать маленькие значения. 

И последний недостаток в том, что значение, которое возвращается из slice map, оно принадлежит этому slice map. И поэтому оно становится невалидным после того, как этот slice map будет переиспользован. И про это нужно помнить. 

RequestCtx re-use

Следующий пример – это переиспользование RequestCtx объекта. RequestCtx объект – это основной объект, который заменяет стандартный HTTP response writer и HTTP request одновременно с Fasthttp. Это единственный объект, который передается в ваш RequestHandler. И этот объект переиспользуется HTTP сервером. 

Вот как это выглядит. Это тоже псевдокод. Мы получаем из пула наш RequestCtx. И потом в цикле читаем запрос. Он помещается в ctx request. Вызываем ваш RequestHandler с вашим RequestCtx и потом записываем response в connection. И это делается в цикле. Т. е. ваш RequestHandler будет переиспользовать тот RequestCtx объект и все, что в нем находится, при каждом вызове вашего RequestHandler.

RequestCtx re-use benefits

Какие в этом преимущества? Преимущества в том, что мы переиспользуем этот RequestCtx. Соответственно, уменьшается количество выделения памяти, увеличивается локальность данных. И увеличивается скорость работы. 

RequestCtx re-use sins

О недостатках, я думаю, вы уже догадались. Такой API легко неправильно использовать. Самый распространенный кейс, когда вы берете какой-то объект из этого RequestCtx в вашем RequestHandler и сохраняете где-нибудь по ссылке, либо передаете в коротину, которая параллельно исполняется. И эта ссылка остается жить после возвращения из RequestHandler. Соответственно, в данном месте вы сразу получаете неправильную работу Fasthttp, т. к. по этой ссылке при последующем переиспользовании этого RequestCtx объекта, будет хранится мусор, а не то, что вы ожидали. 

И есть еще проблема, как в предыдущем примере, это фрагментация памяти. Если у вас в RequestCtx поместится какой-нибудь большой request response body, либо какой-то большой объект. Большой объект при последующем переиспользовании RequestCtx объекта будет занимать память, даже если этот объект уже удалили. Про это нужно помнить. 

DNS caching

Следующий пример – это кэширование DNS. В Fasthttp кроме сервера есть еще и клиент. И для клиента используется resolve DNS имя. И в Fasthttp для реализовано DNS кэширование, т. е. для ускорения этого resolving. Fasthttp кэширует mapping между name и ip в течение минуты. 

Вот псевдокод, как это выглядит. Вот resolveHost. Здесь у нас dnsCache map’a. Мы проверяем есть ли там запись. И если она есть, и если время добавления ее туда не превышает одной минуты, то мы возвращаем эту запись. Это псевдокод, тут не хватает locks. Он специально так сделан, чтобы упростить слайды. На самом деле там все сложнее, но выглядит примерно так. 

DNS caching benefits

Какие преимущества? Мы ускоряем DNS resolving.

DNS caching sins

Недостатки – это то, что такое кэширование ломает ваши HTTP клиенты в окружениях, где часто меняется DNS записи. Типичный пример такого окружения – это Kubernetes, который может при перезагрузке pods, перемещать их между вашими нодами с разными IP-адресами. И, соответственно, у вас после такой перезагрузки Fasthttp клиент будет пытаться подключиться к предыдущей ноде, на которой уже этого pod’а нет. И нужно ждать минуту, пока этот кэш не протухнет. 

Fastjson

С Fasthttp закончили. Теперь пару примеров будет из Fastjson библиотеки. 

Fastjson библиотека – это замена Post JSON библиотеки и она работает быстрее, но не всегда лучше. И сейчас расскажу, почему. 

В ней рассмотрим три недостатка: 

- 
- Это переиспользование памяти.

- 
- Быстрое unescaping строк.  

- 
- И кастомный парсер для integers и floats.

Fastjson memory re-use

Начнем с переиспльзования памяти. Fastjson содержит объект парсер, который используется для парсинга JSON. И все данные, которые вы парсите с помощью этого объекта, принадлежат этому объекту парсера. 

Вот у нас пример. Мы вызывает парсер с этого JSON. Получаем value. И этот value принадлежит парсеру. Т. е. value хранится в объекте парсер. Потом мы читаем строку foo. Помещаем ее в b. Эта строка b также рекурсивно принадлежит этому парсеру. 

И мы на этой строчке вызываем парсер с помощью этого объекта и парсим другой JSON уже. И после этой строчки у нас v и b, и все другие объекты, которые получены через v и через p, становятся невалидными. Т. е. у нас vv возвращается, который содержит уже распарсенную структуру, а все остальное становится уже невалидным. 

Fastjson memory ru-use benefits

Какие преимущества этой штуки? Понятно, что переиспользование памяти позволяет экономить выделение памяти и ускорять парсинг JSON.

Fastjson memory ru-use sins

Недостатки – это то, что API легко неправильно использовать. Если вы будете сохранять ссылки на объекты, полученные из парсера после того, как этот парсер использован для другого JSON’а, то эти ссылки становятся невалидными и в них будет содержаться мусор.

И второй недостаток – это то, что этот парсер может с течением времени вырасти в размерах занимаемой памяти по тем же причинам, о которых рассказывал. Т. е. если у вас парсятся JSON объекты разной конфигурации, где, например, вначале первый элемент занимает один мегабайт, второй 1 байт, а после этого с помощью парсера распарсился JSON, в котором элементы поменялись местами, то у вас парсер уже выделит 2 мегабайта, даже если потом вы будете парсить с помощью него маленькие элементы. Т. е. уже вас парсер будет занимать 2 мегабайта.

Fast string unescaping

Следующий пример – это быстрый unescaping строк. В Fastjson он работает так. Мы ищем escape символ в строке. Если escape символов в строке нет, то мы просто возвращаем эту строку. Там нечего анескэйпить. Если escape символ хотя бы один найден, то мы вызываем функцию, которая не чистит, а проходит по всем байтам этой строки и правильно анескэйпит каждый элемент этой строки.

Fast string unescaping benefist

Преимущества такого подхода в том, что строки, не содержащие escape символов, парсятся очень быстро. Потому что эта функция «strings. IndexByte» - это стандартная go’шная функция. Она оптимизирована по скорости.

Fast string unescaping sins

Недостаток такого метода – это то, что строки с escape символами будут парсятся медленно. 

И второй недостаток – это то, что строки смешанные, где есть в JSON строки с escape символами с не escape символами, т. е. вы получите смешанную производительность, которая может быть иногда быстрой, а иногда медленной. И это не всегда приемлемо. 

Custom parser for integer and floats

Следующий пример – это кастомный парсер для floats и integer в Fastjson.

Преимущества этого кастомного парсера в том, что он быстрее, чем стандартная функция из пакета strconv.

Custom parser for integer and floats sins

Какие недостатки? Сразу возникает вопрос: «Почему стандартные функции из пакета strconv не оптимизированы по скорости максимально?». Потому что они могут обрабатывать ech cases, которые кастомный парсер не может обрабатывать. А на ech cases тратится обычно много ресурсов. Это какие? Это большие числа. Мой парсер умеет работать только с маленькими числами. А если видит большое число, то он просто вызывает strconv функцию для парсинга этих чисел. И, соответственно, на больших числах он будет работать медленней, чем стандартный strconv.

Второй недостаток – это то, что он возвращает 0, если получено неправильное число. 

И третий недостаток – это то, что он может работать неправильно на каких-то corner cases, которые я не предусмотрел.

VictoriaMetrics

Теперь рассмотрим пример из VictoriaMetrics для timeseries данных. И пример интересный. VictoriaMetrics умеет читать данные из Prometheus по remote write API протоколу. И для этого используются protobufs.

Hand-written protobuf parsing

Изначально мы использовали стандартный protobuf парсер, полученный в результате кода генерации из пакета Prometheus. Но, как выяснилось, стандартная реализация выделяет очень много памяти и пришлось этот код оптимизировать вручную. 

Вот пара примеров модификации. Вот как выглядит структура данных из этого protobuf. Есть timeseries, который хранит в себе набор label и набор sample. Каждый label содержит в себе name и value. Sample, соответственно, значение и timestamp.

Пример модификации. В начале у нас в labels тип name имел тип string. Я его поменял на byte slice. И, соответственно, изменил в коде вот эту строчку, куда вписывается name. В этой строчке происходило копирование byte slice data в новую выделенную память в string. Соответственно, в этой строчке у нас было раньше memory location. Я заменил на такую строчку. Убрал string отсюда. Я думаю, многие поняли, какой есть недостаток в этой строчке. Чуть позже расскажу. 

Второй пример модификации. У нас был slice labels, т. е. slice указатель на labels. Я убрал указатели. И поменял добавление нового элемента в label в эту строчку, которая выделяет новый элемент в памяти и добавляет его в label массив на уже знакомый нам код, который проверяет размерность label. И если его cap позволяет, то он не выделяет память, а увеличивает длину массива и переиспользует элемент.

Hand-written protobuf parsing benefits

Преимущество такого подхода в том, что мне удалось уменьшить количество memory location до 0 за счет переиспользования памяти. И, соответственно, удалось увеличить производительность парсинга protobuf.

Hand-written protobuf parsing sins

А недостаток такого метода в том, что этот вручную написанный код сложно поддерживать. И если вспомните пример, где у нас я string удалил, в этом примере data slite byte должен оставаться валидным до тех пор, пока мы используем этот name. Как только мы byte slice, из которого мы парсим protobuf, становится невалидным, соответственно, у нас name становится невалидным, в нем начинается храниться мусор. 

Standard Go packages

Тут два примера посмотрим. Первый пример – это использование pool строк для маленьких чисел. И второй пример – расскажу про sync.pool.

Pools for small numbers

Первый пример выглядит так. Функция «FormatInt» из стандартного пакета Go выглядит примерно так. Пройдет проверка этого числа Int, что оно не превышает nSmalls, который равен 99 или 100. И что base (это система отчислений, в которую надо вернуть результат) равна 10. В этом случае вызывается функция «small», которая возвращает строку из глобальной области видимости без выделения памяти.

Pools for small numbers benefits

Какое преимущество у этого кода? Преимущество в том, что он быстро работает при преобразовании чисел в строку, которая не превышает 100, потому что пропускается выделение памяти и пропускает парсинг этих чисел.

Pools for small numbers sins

Недостаток в том, что этот код работает неожиданно при парсинге смешанных чисел как больших, так и маленьких. Т. е. вы смотрите – профилируется ваша программа и видите, что выделение памяти 0, а потом вдруг началось выделение памяти, потому что вы там начали парсить большие числа. 

CPUs

Последний пример про CPUs. Современные процессоры выполняют инструкции в многоуровневом pipeline. И этот pipeline прерывается, как только процессор находит инструкцию условного перехода, потому что процессоры при нахождении этой инструкции дальше не знают, по какому пути пойти из двух путей. И это замедляет выполнение вашей программы, т. к. pipeline выполнять надо заново. 

Разработчики процессоров нашли выход из этой ситуации с помощью предсказаний ответвлений и спекулятивного выполнения кода. Но как оказалось в прошлом году, что такая оптимизация производительности не совсем бесплатная. И это тоже недостаток, потому что благодаря ей, мы теперь имеем такие уязвимости как Meltdown и Specter, из-за которых теперь приходится замедлять работу процессора. 

Все. 

Questions

*Про* *Fasthttp* *было понятно, зачем это делаете. Т. е. для того чтобы обмануть* *benchmarks**. А остальное? Какие-то метрики повысились на 10 %, т. е. вы душу продали ради повышения производительности на 10 % или на 200 %?*

Это зависит от задач. Но обычно производительность повышается минимум на десятки процентов. Если она повышается меньше, чем на 10 %, то такие оптимизации я обычно не применяю. И в Fasthttp я привел примеры, которые позволяют считить в TechEmpower benchmarks. Но есть куча других примеров c Fasthttp, которые дают реальный выигрыш в производительности по сравнению с Net HTTP.

 


 