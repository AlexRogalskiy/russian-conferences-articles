

**Go optimizations in VictoriaMetrics ⁄** **Александр** **Валялкин****, VictoriaMetrics**

About me

Немного о себе расскажу. Я – Александр Валялкин. Мой GitHub тут написан. Я увлекаюсь Go и оптимизацией производительности. Написал многих всяких полезных и не очень библиотек. Они начинаются либо с fast, либо quick префикса. 

В данный момент я работаю над VictoriaMetrics. Что это такое и что я там делаю? Об этом я расскажу в этой презентации. 

Agenda

План доклада такой:

- Вначале я вам расскажу, что такое VictoriaMetrics.  
- Потом расскажу, что такое временные ряды.  
- Потом расскажу, как работает база данных временных рядов.
- Далее расскажу про архитектуру базы данных: из чего она состоит для хранения временных рядов.  
- И потом перейдем к оптимизациям, которые есть в VictoriaMetrics. Это оптимизация инвертированного индекса и оптимизация для bitset-реализации на Go.

What is VictoriaMetrics?

Что такое VictoriaMetrics кто-нибудь в аудитории знает? Ничего себе, уже много людей знают. Это хорошая новость. Для тех, кто не знает – это база данных для временных рядов. Она основана на архитектуре ClickHouse, на некоторых деталях реализации ClickHouse. Например, на таких, как: MergeTree, параллельное вычисление на всех доступных ядрах процессора и оптимизация производительности с помощью работы над блоками данных, которые помещаются в кэш процессора. 

VictoriaMetrics предоставляет максимальное сжатие данных по сравнению с другими базами данных для временных рядов. 

Она масштабируется как вертикально, т. е. вы можете добавлять большее количество процессоров, большее количество оперативной памяти на одном компе. И VictoriaMetrics будет успешно утилизировать эти доступные ресурсы и будет повышать линейную производительность. 

Также VictoriaMetrics масштабируется и горизонтально, т. е. вы можете добавлять дополнительные ноды в кластер VictoriaMetrics, и ее производительность будет расти почти линейно. 

Как вы догадались, VictoriaMetrics быстрая база данных, потому что других я не могу писать. И она написана на Go, поэтому я про нее рассказываю на этом этапе. 

What is time series?

Кто знает, что такое временной ряд? Тоже много людей знает. Временно ряд -  это серия пар timestamp значения, где эти пары отсортированы по времени. Значение представляет из себя число с плавающей точкой – float64.

И каждый временной ряд уникально идентифицирован ключом. Из чего состоит этот ключ? Он состоит из непустого сета пар ключ-значения. 

Вот пример временного ряда. Ключом этого ряда является список пар _name_= «cpu_usage» – это название метрики. Instance = «my-server» - это компьютер, на котором эта метрика собрана. И datacenter = «us- east» - это datacenter, где стоит этот компьютер.

У нас получилось имя временного ряда, состоящее из трех пар ключ-значения. И этот ключ мэпится на список пар timestamp, value – t1, t3, t3, tN. Это timestamps и значение 10, 20, 12, 15. Это cpu-usage в данный момент времени для данного ряда. 

Time series applications

Где могут быть использованы временные ряды? У кого-нибудь есть идеи? 

- В DevOps можно измерять показания загрузки CPU, RAM, сети, rps, количество ошибок и т. д.  
- IoT – можем мерить температуру давление, geo coordinates и еще что-нибудь.
- Также финансы – можем мониторить цены на всякие стоки.  
- Кроме этого временные ряды могут использоваться в мониторинге производственных процессов на заводах. Есть у нас пользователи, которые используют VictoriaMetrics для мониторинга ветровых турбин, для роботов.  
- Также временные ряды полезны для сбора информации с датчиков машин таких, как здоровье разных частей машин. Например, для двигателя; для измерения давления в шинах; для измерения скорости, расстояния; для измерения расхода бензина и т. д.
- Также временные ряды могут использоваться для мониторинга самолетов. В каждом самолете есть черный ящик. И черный ящик собирает временные ряды по разным параметрам здоровья самолета. И также используются в аэрокосмической промышленности.  
- Healthcare – это диаграмма давления крови.  

Может быть, еще есть применения, про которые я забыл, но, я надеюсь, что вы поняли, что временные ряды активно используются в современном мире. И объем их использования с каждым годом растет. 

What does time series database do?

Для чего нужна база данных для временных рядов? Почему нельзя использовать SQL базу для хранения временных рядов? 

Да, много insertes, большой объем информации, который сложно обрабатывать, сохранять в обычных базах данных. Поэтому появились специализированные данные для временных рядов. И эти базы сохраняют точки timestamp, value с заданным ключом. 

Они предоставляют API для чтения этих сохраненных данных по ключу, по одной паре ключ-значение, либо по нескольким таким парам, либо по regexp. Например, вы хотите найти загрузку процессора всех ваших сервисов в datacenter в Америке, то вот такой запрос. Это псевдозапрос, который позволит вам выбрать все значения временных рядов в datacenter в U.S.

И базы данных для временных рядов представляют специализированные языки запросов, потому что SQL для временных рядов не очень хорошо подходят. Хотя есть базы данных, которые SQL поддерживают, но он не очень хорошо подходит. Более хорошо подходят такие языки запросов как PromQL, InfluxQL, Flux, Q. Я надеюсь, что кто-то слышал хотя бы одном из этих языков. О PromQL, наверное, слышали многие. Это язык запросов Prometheus.

Time series database architecture

Вот как выглядит архитектура современной базы данных для временных рядов на примере VictoriaMetrics.

Она состоит из двух частей. Это хранилище для инвертированного индекса и хранилище для значений временных рядов. Эти хранилища разделены. 

Когда приходит новая запись в базу данных, то мы сначала обращаемся в инвертированный индекс, чтобы найти идентификатор временного ряда по набору label=value для данной метрики. Находим этот идентификатор и сохраняем значение в data storages.

Когда приходит какой-то запрос на выборку данных из time series database, мы в первую очередь лезем в инвертированный индекс. Достаем все timeseries_ids записи, которые соответствуют данному набору label=value. И затем достаем все необходимые данные из data points storage по timeseries_ids.

Query life

Рассмотрим пример запроса, как база данных для временных рядов обрабатывает входящий select-запрос.

- В первую очередь она достает все timeseries_ids c инвертированного индекса, который приходит по заданной паре label=value, либо regexp.  
- И потом достает все data points из всех storages на заданном интервале времени.  
- После этого база данных производит какие-то вычисления над этими data points, которые заданы в запросе пользователя. И после этого возвращает ответ.  

Inverted index

В данной презентации я вам расскажу про первую часть. Это поиск timeseries_ids по инвертированному индексу. Про вторую часть и третью часть вы можете потом посмотреть исходники VictoriaMetrics, либо подождать, пока я подготовлю другие доклады. 

Приступим к инвертированному индексу. Многим может показаться, что это просто. Кто знает, что такое инвертированный индекс и как он работает? О, уже не так много людей. Но давайте попытаемся понять, что это такое. 

На самом деле все просто. Это просто map, которая мэпит ключ-значение. Что такое ключ? Эта пара label=value, где «label» и «value» - это строки. А значения – это набор timeseries_ids, в который входит заданный label + value.

Инвертированный индекс позволяет быстро находить все ids временных рядов, у которых есть заданные label=value.

А также он позволяет находить быстро ids временных рядов для нескольких пар label=value, либо для пар label=regexp. Как это происходит? С помощью нахождения пересечения множества ids для каждой пары label=value.

Inverted index: naive implementation

Рассмотрим различные имплементации инвертированного индекса. Начнем с самой простой наивной имплементации. Она выглядит вот так. 

Функция getMetricIDs получает список строк. Каждая строка содержит label=value, для которых нужна metricIDs. И эта функция возвращает список metricIDs.

Как это работает? Вот у нас есть глобальная переменная, которая называется invertedIndex. Это обычная map, которая мэпит строку на slice … . Строка содержит label=value.

Реализация функции. Достаем metricIDs для первого label=value отсюда значение. И потом проходимся по всем остальным label=value, достаем metricIDs для них. И вызываем функцию «intersectInts», про которую будет рассказано далее. Она находит пересечения списков вот этого с этим. И эта функция возвращает пересечение этих списков. 

Inverted index: naïve implementation issues

Как вы видите, реализация инвертированного индекса не очень сложная. Но это наивная реализация. Какие у нее есть недостатки? Главный недостаток наивной имплементации в том, что такой инвертированный индекс у нас хранится в оперативной памяти. И после перезапуска приложения мы теряем этот индекс. Нет сохранения этого индекса на диск. И для базы данных такой инвертированный индекс вряд ли подойдет.

И второй недостаток тоже связан с памятью. Размер инвертированного индекса должен помещаться в оперативную память. Если он превысит размер оперативной памяти, то очевидно, что мы получим – out of memory error. И программа не будет работать. 

Inverted index: LevelDB

Данную проблему можно решить с помощью готовых решений таких, как LevelDB, либо RocksDB.

Если вкратце, то это база данных, которая позволяет сделать быстро три операции. 

- Первая операция – это запись ключ-значение в эту базу. Это она делает очень быстро, где ключ-значение – это произвольные строки.  
- Вторая операция – это быстрый поиск значения по заданному ключу.
- И третья операция – это быстрый поиск всех значений по заданному префиксу.  

LevelDB и RocksDB – эти базы разработаны в Google и Facebook. Сначала появился LevelDB. Потом Facebook взял LevelDB в реализацию и начал ее улучшать, сделал RocksDB. И сейчас на RocksDB внутри Facebook работают почти все внутренние базы данных, в том числе они перевели на RocksDB и MySQL. Они назвали его MyRocks.

И инвертированный индекс можно реализовать с помощью LevelDB. Как это сделать? Мы сохраняем в качестве ключа label + value. А в качестве значения – идентификатор временного ряда, где есть этот label + value.

И если у нас есть много временных рядов c данной label + value, то будет много строчек в этой базе данных с одинаковыми label + value и разными timeseries_ids. И чтобы получить список всех timeseries_ids, которые начинаются с данной label + value, мы делаем range scan, под который оптимизирована данная база данных. Т. е. убираем все строчки, которые начинаются с label + value и получаем все timeseries_ids.

Вот примерная реализация как она выглядела бы на Go. У нас есть инвертированный индекс. Это LevelDB.

Функция такая же как для наивной реализации. Она почти строчка в строчку повторяет наивную реализацию. Единственный момент, что мы вместо обращения к map обращаемся к инвертированному индексу. Достаем все значения для первой label + value, все значения metricIDs. Потом проходимся по всем label + value оставшимся и достаем все metricIDs для них. И находим пересечения. 

Inverted index: LevelDB issues

Вроде бы все хорошо, но в этом решении есть недостатки. И VictoriaMetrics вначале реализовывала инвертированный индекс на основе LevelDB. Но в итоге пришлось отказаться от него.

Почему? Потому что LevelDB медленней, чем наивная реализация. В наивной реализации по заданному ключу мы сразу достаем весь slice metricIDs. Это операция очень быстрая, она в памяти работает. И моментально весь slice готов для использования. 

А в LevelDB при каждом вызове функции «GetValues» нужно пройтись по всем строчкам, которые начинаются с label=value ключа. И для каждой строчки достать значение timeseries_IDs. И из таких timeseries_IDs собрать slice этих timeseries_IDs. Очевидно, что это медленней намного, чем просто обращение к обычному map ключу. 

И второй недостаток в том, что LevelDB написан на C. И обращение к C-функциям из Go не очень быстрое. Оно занимает сотни наносекунд. И это не очень быстро, потому что по сравнению с обычным вызовом функции go’шной, которая занимает 1-5 наносекунд, разница получается в десятки раз. И для VictoriaMetrics – это было фатальным недостатком. 

Inverted index: mergeset

Поэтому я написал кастомную реализацию инвертированного индекса. И назвал это mergeset.

Mergeset основан на MergeTree структуре данных. Это опять же из ClickHouse. Очевидно, что mergeset должен быть оптимизирован для быстрого поиска IDs временных рядов по заданному ключу. И mergeset написан полностью на Go. И вы можете посмотреть исходники VictoriaMetrics на GitHub. Реализация mergeset лежит в папочке /mergeset. И вы можете попытаться разобраться, что там происходит. 

API mergeset очень похож на LevelDB и RocksDB. Т. е. он позволяет быстро сохранить туда новые записи и быстро выбрать записи по заданному префиксу.

Inverted index: production issues

Про недостатки mergeset мы поговорим попозже. Сейчас поговорим о том, какие возникли в production проблемы в VictoriaMetrics при реализации инвертированного индекса.

Почему они возникли?

Первая причина – это high churn rate. В переводе на русский – это частая смена временных рядов. Т. е. временной ряд заканчивается внезапно и начинается новый ряд, либо начинаются много временных рядов. И это происходит часто. 

И большая проблема – это большое количество временных рядов. Вначале, когда мониторинг набирал популярность, количество временных рядов было маленьким. Например, на каждый компьютер нужно мониторить загрузку процессора, памяти, сети и диска. 4 временных ряда на каждый компьютер. У вас, допустим, 100 компьютеров и 400 временных рядов. Это очень мало. 

С течением времени люди придумали, что можно измерять более детализированную информацию. Например, измерять загрузку не всего процессора, в отдельности каждого процессорного ядра. Если у вас есть 40 процессорных ядер, то, соответственно, у вас в 40 раз больше временных рядов появляется для измерения загрузки процессора. 

Но это еще не все. У каждого процессорного ядра может быть несколько состояний таких, как idle, когда он простаивает. А также работа в user space, работа в kernel space и другие состояния. И каждое такое состояние тоже можно мерить, как отдельный временной ряд. И, соответственно, еще умножает на 7-8 раз количество временных рядов измерения. 

У нас из одной метрики получилось 40 х 8 = 320 метрик на один компьютер только. И умножаем на 100, получаем 3 200 вместо 400. 

Потом появился Kubernetes. И все еще ухудшил, потому что в Kubernetes может хостится много разных кластеров. А каждый кластер в Kubernetes состоит из многих нодов. И все это нужно мониторить. И кроме этого у нас есть постоянный deployment новых версий ваших кластеров. И для каждой новой версии нужно создавать новые временные ряды. И в итоге количество временных рядов растет экспоненциально. И все сталкиваются с проблемой большого количества временных рядов, которая называется high-cardinality. Это проблема. И VictoriaMetrics с ней успешно справляется по сравнению с другими базами данных для временных рядов. 

High churn rate

Рассмотрим подробнее high churn rate. Из-за чего появляется high churn rate на production? Потому что некоторые label или тэги постоянно меняются. 

Например, возьмем Kubernetes, в котором есть понятие «deployment», т. е. когда выкатывается новая версия вашего приложения. И разработчики Kubernetes почему-то решили добавить id’шку deployment в label.

К чему это привело? К тому, что при каждом новом deployment у нас все старые временные ряды прерываются. И начинаются новые временные ряды с новым label, deployment id. И таких рядов может быть сотни тысяч и миллионов. 

Но важная особенность этой штуки в том, что общее количество временных рядов растет, но количество временных рядов, которые в данный момент активны, на которые приходят данные, остается постоянным. И такое состояние называется – high churn rate.

И основная проблема high churn rate в том, чтобы обеспечить постоянную скорость поиска всех временных рядов по заданному набору label за какой-то интервал времени. Обычно это интервал времени за последний час или за последний день. 

Partition inverted index by time

Как эту проблему решить? Вот первый вариант. Это разделять инвертированный индекс на независимые части по времени. Т. е. проходит какой-то интервал времени, заканчиваем работать с инвертированным индексом текущим. И создаем новый инвертированный индекс. Проходит еще один интервал времени, создаем еще один и еще один. 

И при выборке из этих инвертированных индексов мы находим набор инвертированных индексов, которые попадают в заданный интервал. И, соответственно, выбираем оттуда id временных рядов. 

Это позволяет сэкономить ресурсы, потому что нам не нужно просматривать части, которые не попадают в заданный интервал. Т. е. обычно, если мы выбираем данные за последний час, то за предыдущие временные интервалы мы пропускаем запросы. 

Per-day timeseries_ids sets for active time series

Есть еще один вариант решения этой проблемы. Это хранить для каждого дня отдельный список id временных рядов, которые встретились за этот день. 

Преимущество этого решения по сравнению с предыдущим решением в том, что мы не дублируем информацию об временных рядах, которые не исчезают с течением времени. Они постоянно находятся и не меняются. 

Недостаток в том, что такое решение сложнее в реализации и сложнее в дебаге. И VictoriaMetrics выбрала это решение. Так сложилось исторически. Такое решение тоже показывает себя неплохо, по сравнению с этим. Потому что это решение не было реализовано из-за того, что приходится дублировать данные в каждом partition для временных рядов, которые не изменяются, т. е. которые не исчезают с течением времени. И VictoriaMetrics была в первую очередь оптимизирована по потреблению дискового пространства, и данная реализация ухудшала потребление дискового пространства. А вот эта реализация лучше подходит для потребления дискового пространства, поэтому она была выбрана. 

И пришлось с ней бороться. Борьба заключалась в том, что в данной реализации нужно все равно выбирать намного большее количество timeseries_ids для данных, чем, когда инвертированный индекс разделен по памяти.

Store multiple timeseries_ids per mergeset row

И как мы решили эту проблему? Мы ее решили оригинальным образом – путем сохранения нескольких идентификаторов временных рядов в каждой записи инвертированного индекса вместо одного идентификатора. Т. е. у нас есть ключ label + value, который встречается в каждом временном ряду. И теперь мы сделали несколько timeseries_id.

Вот пример. Раньше у нас было N записей, а теперь у нас есть одна запись, префикс у которой такой же, как и у всех остальных. У предыдущей записи значение содержит все id временных рядов. 

Это позволило увеличить скорость сканирования такого инвертированного индекса до 10 раз. И позволило уменьшить потребление памяти для кэша, потому что теперь мы храним строку label + value только один раз в кэше вместе N раз. И эта строка может быть большой, если у вас в тегах и labels хранятся длинные строчки, которые любит туда пихать Kubernetes.

Shard inverted index by time series key (a set of (label=value) pairs)

Еще один вариант ускорения поиска по инвертированному индексу – это шардирование. Создание нескольких инвертированных индексов вместо одного и шардирование данных между ними по ключу. Это набор key value пар. Т. е. у нас получается несколько независимых инвертированных индексов, которые мы можем опрашивать параллельно на нескольких процессорах. Предыдущие реализации позволяли работать только в однопроцессорном режиме, т. е. сканировать данные только на одном ядре. Данное решение позволяет сканировать данные сразу на нескольких ядрах, как любит это делать ClickHouse.

Optimizations for timeseries_ids sets intersection 

А теперь вернемся к нашим баранам – к функции пересечения timeseries_ids. Рассмотрим, какие могут быть реализации. Эта функция позволяет находить timeseries_ids для заданного набора label + value.

Timeseries_ids sets intersection: naive approach

Первый вариант – это наивная реализация. Два вложенных цикла. Вот мы получаем на входе функции «intersectInts» два slices a и b. И на выходе она должна вернуть вам пересечение этих slices.

Наивная реализация выглядит так. Мы проходимся по всем значениям из slice a, внутри этого цикла проходимся по всем значениям slice b. И сравниваем их. Если они совпадают, значит, мы нашли пересечение. И сохраняем его в result.

What’s wrong with the naive approach?

Какие есть недостатки? Квадратичная точность - это главный ее недостаток. Например, если у вас размеры slice a и b по одному миллиону, то эта функция никогда вам не вернет ответ. Потому что ей нужно будет сделать один триллион итераций, что очень много даже для современных компьютеров. 

Timeseries_ids intersection: map

Вторая реализация основана на map. Мы создаем map. Помещаем в эту map все значения из slice a. И потом пробегаемся отдельным циклом по slice b. И проверяем – есть ли это значение из slice b в map. Если оно есть, то добавляем его в результат. 

Set intersection with map

Какие преимущества? Преимущество в том, что тут только линейная сложность. Т. е. функция намного быстрее выполнится для больших размеров slices. Для миллионного размера slice эта функция выполнится за 2 миллиона итераций, в отличии от триллиона итераций, как в предыдущей функции. 

А недостаток в том, что эта функция требует больше памяти для того, чтобы создать эту map.

И второй недостаток – это большой overhead на хеширование. Этот недостаток не очень очевидный. И для нас он тоже был не очень очевидным, поэтому вначале в VictoriaMetrics реализация intersection была через map. И потом профилирование показало, что основное процессорное время тратится вот здесь и вот здесь. Здесь на запись в map, а здесь на проверку наличия значения в этой map.

Почему тратится процессорное время? Потому что в данных строчках Go производит операцию хеширования. Т. е. он вычисляет hash от ключа, чтобы обратиться потом по заданному индексу в HashMap. И операция по вычислению hash’а выполняется за десятки наносекунд. Но это медленно для VictoriaMetrics.

Timeseries_ids intersection: customized bitset

И я решил реализовать bitset, оптимизированный специально для этого случая. И вот теперь как выглядит пересечение двух slices. Тут создаем bitset. Добавляем в него элементы с первого slice. И потом проверяем наличие этих элементов во втором slice. И добавляем их в результат. Т. е. почти не отличается от предыдущего примера. Единственное, что мы здесь заменили обращение к map кастомными функциями «add» и «has».

Set intersection with customized bitset

И с первого взгляда кажется, что это должно работать медленней, если раньше там использовалась стандартная map, а тут еще какие-то функции вызываются, но профилирование показывает, что эта штука работает в 10 раз быстрее, чем стандартная map для кейса VictoriaMetrics.

И кроме этого она использует гораздо меньше памяти по сравнению с реализацией на map. Потому что мы храним здесь биты вместо восьмибайтовых значений. 

Недостаток у такой реализации в том, что она не такая очевидная, не тривиальная. 

И еще один недостаток, который многие могут не замечать – это то, что данная реализация может плохо работать в некоторых случаях. Т. е. она оптимизирована для конкретного случая, для данного случая пересечения ids временных рядов VictoriaMetrics. И это не значит, что она подойдет для всех случаев. И если ее неправильно использовать, то получим не прирост производительности, а out of memory error и замедление производительности. 

Customized bitset: implementation details

Рассмотрим реализацию данной структуры. Если хотите посмотреть, то она находится в исходниках VictoriaMetrics, в папке lib/uint64set. Она оптимизирована именно для случая VictoriaMetrics, где timeseries_id представляет из себя 64-битное значение, где первые 32 бита в основным постоянные и меняются только последние 32 бита.

И данная структура данных не хранится на диске, она только работает в памяти. 

Lib/uint64set API

Вот ее API. Он не очень сложный. API подогнан именно под конкретный пример использования VictoriaMetrics. Т. е. тут нет лишних функций. Здесь функции, которые явно используются VictoriaMetrics.

Есть функции «add», которая добавляет новые значения. Есть функция «has», которая проверяет новые значения. И есть функция «del», которая удаляет значения. Есть вспомогательная функция «len», которая возвращает длину этого сета по количеству записей, которые находятся в сете. Функция «clone» клонирует этот сет. И функция «appendto» преобразует этот сет в slice ids.

Lib/uint64set internals

Вот как выглядит реализация этой структуры данных. Есть в set два элемента. 

ItemsCount – это вспомогательное поле, чтобы быстро вернуть количество элементов в set. Можно было бы обойтись без этого вспомогательного поля, но пришлось его добавить сюда, потому что VictoriaMetrics часто опрашивает в своих алгоритмах длину bitset.

Второе поле – это buckets. Это slice из структуры bucket32. В каждой структуре хранится hi поле. Это верхние 32 бита. И два slice: b16his и buckets из bucket16 структур. 

Тут хранятся верхние 16 бит второй части 64-битной структуры. А здесь хранятся bitsets для младших 16 бит каждого байта. 

Bucket64 состоит из массива uint64. Длина вычисляется с помощью этих констант. В одном bucket16 максимум может хранится 64 000 бит. Если это поделить на 8, то это 8 килобайт. Если поделить еще раз на 8, то это 1 000 uint64 значение. Т. е. Bucket16 – это у нас 8 килобайтная структура. 

Lib/uint64set internals: Set.Add

Рассмотрим, как реализован один из методов этой структуры добавления нового значения. 

Все начинается с uint64 значения. Вычисляем верхние 32 бита, вычисляем нижние 32 бита. Проходимся по всем buckets. Сравниваем верхние 32 бита в каждом bucket с добавляемым значением. И если они совпадают, то вызываем функцию «add» в структуре b32 buckets. И добавляем туда нижние 32 бита. И если это вернуло «true», то это значит, что мы добавили такое значение туда и у нас такого значения не было.  Если он «false» возвращает, то такое значение уже было. И увеличиваем количество элементов в структуре. 

Если мы не нашли нужный bucket с нужным hi-значением, то мы вызываем функцию «addAlloc», которая выделает новый bucket, добавляет его в bucket-структуру.

Lib/uint64set internals: bucket32.add

Это реализация функции b32.add. Она похожа на предыдущую реализацию. Мы вычисляем старшие 16 бит, младшие 16 бит.

Потом мы проходимся по всем верхним 16 битам. Находим совпадения. И при совпадении вызываем метод add, который рассмотрим на следующей странице для bucket16.

Lib/uint64set internals: bucket16.add

И вот самый нижний уровень, который должен быть максимально оптимизирован. Мы вычисляем для uint64 id значение в slice bit, а также bitmask. Это маска для данного 64-битного значения, по которой можно проверить наличие этого бита, либо установить его. Мы проверяем наличие этого бита, установленного и устанавливаем его, и возвращаем наличие. Вот такая реализация у нас, которая позволила ускорить операцию пересечения ids временных рядов в 10 раз по сравнению с обычными maps.

More optimizations

В VictoriaMetrics помимо этой оптимизации есть много других оптимизаций. И большинство из этих оптимизаций добавлены не просто так, а после профилирования кода в production.

Это главное правило оптимизации – не добавлять оптимизацию, предполагая, что здесь будет узкое место, потому что может оказаться, что там не будет узкого места, а оптимизация обычно ухудшает качество кода и качество поддерживаемого кода. Поэтому стоит оптимизировать только после профилирования и желательно в production, чтобы это были реальные данные. Кому интересно, можете посмотреть исходники VictoriaMetrics и изучить другие оптимизации, которые там есть. 

Вопросы

*У меня вопрос про* *bitset**. Очень похоже на реализацию* *C**++* *vector* *…,* *оптимизированный* *bitset**. Вы оттуда взяли реализацию?*

Нет, не оттуда. При реализации этого bitset я ориентировался на знания структуры этих ids timeseries, которые в VictoriaMetrics. А структура их такая, что верхние 32 бита всегда постоянны, в основном постоянны, т. е. они могут меняться, но они постоянные. А нижние 32 бита могут меняться. Чем ниже бит, тем чаще он может меняться. Поэтому эта реализация именно оптимизирована под данную структуру данных. И C++ реализация, насколько я знаю, оптимизирована под общий случай. А если делать оптимизацию под общий случай, то это значит, что она будет не самой оптимальной под конкретный случай. 

Советую вам еще посмотреть доклад Алексея Миловида. Он где-то месяц назад рассказывал про оптимизации в ClickHouse под конкретные специализации. Он как раз рассказывает, что в общем случае C++ реализация или какая-нибудь другая реализация заточены под хорошую работу в среднем по больнице. Она может работать хуже, чем специализированная реализация под конкретные знания, как у нас, когда мы знаем, что верхние 32 бита в основном постоянны.

*У меня второй вопрос. В чем кардинальное отличие от* *InfluxDB**?*

Кардинальных отличий много. Если по производительности и по потреблению памяти, то InfluxDB в тестах показывает в 10 раз больше потребления памяти для high cardinality временных рядов, когда их у вас много, например, миллионы. Например, VictoriaMetrics потребляет 1 GB на миллион рядов активных, а InfluxDB при этом потребляет 10 Gb. И это большая разница. 

И второе кардинальное отличие в том, что в InfluxDB странный язык запросов – flux и IFQL. Оба языка немного странноватые. Они сами изобрели их. Они не очень удобны для работы с временными рядами по сравнению с PromQL, который поддерживается в VictoriaMetrics. PromQL – это язык запросов из Prometheus.

И еще одно отличие – это то, что у InfluxDB тоже немного странноватая модель данных, где в каждой строчке может храниться несколько fields с разным набором тегов. И эти строчки делятся еще на всякие таблицы. Вот эти дополнительные усложнения усложняют обычно последующую работу с этой базой. Ее сложно поддерживать и понимать. 

В VictoriaMetrics все гораздо проще. Там каждый временной ряд представляет из себя ключ-значение. Значение – это набор этих временных точек – timestamp + value, а ключ – это набор label + value. И нет никакого разделения на fields и measurements. И это позволяет вам выбирать любые данные и их вместе комбинировать, складывать, вычитать, умножать, делить в отличие от InfluxDB, где вычисления до сих пор не реализованы, насколько я знаю. И если даже реализованы, то сложно, надо писать кучу кода. 

*У меня есть уточняющий вопрос. Я правильно понял, что была какая-то проблема, про которую вы рассказывали, что этот инвертированный индекс не вмещается в память, поэтому там партицирование идет?*

Вначале я показал наивную реализацию инвертированного индекса на map стандартной go’шной. Такая реализация не подходят для баз данных, потому что этот инвертированный индекс не сохраняется на диске, а база данных должна сохранять на диск, чтобы при рестарте эти данные оставались доступные. А при данной реализации при рестарте приложения у вас инвертированный индекс пропадет. И вы потеряете доступ к данным ко всем, потому что не сможете найти их. 

*Здравствуйте! Спасибо за доклад! Меня зовут Павел. Я из компании* *Wildberries**. У меня несколько вопросов к вам. Вопрос первый. Как вам кажется, если бы вы выбрали другой принцип при построении архитектуры своего приложения и партиционировали данные по времени, то, возможно, у вас получилось бы делать пересечение данных при поиске, основываясь только на том, что в одной партиции находится данные за один промежуток времени, т. е. за один интервал времени и вам бы не пришлось заботиться о том, что у вас куски по-разному разбросаны? Вопрос номер 2 -  раз вы реализуете подобный алгоритм с* *bitset* *и всем остальным, то, возможно, вы пробовали использовать инструкции процессора? Может быть, вы пробовали такие оптимизации?*

На второй сразу отвечу. Мы до этого еще не дошли. Но если надо будет, дойдем. А первый, какой был вопрос?

*Вы обсуждали два сценария. И сказали, что выбрали второй с более сложной имплементацией. И не предпочли первый, где данные партиционированы по времени.* 

Да. В первом случае суммарный объем индекса был бы больше, потому что мы в каждой партиции должны были бы хранить дубли данных для тех временных рядов, которые продолжаются сквозь эти все партиции. И если у вас churn rate у временных рядов небольшой, т. е. постоянно одни и те же ряды используются, то в первом случае мы бы намного сильнее проиграли по сравнению со вторым случаем.

А так – да, партиционирование по времени – хороший вариант. Его Prometheus использует. Но в Prometheus есть другой недостаток. При слиянии этих кусков данных, ему требуется держать в памяти мета-информацию по всем label и timeseries. Поэтому, если куски данных большие, которые он сливает, то потребление памяти очень сильно растет при слиянии, в отличии от VictoriaMetrics. При слиянии VictoriaMetrics вообще не потребляет память, там какие-то пара килобайт потребляются, независимо от размеров кусков данных, которые сливают.  

*Алгоритм, который у вас используется, он использует память. В ней отмечаются* *timeseries**-метки, на которых есть значения. И таким образом вы проверяете парное наличие в одном массиве данных и в другом. И понимаете – произошел* *intersect* *или нет. Обычно в базах данных реализуют курсоры, итераторы, хранящие текущее свое состоящее и бегущее по отсортированным данным за счет вы имеете простую сложность у данных операций.* 

Почему мы не используем курсоры для пересечения данных?

*Да.* 

У нас в LevelDB или в mergeset хранятся как раз отсортированные строчки. И мы можем курсором пройтись и найти пересечение. А почему не используем? Потому что – это медленно. Потому что курсоры подразумевают под собой, что на каждую строчку нужно вызвать функцию. Вызов функции – это 5 наносекунд. И если у вас 100 000 000 строчек, то получается, что мы полсекунды тратим только на вызов функции.

*Такое есть, да. И последний у меня вопрос. Вопрос, может быть, немного странно прозвучит. Почему в момент поступления данных нельзя считать все необходимые агрегаты и в необходимом виде их сохранять? Зачем сохранять огромные объемы в какие-то системы типа* *VictoriaMetrics**,* *ClickHouse* *и т. д., чтобы потом очень много времени тратить на них?*

*Приведу пример, чтобы было понятней. Допустим, как работает маленький игрушечный спидометр? Он записывает расстояние, которое вы проехали, все время его добавляя в одну величину, во вторую – время. И делит. И получает среднюю скорость. Можно делать примерно тоже самое. Складывать налету все необходимые факты.*

Хорошо, понял вопрос. Ваш пример имеет место для жизни. Если вы знаете, какие вам нужны агрегаты, то это самая лучшая реализация. Но проблема в том, что люди сохраняют эти метрики, какие-то данные в ClickHouse и они не знают еще, как они в будущем будут их агрегировать, фильтровать, поэтому приходится сохранять все сырые данные. Но если вы знаете, что вам нужно посчитать что-то среднее, то почему бы не посчитать его, чем сохранять там кучу значений? Но это только в том случае, если вы точно знаете, что вам нужно. 

И, кстати, базы для хранения временных рядов поддерживают подсчет агрегата. Например, Prometheus поддерживает recording rules. Т. е. это можно все сделать, если вы знаете, какие вам понадобятся агрегаты. В VictoriaMetrics этого пока нет, но перед ней обычно ставится Prometheus, в котором можно это сделать в recoding rules.

Я, например, работал в … . И там это нужно было для того, чтобы подсчитывать количество событий за последний час, например. Но проблема в том, что пришлось делать кастомную реализацию на Go, т. е. сервис для подсчета этой штуки. И этот сервис был в итоге нетривиальным, потому что это сложно считать. Можно просто считать, если вам нужно считать какие-то агрегаты на определенных интервалах времени. А если вы хотите считать за последнее какое-то время, то это не так просто, как кажется. И я думаю, это не реализовано до сих пор в ClickHouse или в timeseries-базах, потому что сложная реализация.

*И еще один вопрос. Мы сейчас говорили об усреднении, и я вспомнил, что когда-то была такая штука как* *Graphite* *с бэкендом* *Carbon**. И он умел старые данные прореживать, т. е. оставлять одну точку в минуту, одну точку в час и т. д. В принципе, это достаточно удобно, если нам нужны сырые данные, условно говоря, за месяц, а все остальное можно проредить. Но* *Prometheus**,* *VictoriaMetrics* *эту функциональность не поддерживают. Планируется ли поддерживать? Если нет, то почему?*

Спасибо за вопрос. Наши пользователи его периодически задают. Спрашивают, когда мы добавим поддержку downsampling. Тут несколько проблем. Во-первых, каждый пользователь понимает под «downsampling» что-нибудь свое: кто-то хочет получить любую произвольную точку на заданном интервале, кто-то хочет максимальные, минимальные, средние значения. Если в вашу базу пишут данные многие системы, то нельзя их грести под одну гребенку. Может оказаться, что для каждой системы нужно использовать разное прореживание. И это сложно в реализации.

И второе – это то, что VictoriaMetrics как и ClickHouse оптимизирована под работу над большим объемом сырых данных, поэтому она может перелопатить миллиард строчек меньше, чем за секунду, если у вас есть много ядер в вашей системе. Сканирование точек временного ряда в VictoriaMetrics – 50 000 000 точек в секунду на одно ядро. И эта производительность масштабируется на имеющиеся ядра. Т. е. если у вас 20 ядер, например, то получится сканирование миллиарда точек в секунду. И это свойство VictoriaMetrics и ClickHouse уменьшает потребность необходимости в downsamling.

И еще одно свойство в том, что VictoriaMetrics эффективно сжимает эти данные. Сжатие в среднем на production от 0,4 до 0,8 байт на точку. Каждая точка – это timestamp + значение. И она сжимается меньше, чем в один байт в среднем. 

*Сергей. У меня есть вопрос. Какой минимальный квант времени записи?*

Одна миллисекунда. Недавно у нас был разговор с другими разработчиками баз данных для временных рядов. У них минимальный квант времени – это одна секунда. И в Graphite, например, тоже одна секунда. В OpenTSDB тоже одна секунда. В InfluxDB – наносекундная точность. В VictoriaMetrics – одна миллисекунда, потому что в Prometheus одна миллисекунда. И VictoriaMetrics разрабатывалась как remoteStorage для Prometheus первоначально. Но сейчас она может сохранять данные и из других систем. 

И человек, с которым я разговаривал, говорит, что у них секундная точность им этого достаточно, потому что это зависит от типа данных, которые сохраняются в базу временных рядов. Если это DevOps-данные или данные с инфраструктуры, где вы собираете их с интервалом в 30 секунд, в минуту, то там секунды достаточно, меньше уже не надо. А если вы собираете эти данные high …, то там наносекундная точность нужна. 

И миллисекундная точность в VictoriaMetrics подходит и для DevOps-кейса, и может подойти для большинства кейсов, которые я писал. Единственное, для чего она может не подойти – это high … .

*Спасибо! И еще вопрос. Какая совместимость в* *PromQL**?*

Полная обратная совместимость. VictoriaMetrics поддерживает полностью PromQL. Кроме этого она добавляет еще дополнительную расширенную функциональность на PromQL. И по поводу этой расширенной функциональности есть доклад на YouTube. Я рассказывал на Monitoring Meetup весной в Питере. И позавчера рассказывал наш коллега на London Prometheus Meetup. Вы можете посмотреть и почитать документацию про PromQL на нашем GitHub сайте. 

