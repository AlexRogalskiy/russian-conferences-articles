**Здесь переводится видео в статью Ивана Бурмистрова и Александра Казакова из СКБ Контур "Микросервисная архитектура в теории и на практике"**

Ссылка на видеодоклад https://www.youtube.com/watch?v=LDTa2Nx5qrw

**Текст сделан из субтитров. Буду очень благодарен в форматировании текста, его вычитки. Для этого достаточно знать русский язык.
Присылайте свои pull request или присылайте текст на почту patsev.anton[собака]gmail.com**

всем привет меня зовут иван бурмистров и я вместе с коллегами расскажу про опыт из кого контур построение продуктов микро сервисной архитектуре режиссер стоит заметить что мы будем у нас есть опыт в стеке тут нет работающем под управлением windows соответственно в приложении хостится под ее сам это важно знать потому что некоторые советы которые мы будем давать ходе доклада они другом стыке либо неприменимы либо требует существенного изменения будем рассказывать в архитектуре на примере трех сервисов и dio dio dog экстерн и доклад будет построен по принципу от сервиса с самой простой архитектурой мы назвали архитектура размера с до самой сложной архитектуры размера xl суток вообще микро сервис если так погуглить то можно найти очень много разных всяких статей каких-то мыслей кто то говорит что это то же самое что буква с в аббревиатуре су а кто то говорит что это просто некое легковесное приложение решающую ровно одну задачу кто то говорит что это небольшая утилита из не более чем x строк кода ну допустим не более чем 100 строк кода кто то говорит что это что угодно с публичной вопи через конь протокол http например кто то говорит что это компонент системы который досконально работу которого досконально знает хотя бы один разработчик в команде ну компании ну то есть говорят люди что если какой-то компонент перестал помещаться в голову одного человека значит это уже точно не микро сервис надо его делить дробить в пути с кого контур на самом деле определение микро сервиса очень растут с иной стороны размытое другой стороны очень просто мы этим словом называем просто некий отдельный процесс без учета реплик естественно какую-то задачу выделили в отдельный процесс то значит мы выделим my car service никаких дополнительных ограничений по объему строк кода или количеству разрабатывающих там разработчиков и всему подобному это все на усмотрение каждой отдельной команды чего заранее сказать нельзя имеется опыт разработки продуктов в которых 12 микро сервис до нескольких сотен стоит также сказать что если команда встала на путь на курсировать на архитектуры то мы не припомним чтобы она снова слазила и микро сервиса имеет тенденцию к размножению если у вас было 10 то вполне возможно что скоро станет их 20 и но чем больше микро сервисов тем ну сложнее немножко по-другому с ними приходится работать с продукта размера с на примере родилась раннего идей пару слов скажу про что такое идей идя эта система по сути заказа и поставки между поставщиками и торговыми сетями то есть поставщики все эти обмениваются ежедневным кем-то сообщениями которые обеспечивают поставку товара в магазин и то есть если вы там какому-то супермаркете нашли свой любимый йогурт вполне возможно что это благодаря нашей системе но если не нашли там они виноваты скорее когда сервис маленьким и только начинали его делать там конечно была маленькая команда 12 разработчика и простейшая архитектура состоящие из фруктового приложения кого-то базы данных ну фронты объединенные балансировщика mp1 джексон в общем которые нам достался бесплатно в силу того что мы внутри контура этот продукт стартовали то есть внутри команды маноян там настройку компании джинкс они тратили время просто нам достался какие-то клиентские скрипты вот то есть такая просто архитектура было удобно на момент старта проекта потому что практически там каждую неделю можно сказать требования менялись мы его переписывали чуть не с нуля ну или по-крайней мере носили такие изменения которые проходили через всю систему целиком соответственно нам было удобно что приложение такое компактным и его целиком обновляли целиком контролировали да и когда два разработчика в принципе ничего сложного нет кстати одно приложение ну на этом этапе мы страна то что нужно делать дело делать фичи делать добавлять и убирать все равно нужно задумываться некоторых вещах минимальных как минимум понятно должно быть логирование какими стандартным средством нашем случае это лог фанат особенно логирование ошибок если будет что то еще то хорошо но также стоит думать про обновление с кем базы данных от релиза к релизу может измениться схема хранения данных и нужно подумать перед релизом о том совместимы ли старая схема можем ли мы просто взять накатить обновления ничего не сломать или нужно какой-то конвертацию может быть останавливать сервис нужно лучшим и такие вопросы по решать ну и пожалуй только и больше особо не о чем думать не надо ну к счастью сервис развивался менялись требованию добавлялись требованию и конечно же увеличивалась команда увеличилась команда и появлялись новые фичи из вот наша старая архитектура но в какой-то момент например оказалось что для то что пользователи нужны какие-то хитрые фильтры хитрые фильтры скажем с полнотекстовым поиском может быть еще с какими-то хитрыми условиями и оказалось удобно выделить это эту задачу в отдельный процесс который мы в контру любим называть индексами вот так появился отдельный процесс индекс который решал эту задачу выдавал нам эти фильтры имел какие-то документы возвращать и так далее чуть позже появились всякие задачи которые мы называем в игровом тоска me пасха недавнего чем фоновой задачи всякий доставка писем печать pdf еще что-то ну в общем архитектуру еще немножко усложнилось появилась очередь и появились процессы которые делали эти асинхронные задачи письмо отправляли по девке печатали так далее вот немножко цифра то какого размера все-таки был сервис плане командой данных в общем 46 разработчиков где-то 6 терабайт данных 10 разных микро сервисов и где-то 10 физических серверов и физических серверах были развернуты кита виртуалке вот микро сервисы как то на этих virtual как были разложены в этот момент на этом этапе развития появились проблемы которые раньше который мы раньше не сталкивались первая из них это когда уже 10 серверов и 10 микро сервисов все время что-то из них лежит то сервер отвалился то просто сервис упал по какой-то причине в общем раньше такого так часто это не происходило решение достаточно простое просто написали какие-то скрипты которые по scheduler у запускали и проверяли живость серверов просто как процессов это она была достаточно когда какой-то сервер или сервис отвалился приходила разработчику приходило письмо и дальше вручную в ручном режиме можно было разобраться и это не занималась . много времени то есть все таки еще не достаточно будет большая была система такая штука легко хорошо с ним работать вторая проблема это то что обновляться в ручном режиме стало уже невозможно потому что нужно собрать систему здесь эти разные какие-то сервисов разложить на правильные машины ничего не забыть там при копировании не потерять в общем короче хорошо обновляться стало уже невозможно это был страх для всех для разработчиков и для менеджеров для всех решения тоже напрашивается то просто сделать фрезерованный диплом достаточно просто каких-то простых скриптов написать скрипты которые гарантируют что если они доработали начать система обновилась этого тоже с этим тоже вполне можно жить и в тех редких ситуаций когда что-то обновляется не успешно тут тоже вполне можно решать в ручном режиме следующая проблема на северах все время заканчивается свободное место сервисов много они все время пишет каталоге и вместо стал заканчиваться гораздо быстрее чем чем раньше решение точно такое же примитивное просто процесс мониторинга свободного места с письмом ответственным людям и решим решение решением проблемы у следующая проблема даже ликер лемов столько сколько задача это то что у нас появились микро сервисе появилась между ними взаимодействие и какой-то микро сервис должен знать где лежат где находится расположен на площадке другие микро сервисы почему эта проблема потому что локально грубо говоря одна ситуация когда мы выкладываем другая и нам нужно не забыть если добрался какой-то миг у серых не забыть и вот добавить сделать так чтобы остальные про него знали проблема легко решается просто рядом с каждым сервисом хранились список просто файлики файловой системе где написано как где какой микро сервис лежит и единственное что нужно это не забыть при обновлении тщательно проверить что мы никакой файлик не забыли вот только и всего то есть достаточно просто положить файлик и включать голову при обновлениях это тоже вполне себе нормально работает практически без сбоев и и такая последняя задача которая встала это протокол взаимодействие у нас появилась появились сервисы ну с другом взаимодействующие нужно зафиксировать некий протокол и что делать как момент обновление если протокол поменялся если в общем всякие вопросы тут все достаточно просто мы меняемся протокол http и все что мы передаем это некий стерилизованный контракт вот достаточно выбрать какой-то удобный и реализатор удобно и быстро и желательно которым можно спокойно добавлять удалять поля и он при этом бы не ломался не нужно думать о какую-то version ность и или еще чем-то таком как люди сидят рядом они всегда могут договориться как изменить протокол и каком порядке сервисы выложить так чтобы ничего не сломалось то есть это тоже вполне себе хорошо работает фига рамку нибудь си реализатор например пар табу и опять же при обновлениях действием просто действуем аккуратно на следующем уровне приходят становится с другими проблемами расскажет вам всем привет . какие пользоваться меня зовут иван dashkevich я расскажу про систему уже следующего уровня который мы в рамках этого доклада договорились называть системой уровня л это система продукт называется дело док это система юридически значимого электронного документооборота иван обмениваться заказами там юридической значимости нет все друг друга доверяет значит по сложнее ситуация ну так вот вот это скриншот diadora с их пишут одного из экранов системы дипломата diadora если такой казалось бы простой системе которых делает в принципе тоже самое что предыдущие но вот здесь уже 70 не краснеешь каждая строчка это отдельный микро сервис длина это ну тут не видно что они там чёрточками разделить эта реплики есть те которые надо запускать там 20 экземплярах это видимо какая-то печать есть те которых достаточным духа на того кого знает зачем нужен есть какие-то даже в одном экземпляре вот на при этом такой достаточно большой уже крупный product or забота все ищут компактная команда 8 человек ну то есть мы все сидим рядом смотрим другу в глаза она все еще не встают проблемой версии при обновлениях то есть мы можем это решить просто вот путем общения вот на самом деле конечно я прошу расскажу я расскажу про три по моему таких вещи про которые мы забыли забыли их сделать еще на уровне м а надо было сделать их на уровне м и вы мы были вынуждены эти вещи выносить вот на этом уровне и это конечно был существенный геморрой не так первая проблема ну конечно да мы собираем логе мы еще на предыдущем этапе настроили отправку логов с фронтов разработчикам на почту ну либо нам клиент звонит горит у меня проблема а он видит на экране там разработчики уже него занимаются ваши проблемы либо мы просто там допустим сядем почту разгребаем и видим stack trace и конечно все классно просто смотрим спектр с агатом сказано ошибка при вызывает очередного другого микро сервиса идем и видно какого идем на него читаем его логе ищем там где то непонятно как вот эту вот ту самую ошибку тот самый вызов который привел к ошибке о вампире вызывал еще куда-то тема уже достаточно большая и путь прохождения запроса может быть довольно таки длинным и пройти всю эту цепочку вручную это проблема первая проблема понятно не в том что тут надо с множество сервисов собирать в том что нужно блогах каждого сервиса довольно трудно искать ту самую ошибку которую мы сейчас раскапываем решение конечно очевидно почему мы не догадались раньше его сделать это непонятно но это очень просто нужно просто пробрасывать некий идентификатор запроса трассировки идентификатор вы выдумали при входе на пришел запрос пользователя вы думали request эти будет какой нибудь ну и как-то обеспечили его проброс на всех уровнях почти тебе там если там через revit или там допустим потоки какие-то у нас там по граму несет обрабатываю просто надо это делать но и понятно что не забывать логировать вот отсюда на самом деле очевидно вторая проблема ой-ёй-ёй как ты так быстро вот ну понятно что хорошо когда нас есть точка входа а ну еще раз резюме внедряйте качественное логирование как можно раньше на уровне ему же надо было это сделать здесь у нас заняло наверное где-то неделю и еще этот процесс не завершён все еще есть какие-то логе в которых нет request a виде загадочным образом откуда взялись непонятно вот дальше но не все проблемы вызваны именно какими-то ошибками которые мы видим ну например мы раскопали ошибку увидели что проблема из за какого то мусора в данных в виде мага в данных пользователя с таким-то иди есть какая-то фигня кто-то записал ну понятно что теперь чтобы это раскопать а мы все уберу им качественно нам нужно теперь уже по всем вот этим услугам вот этих вот этих вот сервисов за все время собрать записи фильтровать их поэтому юзера иди уже это уже просто не реалистично очевидная вещь конечно который тоже надо было сделать господин of the storm инвертированные это централизованное логирование вот ну здесь такой стандартный стек который общепризнанный фактически ею карла стекло вспышки бана внедрить его в свой продукт на начальном этапе ну либо на этапе среднем это очень просто не займет у вас больше дня я так полагаю ну тут наверное стоит по честному сказать что эта штука у нас еще внедряется в диалоге до сих пор это ее ее сейчас уже почти сделали скоро пойдет в продакшен но сейчас пока у нас некое свое самодельное решение для централизованного логирования которые мы я его не показываю вам что рекламировать его не хочу мы его разбираем но мы его сделали в том числе и была такая мотивация мы тогда внедряли кассандру у себя в продукте и хотели взять какую-нибудь задачу чтобы на ее примере с кассандрой ансамбль разобраться вы выбрали вот это логирования ладно вам этого не говорил дальше следующее тоже достаточно примитивная проблема вот на предыдущем этапе его не рассказывал что они сейчас добавили там мониторинг живости сервисов нам живости недостаточно у нас сервисов очень много реплик очень много нам важно знать не то жив ли сервиса или нет напишем же funcom satrip а вот просто он ведет ли он себя адекватно это гораздо важнее ну что с доклад ну то есть отвечает за разумное время или там допустим случае с диалогом сколько документов в минуту доставляется то есть характерное там допустим цифрам 1000 внезапно упала там до 500 что случилось из хочется видеть во первых графики в реальном времени красивый зарелизили скажем новый сервис там проверки подписи электронных abs стал доставляться гораздо быстрее супер или там наоборот упала то есть если помимо графиков реального времени понятно что хочется видеть еще и иметь возможность встраивать какие-то лимиты если эти элементы были нарушены мы хотим получать какое-то уведомление или push уведомления на телефон эсэмэску письмо о чем что-нибудь такое ну наш опыт это вот такой вот замечательная такая замечательная связка графит grafana сирена это вот на экране скриншот с графами гр фанково графит нас такой график график эта система которая позволяет хранить time series да это то есть условно в него можно за хоть какие-то факты сервис работал за десять миллисекунд понятно и он умеет строить красивые графики прыгнул этого практически общепризнанная вещь для мониторинга таких вещей к нему есть огромное количество разных крутых дашбордов ну вот на используем как раз графа ну потому что в ней очень удобный редактор запросов графиту и используем сирену для того что вселенная она как раз та штука которая умеет не можно настраивать лимит она умеет открывать практически любые уведомления о том что что-то пошло не так вот ну следующая вещь иван рассказывала диплом анти конечно на нашем уровне deployment решать простыми скриптами уже вопрос deployment решать простыми с картами не получается нам недостаточно просто развить какие-то версии сервисов нам хочется в реальном времени уметь добавлять убирать новые реплики смотри что получилось уметь разворачивать новой версии софта по сервис на или если что-то пошло не так уметь откатывать этот назад вот ну понятно что смотреть состояние собственно вот как вот наша штука выглядит боевой площадке это тоже очень прикольная возможность некий скрипты такого не позволят ну мы не нашли никакого подходящего инструмента для нашего стыка который вы нас устроил на самом деле потому что написали свой свой инструмент и видимо когда вы дойдёте до уровня если еще не дошли если уже дошли вы как-то решили для себя эту проблему когда дойдете но видимо вам придется что-то свое писать либо к тому моменту будет уже что-то хорошее известная либо будете решать свою проблему методами уровня excel по которой саши вас скажет дальше вот но мы написали свой инструмент вот скриншот из этого инструмента чё тут такого необычного есть ну то есть он он решает вот то что я перечислил еще несколько других задач ну то что нам было надо например вот эта штука похожа на far это не фар это скриншот из одного из экранов системы тепло его просто пароль и сделали подход подтверждали так ну эта штука может хоть консоль к серверной площадке то есть там можно все таки запускать какие-то утилиты например можно что-то надо копировать можно синхронизировать опять же разливать что-то по разным сервисам но вот это экран и там просто отображает состояние занимать синхронизации или разливка и новых сервисов вот ну помимо вот этих задач наш инструмент тепло и заодно для нас решил ну раз мы его сами пишем можем его наворачивать решил для нас проблему сервис discovery сервис к варе понятно то есть две но снова лесных проблемы первая это синхронизация ну то есть понятно что каждый сервис системе должен иметь одинаковое представление о топологии серверы площадке где там что куда делать вызовы это первое второе понятно полнота это про каждый сервис которого требуется топология она должна быть описано не должны никого забывать вот то есть скрипты да но хочется то иметь какой-то контроль ну вот просто один из вариантов как мы дотащили свой инструмент диплом и оставили все еще те же самые файлики с топология my про который говорил предыдущий докладчик вот просто наша система теперь умеют их синхронизировать следить за тем что они в порядке что на всех серых на всех серверах все хорошо ну в общем вот кроме того если мы разворачиваем новую реплику она может автоматически нам топологии поправить в общем решает вот все вот такие вот задачи ну как вот это вот можно решать по-другому например в контурах стерни как раз системе следующие уровни уровня xl ребята выбрали централизованное хранение настроек и топологии будут они сделали такую штуку как мастер конфиг ну а нам пока хватает своего инструмента тепло и ну вот еще раз все эти решения ну кроме наверное про deployments про него не будет говорить но все что касается трассировки касается логирование то что касается метриках надо просто обязательно надо внедрять еще на предыдущем уровне в системе когда у вас всего там максимум 10 разных микро сервисов это очень просто занят в час у нас на каждый из этих штук но наверное неделя ушла и неделя не fanara недели головной боли поэтому рекомендую об этом не забывать казалось бы очевидно но мы об этом забыли и так я наверное все и передаю слово докладчику который расскажет по системам следующего уровня уровня xl ты ваня просто вверх тормашки на держал штуку поэтому тебе угодно удобно всем привет меня зовут александр казаков я в контуре как раз работаю над продуктом размеры excel условно продукт называется контур экстерн вот немножко цифр у нас порядка 70 разработчиков над этим продуктом трудятся примерно 10 команд это неточные цифры так примерно и вот архитектура сервис ориентирован на и как вы уже поняли вот по состоянию на вчерашний день у нас продакшне более 200 различных сервисов крутится которые запущены в экземплярах более чем полутора тысяч на серверах в общем на этом уровне весь контекст относительно такого большого продукт удержать в одной голове тяжело поэтому разработка ведется ни в одной универсальной команде а в нескольких узкоспециализированных в принципе для каждой команды справедливый все те все те практические рекомендации которых коллеги для меня рассказали их обязательно стоит придерживаться но вот эта картинка и to work структура она сложнее чем рук структура из предыдущих примеров причем осложняют ее не тот факт что тут несколько команд разработки осложняет youtube факт что этим командам разработки приходится взаимодействовать то есть есть сервисы которые разрабатываются одной команды используются другими чтобы нам дальше в терминологии не путаться давайте такие сервисы называть внешними вот само наличие этих внешних сервисов сильно усложняет жизнь потому что ну вообще проблема взаимодействия с внешними сервисами на очень сложная настолько многогранна что я по сути большую часть твоего выступления именно этой проблеме посвящен в условиях когда у нас несколько команд мы имеем людей с разными взглядами на то как должна быть устроена разработка там может быть разная культура разные подходы людям могут нравиться разные инструментов еще у них разные лица релизные циклы в этом в таких условиях очень много приходится тяжко сервисов и начать с того что он может произвольно и время произвольным образом не читает ну потому что других что-то пойдет не так приходится очень много думать о version нас приходится просто делать version is для об этих внешних сервисов причем желательно делать с самого начала самого первого релиза иначе потом будет тяжело и больно ну versions сделается просто надо лишь договориться каких-то правилах задокументировать их придерживаться а документации кстати документация должна быть должна быть качественной с этим моим к этому никаких вопросов нет но что ещё круче надо постараться сделать api таким чтобы разработчик который будет с ним взаимодействовать он мог очень быстро и просто в нем разобраться там считаю документацию по диагонали наверное самый простой путь в этом случае сейчас это делать api соответствующими идеологии rest хороший вопрос кстати стоит ли также тщательно работать над api внутренних сервисов которые внутри команды используются ну мне кажется нет потому что люди которые за соседними столами сидят они очевидно очень просто договорятся о том как изменить офис без всякой version насти еще разберутся как его использовать без всякой документации вообще хороший http api для внешних сервисов это лишь малая самая простая часть того что надо сделать в идеальном случае нужно написать своему сервис очень хорошую клиентскую библиотеку на самом деле это очень очевидная мысль я сейчас объясню почему так считаю вот микро сервисную архитектура она помимо слабой связанности дает еще легкое масштабирование то есть мы каждый отдельный сервис можем поднять не в одном экземпляре в нескольких называются это репликами как обычно код работает то есть мы хотим пообщаться с сервисом идем к одной реплики причем клиент ждет в ч некоторого времени ответ этот это время обычно называется тайм-аутом если за отведенный тайм-аут ответ не получили идем ко второй реплики потом к 3 а что происходит когда одна из реплик тормозит но очевидно если у нас поймала достаточно большой а реплика выбирается произвольным образом то например при тайм-ауте в 30 секунд треть запроса будет работать дольше 30 секунд до это какая-то очень нелогично ситуация там два сервис отвечает за миллисекунды один за 30 секунд с точки зрения пользоваться внешнего продукт работает очень плохо все говорят ну блин 30 секунд это какое-то дурное то и мало давайте использовать разум и этой молод и все будет хорошо но что такое разумный тайм-аут разумный тайм-аут он с одной стороны должен отражать желаемое время отклика с другой стороны какие-то особенности сервер сервиса учитывать потому что если мы не будем учитывать особенности сервера то можно выбрать просто такой тайм-аут и который сервис будет не вступать отвечать amer реальность жизни тут сервис 2 это экстерн series 1 это диалог нас реально так было dedok дело вызову в extern делала тайм-аута 10 секунд ну и там думал что 1 сезон ответ не получил то делает 10 попыток надежде получить все таки как бы это работало замечательно но в часы пиковой нагрузки этот самый сервис externa начинала кричать не за 10 секунд а за 12 что мы в итоге получили приходит пользователь из диадох отправляет запрос в диалог кто-то отправляет запросов extern делает 10 попыток ждет каждый раз по 10 секунд и пользователя через полторы минуты все равно видит ошибку на странице хотя казалось бы через 12 секунд он мог увидеть нормальный ответ да сервис будь тормозил но хотя бы работал с точки зрения пользователя понятно даст им аутами как-то не очевидно что же делать выход из этой ситуации простой надо использовать какую-то хитрую стратегия работы с кластером из хитрую стратегий отправки запросов вот в предыдущем примере было последовательность стратегия так называемая то есть мы когда стали запрос на реплики один один за другим есть более есть другая стратегия можно например отправлять параллельно несколько запросов сразу же дождаться ответа от первой попавшейся реплики отдавать клиенту стратегия нормально работает но в ситуации externa например мы этой стратегии пользоваться не можем потому что если помножить наш сетевой трафик на количество реплик то качество сервиса мы не ухудшим он обороту лучшим душ нас просто сидит всей заткнется мы для себя выбрали следующий подход мы используем стратегию которую назвали адаптивная параллельность сейчас будут несколько слайдов которые помогут понять что ж это такое вот вверху изображена временная шкала ты большое это тот тайм-аут с которым клиент обращается к сервису диадох например общаясь обращается к сервису externa r1 r2 и r3 это реплики как мы поступаем но мы в начальный момент времени отправляем запрос на одной из реплик стрима вот он то большое если ответ получили быстро ok отдаем его клиенту если следующий check point момент времени 1 3 ты большого мы ответ мы получили мы просто шлем запрос на вторую реплику не прерывая 1 там следующий раз действуем аналогичным образом причем в любой момент времени как только мы получили ответ мы его тут же клиенту отдаем почему эта стратегия называется адаптивная параллельность потому что ее можно подтачивать под особенности каждого конкретного сервиса да то есть сервисы разные там более того разные методы в одних и тех же сервисах к ним могут быть применимы разные стратегии вот на самом деле никто кроме человека который сервис написала не понимает в точности как он работает и какую стратегию надо использовать для того чтобы получить добиться ответа от этого сервиса и вот тут собственно оттуда идет этот тезис и написал сервис напиши клиентскую библиотеку не ограничивай сяопин проблема взаимодействия сервиса на она многогранна да как я сказал не есть еще такой например аспект как понять сколько делать попыток при обращении к сервису обычно делается как то так то есть мы выбираем какое то там количество попыток например 3 и говорим вот мы там попробуем пообщаться с сервисом три раза сделаем три попытки почему это плохо можно рассмотреть два крайних случаях крайний случай номер один все три попытки очень быстро за десять миллисекунд отвалятся и мы значит ответ от сервера не получим мы дадим на пользователя ошибку хотя наверное было бы круче если бы мы допустим через секунду не через десять миллисекунд на через секунду отдали бы клиент у кого-нибудь положительный ответ крайний случай номер 2 да там все три попытки будут тормозить по 30 секунд и клиент уже давно отвалился выходом из этой ситуации вот мы нашли вот такой выход из этой ситуации мы клиентов пишем таким образом что программист который общается с нашим сервисом он определяет не количество попыток а говорит какой-то разумный тайм-аут который он готов ждать а клиент уже внутри используя те самые хитрые стратегии отправки запросов как-то распределяет время вот этого тайм-аута и пытается сделать все возможное для того чтобы достучаться до сервиса вот практика показывает что такой подход он реально помогает снизить время отклика сервиса проблема большая многогранная я про нее очень много могу рассказывать но времени к сожалению нет я хотел лишь я хотелось чтобы те люди которые которые строят свои продукты в сервисные ориентированные архитектуре они хотя бы задумались над тем что такая проблема действительно существует и о том что надо не надо думать вот о последнем значит последнюю минуту я расскажу еще вот о чем понятно что несмотря на то что команда разработки несколько продукта один он хостится на одних и тех же серверах и за его хостинг отвечает одна команда оперевшись если каждая команда разработки будет использовать свои инструменты для diplo с инструмента для мониторинга свои подходы для определения ошибок то для службы поддержки это будет просто от да я говорю у нас по полторы тысячи различных экземпляров сервиса вот да у нас был зоопарк реально были очень большие проблемы когда служба поддержки делал реки делала какие-то ошибки и в конечном счете все это вываливалось на пользователь решение очень простое надо просто взять и выделите говорится о том что нас есть отдельная группа которую можно условно назвать devops который будет заниматься вопросами деплоя мониторинга который вот эти решения будет поставлять на ком в команды разработки на этом у нас всё спасибо пожалуйста ваши вопросы говорю о вас есть скажите пожалуйста вот если предположить ситуацию когда у нас существует несколько реплик с кэшем который например берут данные первоначальные из базы какой-то например event log when совсем что угодно но также есть паблик seat скраб и на которой реплики подписываются из исходя из обновлений тоже как бы обновляет свой кэш до актуального состояния вот каким образом вы сталкивались с такой проблемой что при поднятии нового экземпляра реплики необходимо как то что ли читать новые события взять из базы данных инициализирован ее но возможно вот как вы синхронизировали был бы было ли что-то такое не понял попробую предположить то есть есть какое-то место где хранятся события типа event log и мы хотим его читать как-то индексировать уметь отвечать на вопросы типа понятно скажем неправильно на момент запуска например у нас есть состояние этого блога а дома к мы запускаемся приходят новые события вон тока обновляется но мы об этом узнаем только через publish субскрайб то есть мы уже из базы как бы не получим например вот эти события пока мы поднимаемся мы можем пропустить два три события ну в нашей где токи есть подобная штука то есть там есть некоторые неважно где то файловой системе лежит поток событий но в распределенной файловой системе неважно и есть индексы они читают эти события индексировать памяти строит там беркли базу и и умеет отвечать на вопросы сколько там дайте мне вот все документы за такой диапазон времени и как разворачивать новую реплику в этом вопрос ну моем примере так это так построен есть вот болванка некоторые из которой все имеет стартовой ты ее копирующий на другой на другую реплику запускаешь там в болванки написано да куда она типа этот поток событий дочитала вопрос задачи ты вы этого до конца дочитала открывает ешьте теперь начинают отвечать на вопросы всего добрый день скажите пожалуйста как проблему version насти для публичных restful api вы решали вот в аспекте формата джисона xml-документов когда формат менялся поле добавлялось или ударялась там и имя менялась в разных версиях то есть если если новый формат взаимодействия если новый формат протокола он несовместим со старым то как то мы просто меняем версию на новые говорю что старый клиент который рассчитан на старую версию он просто не понимает эту новую версию данных то есть проблема в этом обычно нет выпускается новый клиент который спускается новый клиент который рассчитан на это новую версию данных ему возможность обратной совместимости дата ну не важно для внешних а ты тоже можно придумать такую структуру данных которая будет обратно совместима при изменении то есть если есть такие специальные инструменты который называется например про the buffet через специальный формат который предполагает что структура данных может меняться да и там которые которая сохраняет обратную совместимость я ответил на вопрос или не очень один маленький вопрос про табов понравился ли он вам либо есть альтернатива какие-то но у нас разные стерилизаторы используется для внутренней сервисов ну вот в нашем саваном примере только бинарные в диалог и используются prada буфер исключительно и он прикольный с ним нас никаких проблем нет ну про таков нет вот это вот штуковина у вас там что у нас было место где очень критично к реализации данных поразительно стерилизации на шоколаде собственный сирле завтра бинарных для внутренних коммуникаций не для внешних вот то есть он просто быстрее просто буфа если бы против острове ла то мы бы его взяли конечно так свой написали пришлось ну это одна из большой причин почему иногда переходит бинарным форматами пышный и еще один маленький вопрос он на самом деле большой как вы решаете вопрос интеграционным тестированием например так команда вместе с клиентом предоставляют еще какой-то in memory например реализацию своего сервиса который может другая команда тестировать использовать своих тестах это вариант один вариант 2 имеются тестовые тестовые реализации которые также развернуты на тестовых все горах и соответственно которым можно делать запрос мужа снасти и дядя документах тестируются мы обращаемся к тестовому серверу для дока и делаем свои задачи то есть наши тесты проверяют и тут это интеграционным эту интеграцию но это тесты интеграционное недолгие их мало если быстро и много используется заглушка и оттоком спасибо скажите а как вы относитесь к идее общего разделяя wakodo микро серво всяко выстроить избегать этого или вас это как тренируется ну вот где а доки например это все лежит в одном solution да я не считаю что без вот мы сидим все вместе рядом да вот нас восемь человек не делал в один продукт конечно нам хочется и код разделять и какие то вопросы совместимости мы решим просто глядя друг другу в глаза между командами я хотел добавить просто чтобы стороны там немножко другая практика там общий код мы стараемся взять какие-то отдельные библиотеки какие-то отдельные репозитории в гите вот собственно так мы используем для им год у меня такой вопросов у нас искать маленький сервис он этого защищает предположим пару значений но для этого ему нужно переработать там огромную базу вот и вдруг этот сервис потребовал чтобы эта база поменялось она очень большая вот как вы решаете проблемы миграции с одной структуры на другую вот вообще не сталкивались с таким ну часто сталкиваемся конечно все зависит от размера базы но как правило так что если есть старое представление новое представление выпуском промежуточную версию которую допустим пишут уже в новое читает из обоих и она живет и в это время фоновый процесс перекидывать старые так все данные храним что есть историчности мы можем этот процесс запустить перенос старых данных вот то есть вот такое то что учитываем это при проектировании способа хранения и делаем фоне не останавливает основную систему после последнего просто вот еще такой вопрос много микро сервиса в 70 там плюс еще базы данных наверное всякие разные как вы управляетесь с авторизацией между этими всеми сервисами там логин и пароль и где они хранятся и как прекрасно вопрос все эти сервисы крутя среди являть разумные зоне в нашем случае все спасибо
