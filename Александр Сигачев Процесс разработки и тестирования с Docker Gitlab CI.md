**Предлагаю ознакомиться с рашифровкой доклада Александра Сигачева из Inventos "Процесс разработки и тестирования с Docker + Gitlab CI"**

Этот доклад хорош тем, что структурировано рассказывает о процессе разрабоки и тестировании с использованием Docker и Gitlab CI. Сам доклад 2017 года. Но основы, методология, идея, опыт использования актуально до сих пор. 

Ссылка на видеодоклад https://www.youtube.com/watch?v=lJsqRwULRVA

**Текст сделан из субтитров. Буду очень благодарен в форматировании текста, его вычитки. Для этого достаточно знать русский язык.
Присылайте свои pull request или присылайте текст на почту patsev.anton[собака]gmail.com**

![](https://habrastorage.org/webt/l5/w1/tm/l5w1tmfjs8_-3upcghdkwrcv2ma.png)

Меня зовут Александр Сигачев. Работаю в компании Inventos. Расскажу о своем опыте использования Docker и о том как его понемногу внедряем на проектах в компании.

Тема доклада: Процесс разработки с использованием Docker и Gitlab CI. 

![](https://habrastorage.org/webt/iu/r2/qt/iur2qtjwp2qa_f4vdvmetzii-no.png)

Это второй мой доклад про Docker. На момент первого доклада мы использовали Docker только в Development на машинах разработчиков. Количество сотрудников, которые использовали Docker было около 2-3 человек. Постепенно был накоплен опыт. Мы немного продвинулись дальше. Сылочка здесь приведена на первый наш доклад.

Что будет в этом докладе? Мы поделимся опытом о том какие грабли собрали, какие проблемы как решили. Не везде это было красивого, но позволило двинуться дальше.

Наш девиз: докеризируй все до чего доходят наши руки.

![](https://habrastorage.org/webt/rg/wx/pb/rgwxpblyvmjf0hb0ly4eecmntf0.png)

Какие проблемы решаем?

Когда в компании несколько команд, то программист является жареным ресурсом. Бывают этапы когда программистов выдергивают из одного проекта и дают на какое-то время в другой проект.

Чтобы программист быстро вник ему необходимо скачать исходный код проекта и как можно быстрее запустить среду, которая позволит ему дальаше подвигаться решая задачи данного проекта.

Обычно, если начиная с нуля, то документации проекта ведется мало. Информация о том, как настроить, есть только у старожилов. Самостоятельно сотрудники настраивают свое рабочее место за один-два дня. Чтобы это ускорить мы применили Docker.

Следующая причина - это стандартизации настроек в  Development. По-моему опыту, разработчики всегда проявляют инициативу. Где-то в случае в 1 из 5 каждый волен свой какой-то кастомными домен допустим vasya.dev. Рядом сидит сосед Петя, у которого домен petya.dev. Они разрабатывают сайт или какой-то компонент системы, используя это доменное имя.

Когда система разрастается и эти доменные имена начинают попадать в конфигурации, то возникает конфликт Development окружений и переписывают путь сайта.

То же самое происходит с настройками базы данных. Кто-то не заморачивается безопасностью и работает с пустым паролем root. У кого-то на этапе установки MySQL потребовал пароль и пароль оказался один 123. Часто случось что конфиг базы данных постоянно менялся в зависимости от commit разработчика. Кто-то был поправил, кто-то не поправил конфиг. Были ухищрения, когда мы выносили какой-то тестовый конфиг в .gitignore и каждый разработчик должен был устанавливать базу данных. Это усложняло процесс старта. Нужно кроме всего прочего помнить про базу данных. Базу данных надо инициилизировать, надо прописать пароль, надо прописать пользователя, создать табличку и так далее.

Еще одна из проблем - это разные версии библиотек. Часто бывает что разработчик работает с разными проектами. Есть Legacy проект, который начинали пять лет назад (от 2017 года  - примеч. ред.). В момент старта стартовали с MySQL 5.5. Так и современные проекты. Мы стараемся внедрять уже более современные MySQL, например 5.7 или старше (в 2017 году - примеч. ред.)

Тот кто работает с MySQL знает, что эти библиотеки тянут за собой зависимости. Достаточно проблематично 2 запустить базы вместе. По крайне мере, старые клиенты проблематично подключить к новой базе данных. Это порождает свою очередь несколько проблем.

Следующая проблема - это когда разработчик работает на локальной машине он использует локальные ресурсы, локальные файлы, локальное ОЗУ. Все взаимодействие на момент разработки решения задач выполняется в рамках того что это работает на одной машине.  Пример может служить когда у нас в Production 3 backend-сервера, а разработчик сохраняет файлы в root каталог и оттуда Nginx берет файлы для ответа на запрос. Когда такой код попадает в Production, то получается что файл присутствует на одном из 3 серверов.

Cейчас развивается направление микросервисов. Когда мы большие наши приложения делим на какие-то небольшие компоненты, взаимодействующие между собой. Это позволяет под конкретный Stack задач выбрать технологии. Это позволяет разделить работу и зону ответственности между разработчиками.

Frondend-разработчик разрабатывая на JS практически не влияет на Backend. Backend-разработчик свою очередь разрабатывает в нашем случае Ruby on Rails не мешает по Frondend. Взаимодействие выполняется по при помощи API.

Как бонус, при помощи Docker нам удалось утилизировать ресурсы на Staging. Каждый проект в силу своей специфики требовал определенные настройки. Физические нужно было выделять либо по виртуальному серверу и отдельно их настраивать, либо же делить какое-то переменное окружение и проекты могли в зависимости от версии библиотек влиять друг на друга.

![](https://habrastorage.org/webt/ue/fu/5h/uefu5hbokfkhivcwpffwt5xi4hi.png)

Инструменты. Что мы используем? 

- Непосредственно сам Docker, Dockerfile, в котором описывается зависимости для одного приложения. 
- Docker-compose - это связка, которая объединяет самые несколько наших Docker приложений.

- Gitlab мы используем для хранения исходного кода.

- Gitlab-CI мы используем для системной интеграции.


![](https://habrastorage.org/webt/yq/z-/zb/yqz-zbkwsjbklpjpkxf6pzqasya.png)

Доклад состоит из двух частей.

Первая часть расскажет о том как запускали Docker на машинах разработчиков.

Вторая часть расскажет о том как непосредственно взаимодействует с Gitlab, как запускаем тесты и как мы выкатываем на Staging.

![](https://habrastorage.org/webt/tk/4w/lm/tk4wlm_ul847kzentzlw1ikjypq.png)

Docker - эта технология, которая позволяет используя деклоративный подход описать необходимые компоненты. Это пример Dockerfile. Здесь мы объявляем что мы наследуемся от официального Docker-контейнера Ruby:2.3.0. Он содержит в себе установленый Ruby версии 2.3. Мы устанавливаем необходимые библиотеки сборки и NodeJS. Описываем что создаем каталог app. Назначаем каталог app как рабочей директорией. В этот каталог помещаем необходимый минимальный Gemfile и Gemfile.lock. Затем выполняем сборку непосредственно проектов, которые устанавливают этот образ зависимости. Настраиваем что контейнер будет готов слушать на внешнем порту 3000. Последная команда - это команда, которая непосредственно запускает наше приложение. Если мы выполним команду запуска проекта, то приложие попробует выполнится и запустит последнюю команду в Dockerfile.

![](https://habrastorage.org/webt/yc/vn/mp/ycvnmp4_o5pcl9r9hobxbchmntw.png)

Это минимальными пример docker-compose файла. В данном случае мы показываем что присходит связь двух контейнеров. Это непосредственно в сервис базы данных и сервис web. Наши web-приложения в большинстве случаях требуют качестве backend для хранения данных какую-то базу данных. Так как мы используем MySQL то пример с MySQL - но ничто не мешает использовать какую-то другу базу данных (PostgreSQL, Redis).

Мы берем из официального источника с Docker hub образ MySQL 5.7.14 без изменений. Образ, который отвечает за наше web-приложение мы собираем с текущей директории. Он во время первого запуска собирает нам образ. После чего запускает команду которую мы здесь выполняем. Если мы вернёмся назад, то увидим что была определена команда запуска через Puma. Puma - сервис, написанным на Ruby. Во втором случаем мы переопределяем. Эта команда может быть произвольной в зависимости от наших потребностей или задачи.

Также мы описываем что нужно пробросить порт на нашей хост-машине разработчика с 3000 на 3000 порт контейнера. Это выполняется автоматически при помощи iptables и своего механизма, который непосредственно заложен в Docker. 

Разработчик может также как и ранее обратиться на любой доступный IP-адрес, например 127.0.0.1 локальный или внешний IP-адрес машины.

Последняя строчка говорит что контейнер web зависит от контейнера db. Когда мы вызываем запуск контейнера web предварительно docker-compose запустит нам базу данных. Уже по старту базы данных запустить нам приложение наш backend.

Это позволяет избежать ошибок, когда база данных не поднята и позволяет экономить ресурсы, когда мы останавливаем контейнер баз данных, освобождая этим сам ресурсы под другие проекты. 

![](https://habrastorage.org/webt/s3/r1/9g/s3r19gc5pybevceyecewxzsdyxq.png)

Что нам даёт использование docker-зации базы данных на проекте. Мы у всех разработчиков фиксируем версию MySQL. Это позволяет избежать некоторых ошибок, которые могут возникнуть при расхождении версий, когда меняется какой-то синтаксис, конфигурация, дефолтные настройки. Это позволяет указать hostname для базы данных, login, password. Отходим от того зоопарка имен и конфликтов в config-файлов, которые были ранее. 

Мы имеем возможность использовать более оптимальный config для Development среды, который будет отличаться от дефолтного. MySQL по дефолту настроен на слабые машины. Производительность дефолтного MySQL будет значительно ниже.

![](https://habrastorage.org/webt/b2/qh/uz/b2qhuzt2zgbx5upj9etfnoilrnw.png)

Docker позволяет использовать интерпретатор Python, Ruby, NodeJS нужной версии. Мы избавляемся от необходимости использовать какой-то менеджер версий. Раньше для Ruby  использовали грм-пакет, который позволял менять версию в зависимости от проекта. Также это позволяет благодаря Docker-контейнеру можно плавно мигрировать код и версионировать его вместе зависимостями. У нас не возникает проблемы понять версию как интерпретатора так и кода. Для обновления версии необходимо опустить старый контейнер и поднять новый контейнерю. Если что пошло не так, мы можем опустить новый контейнер, поднять старый контейнер.

Результат сборки контейнера как Development так и в Production будет одинаковый. Это особо актуально для больших инсталляций.

![](https://habrastorage.org/webt/c3/vy/yx/c3vyyxestdms3fuo6vqpst5hhus.png)На Frontend мы используем JavaScipt и NodeJS.

Сейчас последний проект у нас на ReacJS. Разработчик запускал все контейнере.

Далее запускается задача по сборке JavaScipt и код, собранный в статику, отдается через Nginx экономя ресурсы.

![](https://habrastorage.org/webt/6k/8y/ds/6k8yds401e1tmspqtvsnpv9hyq8.png)

Здесь я привел схему нашего последнего проекта.

Какие задачи решали? У нас возника необходимость построить систему, с которой взаимодействует мобильные устройства. Они получают данные. Одна из возможнойстей  послать push-уведомления в данное устройство. 

Чтоы мы для этого сделали?

Мы разделили на приложение такие компоненты как: админская часть на JS, backend, который работает через REST-интерфейс. Backend взаимодействует с базой данных. Результат, который генерируется отдаются клиенту. Админка с backend и базой данных взаимодействует по REST-интерфейсу.

Также у нас была необходимость отправлять Push уведомления. До этого у нас был проект,  в котором было реализован механизм, который отвечает за доставку уведомлений на мобильные платформы. 

Мы разработали такую схему: оператор из браузера взаимодействует с админкой, админка  взаимодействует с backend, ставится задача что надо послать Push уведомления.

Push уведомления взаимодействуют с другим компонентом, который реализован на NodeJS.

Строятся очереди и дальше идет по своему механизму отправка уведомлений.

Здесь нарисованы две базы данных. На текущий момент у нас при помощи Docker мы используем 2 независимые базы данных, которые никак не связаны с собой. Кроме того, что у них это общая виртуальная сеть, физические данные хранятся в разных каталогах.

![](https://habrastorage.org/webt/7l/vd/iz/7lvdizkbaiozesiqkfy6zeu0ila.png)

Тоже самое но в цифрах. Здесь важно переиспользование кода.

Если ранее мы говорили про переиспользование кода в виде библиотек. То в данном примере наш сервис, который отвечает Push уведомления, переиспользуется как полностью сервер. Он предоставляет API. А с ним взаимодействует уже наша новая разработка.

На тот момент мы использовали 4 версию NodeJS. Сейчас (в 2017 году - примеч. ред.) в свежих разработках мы используем 7 версию NodeJS. Нет проблем в новых компонентах привлекать новые версии библиотек. 

При необходимости можно провести рефакторинг и поднять версию NodeJS у сервиса Push уведомлений. 

А если мы сможем сохранить совместимость по API, то можно будет его заменить на других проектах, которые использовались ранее.

![](https://habrastorage.org/webt/wd/xv/4a/wdxv4acjvor1iz_4ay2iaxhwylg.png)

Что надо чтобы добавить Docker? Добавляем в наш репозиторий Dockerfile, который описывает необходимые зависимости. В данном примере компоненты разбиты по логике. Это минимальный набор backend-разработчика.

При создании нового проекта создаем Dockerfile, описываем нужную экосистему (Python, Ruby, NodeJS). В docker-compose описывает необходимую зависимость - базу данных. Описываем что нужна база такой-то версии, хранить данные там там-то.

Мы используем для отдачи статики отдельный третий контейнер. В есть возможность загрузки картинок. Backend кладет заранее подготовили том, а nginx монтирован на этот том для отдачи статики.

Чтобы хранить конфигурацию nginx, mysql мы добавили папку Docker, в которой храним необходимые конфиги, а образы скачиваем из репозитория.

Когда разработчик делает git clone репозитория себе на машину, у него получается уже проект готовый для локальной разработки. Не возникает вопрос какой порт или какие настройки применить.

![](https://habrastorage.org/webt/vx/by/f2/vxbyf2i8sss85npgtavlhvgoe10.png)

Далее у нас есть несколько компонентов: админ, информ-API, push-уведомления.

Для того чтобы это все запустить мы создали еще один репозиторий, которые назвали dockerized-app. На текущий момент мы используем несколько репозиториев до каждого компонента. Они просто логически отличаются - в GitLab это как папка, а на машине разработчика папка под конкретный проект. На уровень ниже лежат компоненты, которые будут объединяться.

![](https://habrastorage.org/webt/jd/ny/wq/jdnywqfo2xha1huuqqmnhb1bvpm.png)

Это пример как раз содержимого dockerized-app. Мы также выносим сюда Docker каталог, в котором наполняем конфигурации, требуемые для взаимодействий всех компонентов. Есть README.md, в котором кратко описано как запускать проект.

Здесь мы применили два docker-compose файла. Это сделано для того чтобы иметь возможность запускать ступенчато. Когда разработчик работать с ядром, ему не нужны Push-уведомления, то он запускает просто docker-compose файл и соответственно ресурс экономится.

Если есть необходимость интеграции с Push-уведомлениями, то запускается docker-compose.yaml и docker-compose-push.yaml.

Так как docker-compose.yaml и docker-compose-push.yaml лежат в папке, то автоматически создается единая виртуальную сеть.

![](https://habrastorage.org/webt/hb/sj/ua/hbsjuao3tzaozzkhtv97lzhinn8.png)

Описание компонентов. Это более расширенный файлик, который отвечает за сбор компонентов. Что здесь примечательно? Здесь мы вводим компонент балансир.

Это готовый Docker-образ, в котором запускается nginx и приложение, которое слушает Docker socket. Динамические, по мере включения и выключения контейнеров, перегенирирует конфиг nginx. Обращение с компонентами мы разносим по доменным именам третьего уровня.

Для Development среды мы используем домен .dev - api.informer.dev. Приложения с доменом .dev допустен  на локальной машине разработчика.

Дальше передаться конфиги до каждого проекта и запускается все проекты вместе одновременно.

![](https://habrastorage.org/webt/ew/i0/_u/ewi0_udkilodmivdjgfjoqt9try.png)

Если графически изобразить, то получается клиент это наш браузер или какой-то инструмент, которым мы осуществляем запросы на баланер.

Балансен по доменному имени определяет к какому контейнеру нужно обратиться.

Это может быть nginx, который отдает JS админки. Это может nginx, который отдает API или же статические файлы, которые отдаются nginx в виде загрузки картинок.

На схеме видно что контейнеры объединены виртуальную сеть и скрыты за прокси.

На машине разработчика можно обратиться к контейнеру зная IP, но мы в принципе это не применяем. Необходимости прямого обращения практически не возникает.

![](https://habrastorage.org/webt/6j/wa/af/6jwaaff9m7e_lwmgvxi88fxxkbi.png)

Какой пример посмотреть чтобы докеризировать свое приложение? На мой взгляд хорошим примером является официальной docker образ для MySQL.

Он достаточно сложной. Там много версий. Но его функционал позволяет покрыть множество потребностей, которые могут возникнуть в процессе дальнейшей разработки. Если вы потратите время и разберетесь как все это взаимодействует, то я думаю в самостоятельном внедрении у вас проблем не возникнет.

В hub.docker.com обычно присутстуют ссылки на github.com, где приведены непосредственно сырые данные, из которых можно самостоятельно собрать образ.

Дальше в этом репозитории присутствует скрипт docker-endpoint.sh, который отвечает за первичную инициализацию и за дальнейшую обработку запуска приложения.

Также в этом примере присутствует возможность конфигурирования при помощи переменных окружения.

Определяя переменное окружение при запуске одиночного контейнера или через docker-compose можно сказать что нам нужно задать пустой пароль для docker на для root на MySQL или либо же какой-то, который мы хотим.

Есть вариант создать рандомный пароль. Мы говорим что нам требуется пользователь, требуется установить пользователю пароль и требуется создать базу данных.

В своих проектах мы немного унифицировали Dockerfile, который отвечает за инициализацию. 

Там мы поправили под свои нужды сделать просто расширение прав пользователя, которую использует приложение.

Это пользволило в дальнейшем просто создать базу данных из консоли приложения. 

В Ruby приложениях есть команда создания, изменения и удаления баз данных.

![](https://habrastorage.org/webt/sj/cn/jy/sjcnjytvgchv17vawenbd66upls.png)

Этот пример того как выглядит конкретная версия MySQL на github.com. Dockerfile можно открыть и посмотреть как там происходит устанавка.

docker-endpoint.sh скрипт отвечающий за точку входа. При первичной инициализации требуются некоторые действия по подготовке и все эти действия вынесены как раз в скрипт инициализации.

![](https://habrastorage.org/webt/7h/z4/hy/7hz4hyunbt1ftm38hf9lnmesv3q.png)

Переходим ко второй части.

Для хранения исходных кодов мы перешли на gitlab. 

Это достаточно мощная система, которая имеет визуальный интерфейс.

Один из компонентов Gitlab это Gitlab CI. Он позволяет описывать последовать команд, которые впоследствии будут использоваться того чтобы организовать систему доставки кода или запуска автоматического тестирования.

Доклад по Gitlab CI 2 https://goo.gl/uohKjI - доклад с Ruby Russia club - достаточно подробный и возможно вас заинтересует.

![](https://habrastorage.org/webt/9f/s8/pk/9fs8pkzbqq4uozrmd2bu4tq0-dg.png)

Cейчас мы с вами рассмотрим что требуется для того чтобы активировать Gitlab CI. Для того чтобы запустить Gitlab CI нам достаточно в корень проекта положить файлик .gitlab-ci.yml.

Здесь мы описываем что мы хотим выполнять последовательность состояний типа теста, деплоя.

Выполняем скрипты, которые вызывают непосредственно docker-compose сборку нашего приложения. Это пример как раз backend.

Далее мы говорим что необходимо прогнать миграции по изменению базы данных и выполнить тесты.

Если скрипты выполняются корректно и не возвращает код ошибки, то соответственно система переходит ко второй стадии деполоя.

Стадия деплоя на текущий момент реализована для staging. Мы не организовывали безпростойный перезапуск.

Мы принудительно гасим все контейнеры, а потом все контейнеры поднимаем заново, собранные на первом этапе при тестировании.

Прогоняем уже для текущего переменного окружения миграции баз данных, которые были написаны разработчиками.

Есть пометка, что применять это только для ветки мастер.

При изменении других веток не выполяется.

Есть возможность организовать выкатки по веткам.

![](https://habrastorage.org/webt/2x/vc/6t/2xvc6tp0zrtrewnpnekatolnipm.png)

Чтобы дальше это организовать, нам нужно установить Gitlab Runner.

Это утилита написана на Golang. Она является единичным файлом как это принято в мире Golang, которые не требуется никаких независимостей.

При запуске мы регистрируем Gitlab Runner.

Получаем в web-интерфейсе Gitlab ключ.

Потом вызываем команду инициилизации в командной строке.

Настраиваем Gitlab Runner в диалоговом режиме (Shell, Docker, VirtualBox, SSH)

Код на Gitlab Runner будет выполнять при каждом коммите в зависимости от настройки .gitlab-ci.yml.

![](https://habrastorage.org/webt/jj/nx/c-/jjnxc-msfkvaey7qs-t6n3kbzww.png)

Как это визуально выглядит в Gitlab в web-интерфейсе. После того как подключили GItlab CI у нас появляется флаг, который показывает в каком состоянии находится билд на текущий момент.

Мы видим что вот 4 минут назад был сделан коммит, который прошел все тесты и проблем не вызвал.

![](https://habrastorage.org/webt/da/vg/3h/davg3h0aqnoewa2xlu3eebuz5jg.png)

Мы можем более подробно посмотреть по билдам. Здесь мы видим что уже прошли два состояния.  Состояние тестирования и состояние деплоя на staging.

Если мы кликнем на конкретный билд, то там будет консольный вывод команд, которые были запущены в процессе согласно .gitlab-ci.yml.

![](https://habrastorage.org/webt/dg/zq/a5/dgzqa5difiuz1yw0e4i99jzgfik.png)

Вот так выглядит история нашего продукта. Мы видим чтобы были удачные попытки. Когда тесты подают, то на следующий шаг не переходит и код на staging не обновляется.

![](https://habrastorage.org/webt/cp/g9/rv/cpg9rvvjttpmkvdmi-ve-ugtoh0.png)

Какие задачи мы решали на staging когда внедрили docker? Наша система состоит из компонентов и у нас возникла необходимость перезапускать, только часть компонентов, которые были обновлены в репозитории, а не всю систему целиком.

Для этого нам пришлось разнести все по отдельным папочкам.

После того как мы это сделали у нас возникла проблема с тем что Docker-compose создает для каждой папочки свое пространство сети не видит компоненты соседа.

Для того чтобы обойти, мы создали сеть в Docker вручную. В Docker-compose прописали что используй для этого проекта такую сеть.

Таким образом каждый компонент, который стартует с этой сеткой видит компоненты в других частях системы.

Следующая проблема - это разделение staging между несколькими проектами.

Так как чтобы все это выглядело красиво и максимально прибрежено к production хорошо использовать 80 либо 443 порт, который использует повсеместно в WEB.

![](https://habrastorage.org/webt/7w/4n/_d/7w4n_dfgts6p7pxijvb6eznq0m0.png)

Как мы решили это? Мы назначили один Gitlab Runner всем крупным проектам.

Gitlab позволяет запустить несколько распределенных  Gitlab Runner, которые будут просто по-очереди в хаотичном порядке брать все задания, прогонять их.

что у нас не возникло хаусом и ограничить группу наших проектов одним runner которую принципе крыльями наших объемах кушай справляются без проблем 

создали написала выше или отдельную сеточка виртуального 

подписали его духе compose для что особенно для как раз у жены в спросе который слушай Docker мы вынесли его в отдельный скрипт запуска и днем прописали сетки в stem проектов 

то есть у нас проект имеет одно сетку а balancelle имеет несколько сеточек по именам проектов и соответственно он уже может по доменным именем боксировать дальше 

соответственно у нас вопрос приходит по домену 85 дашь разводится уже в группу контейнеров которая будет один домен 

![](https://habrastorage.org/webt/az/ke/y8/azkey8kpxp3ruww8fbdc1bnfowk.png)

что еще и из проблем которые составе потом это то что по умолчанию все контейнеры выполняют от рута 

то есть это руб неравные рута ucoz системы 

но однако если при коннекте на этих моделях то это бы туру и файл который мы создаем из эту контейнер получают права рута 

соответствии с разработчик прошел контейнер сделал сам какие-то команды которые порождают файлы вышел из контейнера в свое ро ваша виктория он имеет файл который доступ не имеет 

как можно это решить можно добавить пользователи который будет тут же контейнере 

какие проблемы какие сложности на грунтами когда мы давай пользе

то что с той пользе у нас часто не совпадают они жки они schnucki группы и аниш ники пользователя так называемый ей ней и горения 

чтобы вы решите проблему мы просто говорим что отеля мы будем использовать пользователи с 1000 mah яичников 

в нашем случае это совпал с тем что практически у всех разработчики сайта 

убунта all год 1 пользователям означает песни на идиш если кто ценность 2 пользователя то он сразу проблем которые потребуются на решительно на следующем этапе 

![](https://habrastorage.org/webt/wx/th/35/wxth352n7tza8gkpdfcmtaern3m.png)

какие у вас планы перечитать документацию по доллару проект активно развивается документация меняется и данные которые были получены два-три месяца назад уже потихоньку устаревают 

часть проблем которые мы решали процессе это 2 место в бангалор уже решены стандартными средствами 

так хочется пройти дальше уже перейти непосредственно к регистрации

один из примеров это встроенный в Docker механизм с вами которые непосредственно представляется из коробки и соответственно запустить что-то продакшене на базе видеть технология пробовать точно какой-то опыт фидбэк соответственно 6 которые знания 

и также порождение контейнеров делать между обусловлен 

потому что у нас сейчас налоги и изолированные они радостных протеин и и достучаться до них не проблема но вызывает некий дискомфорт

поэтому одна из задач сделать удобный доступ к очередному интерфейса 

тем более системно текущий момент которые этим занимаются существует тех проблем особых не должно составить

![](https://habrastorage.org/webt/k8/uh/lw/k8uhlwuir8yffyugvdrupropoba.png)

на этом спасибо за внимание

случае тестирование что насчет запуск по расписанию 

то есть например запустить кучу разноцветных истомина выходные

то есть я понял вы сейчас запускали только пропускаем по коллеги нас инициатором к метро является непосредственно событие которое правда нет вам пришел к мид vanilla событие отправилась по расписанию соответствие кто-то должен получить сказать про гонимый находят этих команд 

то есть классической схеме использован допустим про дает деревцами контейнер клона не принято использовать те кто слышал в принципе концепции является что toupper даем это какой-то не процесс не какая-то истинно об стопроцентное но желатель стремится домов чтобы один контейнер кричал на какое-то одно действие

это позволяет далее масштабируется горизонтально 