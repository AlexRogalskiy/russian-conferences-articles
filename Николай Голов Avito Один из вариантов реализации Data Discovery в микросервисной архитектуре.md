**Здесь переводится видео в статью Николая Голова из Avito "Один из вариантов реализации Data Discovery в микросервисной архитектуре"**

Ссылка на видеодоклад https://www.youtube.com/watch?v=yYp6Nqf-SME

**Текст сделан из субтитров. Буду очень благодарен в форматировании текста, его вычитки. Для этого достаточно знать русский язык.
Присылайте свои pull request или присылайте текст на почту patsev.anton[собака]gmail.com**

Всем привет. Я Николай Голов. Расскажу вам про микросервисы, но с другой точки зрения. Авито большая компания. Мы недавно стали вторым classified на планете. Небольшая приписка: "А еще мы монолит пилим".

Я считаю себя разработчиком баз данных (далее просто база). И тут внезапно вопрос: при чем тут микросервисы? Я возглавляю в Авито Data платформу. Мы занимаемся всеми базами, которые используются в Авито: Vertica, PostgreSQL, Redis, MongoDB, Tarantool, VoltDB, самая наша любимая база SQLite. Баз много. У нас сотни сервисов, у нас сотни баз данных. 

В принципе, как должна быть построена микросервисной архитектура с точки зрения баз все неплохо знают. Вот паттерн с которого все обычно начинают. Есть общая база  между сервисами. На слайде оранжевые это сервисы и между ними общая база.

Так жить нельзя, потому что вы не можете изолировано тестировать сервис, когда между ними помимо прямой связи есть еще вот такая связь через базу.

Один сервис запросами может замедлять другой сервис. Это плохо.

С точки зрения работы с базами для микросервисной архитектуры должен применятся паттерн DataBase-per-Service - у каждого сервиса своя база. Если в базе много шардов, то база должна быть общая чтобы они синхронизировались.

Это теория, а в реальности все не так. В реальности когда мы смотрим реальную компанию. В ей используют не только микросервисы, но и монолит. Есть сервисы написанные правильно. Есть старые сервисы, которые до сих пор используют паттерн с общей базой.

Вадим Мэдисон на своей презентации показывал эту картину со связанностью. Только он не показывал без одного компонента и была равномерна сеть. В этой сети в центре есть точка, которая связана с многими точками (микросервисами). Это монолит. Он небольшой на схеме. На самом деле монолит большой. Когда мы говорим про реальную компанию, то вам нужно понимать нюансы сосуществования микросервисный возрождающиеся и уходящий, но по прежнему важной монолитической аналитической архитектуры.

С чего начинается переписывание монолита на микросервисную архитектуру на уровне планирования? Правильно доменное моделирование. Во всех пунктах написано что нужно сделать доменное моделирование. Мы в Авито несколько лет создавали микросервисы без доменного моделирования.

Вопрос: кто в компании может сделал доменное моделирование? Ответ из зала: Никто. Это тоже правильный ответ, но есть люди, которые могут попробовать. У нас в Авито так получилось архитекторы, которые занимаются микросервисами. Но доменное моделирование делал я и разработчики баз данных. Мы имеем представление о полных потоков данных. Эти знания неплохо помогают спроектировать доменную модель.

У Data discovery есть классическая интерпретация - это как работать с данными разбросанными по разным хранилищам чтобы приводить к совокупным выводам и делать какие-нибудь правильные выводы. На самом деле это все маркетинговый bullshit. Эти определения про то как все данные с микросервисов загрузить хранилище. Про это у меня были доклады несколько лет назад. Ничего подобного рассказывать не буду.

Я вам расскажу про другой процесс, который ближе к процессу перехода на микросервисы. Хочу показать способ как вы можете осознать сложность непрерывно эволюционирующей системы с точки зрения данных, с точки зрения микросервисов. В чем суть идеи. Сотни микросервисов сотни баз. Где посмотреть цельную картину сотен сервисов, баз, команд, людей?

Фактически этот вопрос является основной идеей доклада. Чтобы в этой микросервисной архитектуре не умереть, вам нужен digital twin. Ваша компания это совокупность всего что обеспечивает технологическую инфраструктуру. Вам нужно создать адекватный образ всех этих сложностей, на основе которых можно быстро решать задачи. И это не аналитическое хранилище. 

Кстати digital один из главных хайпов. Это уже круче blockchain. Какие задачи мы можем поставить такому digital twin. Все начиналось с data discovery. Вопросы: В каких сервисах хранятся важные данные? В каких не хранятся персональные данные? У вас сотни баз. В каких персональные данные есть? А в каких нет? Как важные данные ходят между сервисом? Например, в сервисе не было персональных данных, а потом он начал слушать шину и они появились. Куда данные копируются когда стираются? Кто с какими данными может работать? Кто может получить доступ напрямую через сервис, кто через базу, кто через шину слышать? Кто через другой сервис может дернуть API ручку (запрос) и себе что-то скачать?

Ответ на эти вопросы практически всегда является граф элементов, граф связей. Этот граф нужно наполнить, актуализировать и поддерживать свежими данными. Мы этот граф решили назвать Persistent Fabric (в переводе помнящая ткань). 

Точки интерфейса. Начнем с пользователей. Это элементы пользовательского взаимодействия с графическим интерфейсом. На одной странице может быть несколько UI points. Это пользовательские ключевые действия, которые пользователь делает.

UI points дергают Endpoint. В русской традиции это называется ручками. Ручки сервисов.

Endpoint дергают сервисы. Сервисов сотни. Сервисы связаны друг с другом. Мы понимаем какой сервис может дернуть сервис. Мы понимаем какой вызов UI points может вызвать какие сервисы по цепочке. Это только начало.

Базы в логическом смысле. База как термин хранилища плохо звучит потому что под этим термином понимается аналитическое что то. Сейчас мы рассматривает базу как storage. Например, redis, postgresql, tarantool. Если сервис использует базу, то обычно использует несколько баз. 

- Для долговременного хранения данных, например PostgreSQL.
- Redis использется как кэш. 
- Tarantool, который может что-то быстро посчитать в потоке данных.

У базы есть развертывание на хосты. Одна база, один redis на самом деле может жить на 16 машинах (мастер кольцо) и еще на 16 живут slave. Это даст понимание того к каким серверам нужно ограничивать доступ чтобы какие-то важные данные не утекли.

В базах хранятся сущности. Примеры сущностей: пользователь, объявление, платеж. Сущности могут храниться в нескольких базах. И тут важно не просто знать что эта сущность там есть. Важно знать что у этой сущности 1 storage является golden source. Golden Source это база где сущность создается и редактируется. Все остальные базы являются функциональными кешами. Важный момент. Если не дай бог у какой-то сущности 2 golden source, то необходимо трудоемкое согласование разделенных источников.

Нужно сущностям лежащими в базе выдать доступ для сервиса, если хотим этом сервис обогатить новой функциональностью. Тогда у нас появляется команды, которые владеют сервисами. Сервис, который не принадлежит командам, это плохой сервис. непонятно где найти отвещяющего за сервис.

Сейчас буду сильно коррелировать с докладом Вадима Мэдисона, потому что он упоминал что в сервисах отражается человек, который туда последним делал commit. Это неплохо как отправная точка для того, чтобы для сервиса найти крайне unit. В долгосрочной перспективе это плохо, потому что человек который последний туда делал commit может уволиться. 

Поэтому нужно знать команды, людей в командах, с их ролями. У нас получился такой простенький граф, где на каждом слое несколько 100 элементов. Знаете ли вы систему где вот это все можно хранить?

Ключевой момент. чтобы этой Persistent Fabric жил, он должна не просто один раз наполнится. Можно всех разработчиков попросить заполнить, но Persistent Fabric должен жить. Так как сервисы создаются, умирают, storage выделяются, перемещаются по серверам, команды создаются, разбиваются, люди переходят в другие команды. Сущности появляются новые, добавляются в новые сервисы, удаляются. Endpoint создаются, регистрируется, пользовательские траектории с точки зрения GUI тоже переделываются.

Cамое важное не то что где-то это технически хранить. Самое важное сделать так чтобы каждый слой Persistent Fabric был свежим и актуальным. Чтобы он актуализировался. 

Предлагаю пройтись по слоям. Проиллюстрирую как делается у нас. Покажу как это можно сделать на уровне отдельных слоев.

Информацию про команды можно взять из организационной структуры 1С. Для заполнения Persistent Fabric не нужно заполнять весь гигантский граф. Нужно чтобы каждый слой правильно заполнялся.

Информацию про людей можно взять из LDAP. Один человек может в разных командах занимать разные роли. Это абсолютно нормально. Сейчас мы хотим сделать новую систему авито people и из нее брать людей, но это не так важно. Самое главное чтобы такие простейшие данные шли, чтобы хотя бы сохраняли ссылки на концы. Чтобы названия команд соответствовали командам из организационной структуры 1С.

Сервисы. Для сервиса нужно получить название и команду, которая им владеет. Источник это service discovery. Service discovery это та система, которую упоминал Вадин Мэдисон под названием Atlas. Atlas - общий реестр сервисов.

Важный нюанс. В Atlas сервисы есть не все. Почти все такие системы типа Atlas хранят информацию про 95% сервисов. 5% сервисов в таких системах отстутвуют, т.к. сервисы старые, созданные без регистрации в Atlas. Когда вы начинаете с этой схемой работать, вы чувствуете, чего вам не хватает.

Storages это обобщенные хранилища. Это может быть postgresql, mongodb, memcache, vertica. Есть несколько источников storage discovery. Для nosql баз используется своя половинка атласа. Для информации о postgresql базах применяется парсинг yaml. Но они хотят сделать свой storage discovery более правильным. И так, storages и информация о том, что сервис использует, ну или владеет (это разные типы) владеет storage. Смотрите, все, что я описал, в принципе, довольно просто.

Что с этим можно делать? Давайте представим, что это граф. Как работать с графом? Добавить его в графовую базу. Например, в neo4j. Это уже примеры реальных запросов и примеры результатов этих запросов. 

Первый сценарий. Нам нужно выдавать права на базу. База должна быть строго в сервисе. Туда должен входить только этот сервис и только члены команды, которая сервисом владеет. Но мы живем в реальном мире. Довольно часто другим командам полезно сходить в базу другого сервиса. Это вопрос: у кого спросить о выдаче прав. Реально большая проблема для сотни баз понять кто там главный. Кто ее создавал, давно уволился или перешёл на другую должность, вообще не помнит, кто с ней работает.

И тут простейший графовый запрос (neo4j). Вам нужен доступ к storage. Вы от storage идете к сервису, который владеет им. Переходите в команду, которая владеет сервисом. Дальше для сервиса вы узнаете, кто у этой команды TechLead. В Авито у продуктовых команд есть технический руководитель и продуктовый руководитель, который не сможет помочь по базам. Это на самом деле только половина запроса. Доступ к storage это не атомарная операция. Чтобы получить доступ к storage нужно получить доступ к серверам, на которых он установлен.

Это довольно интересная отдельная задача. Для этого мы добавляем новую сущность. Это  инсталляция. Здесь терминологическая проблема. Есть storage, например redis база (redis:address). Есть host - это может быть физическая машина, lxc-контейнер, kubernetes. Установка storage на хост мы называем Intance.

В примере показана 4 установки 1 storage (redis) на 3 хоста. Storage для production разумно установить на отдельные физические машины для увеличения performance. Для development среды вам достаточно установить на 1 хост и назначить redis разные порты.

Первый запрос по выдаче прав на базу ушел к руководителю. Руководитель подтвердил что права можно выдать. Второй запрос от storage идет к instance и host. Этот запрос рассматривает все инсталляции для соответствующего окружения. На слайде пример для production окружения. Исходя из этого выдаются уже права на подключение к конкретным хостам, конкретным портам. Это пример запроса по выдаче прав для сотрудника не из команды.

Рассмотрим пример, когда в команду нужно взять нового сотруднику. Новому сотруднику нужно выдать доступ readonly на испытательный срок ко всем сервисам, ко всем storage этой команды. На слайде реальная команда с неполной выборкой. Зеленые круги это руководители команд. Красные это команды. Желтые это сервисы. У ряда жёлтых сервисов есть storage. Синие это storage. Серые это хосты. Фиолетовые это инсталляции storage на хосты. Это пример небольшого unit. Есть много юнитов, у которых сервисов не 7, а 27. Для таких юнитов картинка будет большая. Если использовать есть Persistent Fabric, вы можете сделать запросы в ней и получить ответы списком.

Сущности в Авито это объявления, пользователи, платежи и так далее. Из моих докладах про хранилища данных вы знаете что в Авито этих сущностей сотни. На самом деле все их логировать не обязательно. Откуда можно получить список сущностей? Из аналитического хранилища. Из аналитического хранилища можно выгрузить информацию о том, откуда они эту сущность берут. На первом этапе этого достаточно.

Для каждой сущности необходимо составить список хранилищ где это сущность хранится. Там же указываем что storage же хранит сущность как кэш или storage хранит сущность как golden source - является ее первоисточником.

Когда вы заполняете этот граф, у вас появляется возможность делать запросы к этому графу. У вас есть некоторая сущность и вам нужно понять: в каких сервисах живет сущность, в каких storage, на какие хосты инсталлированна?

При обработке персональных данных нужно уничтожать лог-файлы. Для этого очень важно понять на каких физических машинах могут остаться лог-файлы.

На слайде иллюстрация простого запроса для воображаемой сущности. Количество storage уменьшено чтобы граф влез на слайд. Красные круги это сущности. Синие круги это базы где эта сущность находится. Остальные как на предыдущих слайдах. Серые круги это хосты.  Фиолетовые это инсталляции storage на хосты.

Если вы хотите пройти PCI DSS вам нужно ограничить доступ к определенным сущностям. Для этого вам нужно ограничить доступ к серым кругам. Если нужно имеется ввиду real-time доступ, то закрываем доступ к фиолетовым кругам. 

Эта статическая информация. Когда мы говорим про микросервисную архитектуру самое важное это то что она меняется. Важно иметь не просто иерархическую связь между сущностями, но и одноуровневые связи. Вот пример одноуровневых связей, которую мы неплохо прокачали и используем это связка сервисов. Связка вида сервис вызывает сервис. Тут есть информация о прямых вызовах - сервис вызывает api другово сервиса.

Здесь также  должна быть информация о связи вида: сервис один отправляет в ширу (очередь) события, a service № 2 на это событие подписан. Это похоже на асинхронную медленную связь проходящую через шину. Такая связь важна с точки зрения движения данных. С помощью подобных связей можно проверить работу сервисов, если поменялась версия сервиса, на которые они подписаны.

Есть сущность и мы знаем что она хранится в определенных storage. Если мы рассматриваем задачу поиска точек использования сущности, то очевидный запрос, который у нас возникает это проверка периметра. Storage принадлежат некоторым сервисам. Куда эта сущность может быть утечь (скопирована) из периметра. Она может утечь через вызовы сервисов. Сервис обратился, получил и сохранил у себя пользователя. Она может утечь через шины. Шины могут вас могут быть соединены друг с другом используя RabbitMQ, Londiste. На слайде landis мы еще не подгрузили. А вот вызовы уже подгрузили.

Вот пример реального запроса: объявление, две базы где оно хранится, два сервиса, которые владеют этими базами. После трех колонок идут сервисы, которые работают с сервисами, владеющими этой сущностью. Это потенциальные утечки, которые стоит добавить.

Еndpoints. Вадим упоминал, что для построения реестра еndpoints сервисов можно использовать документацию. Можно сделать запрос в атлас. Эту информацию также можно получить из мониторинга. Если Еndpoint важный, то сами разработчики добавят его в мониторинг.  Если Еndpoint не мониторится, то он нам не нужен.

Из мониторинга можно получить метрики. Привязка метрик к storage, к сервисам, к хостам, к instance (шардам баз) и endpoint. Когда у вас возникает сбой, например endpoint выдает HTTP код 500, то сделать запрос по этому endpoint с целью отследить корни проблемы. От endpoint переходите к сервису, переходите к сервисам, которые этот сервис вызывает, от сервисов идете к storage, от storage идете instance и хостам. Дальше просто на уровне то есть а от всех них то есть найдя вот этот граф вниз вы уже можете взять и получить по ним мониторинге и посмотреть вот именно для этого in the point а вот эта цепочка вниз среди мониторингов не было ли чего-нибудь что вызвало косяк потому что вы понимаете да когда мы говорим про ролями красивую архитектуру там сбой на and panty а он может быть вызван например что там на каком-то серые на каком-то сервере на котором развернуто 1 шард база там чихнула сеть мониторинга это видно но когда у нас структура сервисов и стороны и даже вот такая то проверяйте вспоминать все мониторинге это можно застрелить снави особенно когда они еще новые постами появляются вот помимо этого сценария есть еще очень интересный сценарий что сценарий связанные с тестированием потому что смотрите на самом деле чтобы адекватно тестировать с учетом всех нюансов то есть вадим описывал кейса как можно все микро сервиса тестировать как бы в изоляции но на самом деле чтобы адекватно тестировать вам нужно сервис брать и все те кто нужен ему для работы то есть вам нужно поднять вашим тестовом окружения сервисы сервиса который он вызывает и для них все базы то есть технически вам вашим вашей ткани нужно поднять как бы связанный под графа там вам не все связи нужно этом некоторые можно подчеркнуть но можно нужно поднять под граф и уже этот под граф уже изолирована нагрузочном тесте как полностью такую замкнутую систему это прямо цель вот я сразу скажу тут бы конечно было бы круто крисом я показал как вот у нас граф сущности авито ну да вот это вот на тему все микро сервисной архитектуры был бы круто мне сейчас показать что сейчас граца граф сервисов авито и вот тут изолированные под гриф и которые можно изолированной под графа микро сервисов которые можно поднять независимо тестировать и катись продакшен типа быстро красиво но на самом деле когда вот так мы попытались сделать практически оказалось что любой микро сервис у него вот этот под граф вызовов он на входит и выходит из монолита а монолит он ходит выходит практически везде и то есть как бы это просто иллюстрация того что если микро сервису и пилить не думаю о таких вещах в итоге будет микро сервисной архитектура которая все равно без монолит не работает изолированно и тестирование не позволяет но вот подобное графова и представление позволяет вам найти кандидаты под графа изолированы то есть узнать вот это подмножество сервис оно почти готово для того чтобы работать изолирована нужно вот только парочку вызовов отсечь ну или там вот там как-нибудь из-за изолироваться и уже будет хорошо то есть это позволяет приоритизировать разбил монолита именно чтобы увеличить уменьшить связанность архитектуры вот ну и собственно на последок про то как всю эту штуку можно поддерживать и наполнять потому что я не знаю может кажется что этот все не знаю как это воспринимается снаружи на сложно не сложно просто я про это долго думал с этим долго работал нет кажется довольно простым 10 довольно просто сделать все при соблюдении маленьких правил то есть правило первое что с эта штука нельзя работать заставляя людей заполнять все зависимости понимает структур нужно чтобы каждый источник заполнял только свой маленький кусок информации то есть из одного источника при чем тут возможно доливка и из ручных инструментов воды и запер ручек заполняли сервисы из другого стороже из 3 сервера из 4 in the point и и просто а из отдельных источников заполнялись ссылки что сервис принадлежит команде что человек стал тех рядом команды это первый момент туда просто должна лица от максимально быстро просто то есть в режиме стерли пересолили информации из них отстой внешних источников дальше вот эта информация аграфия она соблюдением историчности должна загружаться в базу тут очень важная историчность потому что понимаете реальной микро cyrus на верхотуру она постоянно дышит и меняется то есть сервис ип и возникают умирают появляются новые базы появляется connection и старый connection отменяют люди ходят между командами и вам довольно часто в контексте того же расследования там какая сущность как ходит вам бывает полезно понять что эта сущность вот она сейчас живет у этого сервиса а она была жила в том сервисе туда за и за ней все ходили туда вот то есть нужно что все связи были историчной как делать граф исторических связей этого там мои дак доклады пранкер modeling отдельная история но историчности статичностью большинству людей для того чтобы работать с данными им историчность не нужно им нужен актуальный срез на сейчас и на основании исторического графу должна уже строится витрина графа на состояние сейчас которую уже можно в принципе можно с ней конечно их подгрести работать но намного проще и поднять во всякие инструменты вот он у нас например него fuji базы поднята еще один сторож которому ее можно использовать для визуализации и туда же через api и ручки можно вот эти запросы которые сказал кидать то есть вот этот запрос простейший пример вот база кто владелец базы графов и запрос ответ вот база доступа к ней какие сервера какие хасты в продакшен окружений это просто запрос в api ответа , опять же вот сервис история просто цепочка метрика от этого сервиса вот да например его стороны и его хранилищ и так далее вот то есть и чем чаще чем больше такой штукой люди используют тем больше у них появляются стимула самим следить за ее актуальностью потому что каждый каждый слой этого ткани его может наполнять отдельная команда то есть ой точки наполняют например front in der и сервисы заполняют backend разработчики стороны заполняют тебе сервера заполняют devops сущности заполняют аналитики то есть все чем-то заняты и они должны чувствовать что их заполнение она живет и приносит пользу вот ну и напоследок пара вещей то есть вот практически то что я вам все показал она уже такое живое там не везде пока 100-процентная заполненность но вот по некоторым вещам у нас шанс продолжается активная работа мы его сейчас хотим у нас сейчас есть информация уже залитая а вызовы сервисом сервисов мы хотим добавить еще информацию о потоке данных через жену вот шотландии стрижек и прочие этом те самые ребята и прочее и последняя вещь которую мы пытаемся добавить час это вот самая горячая вещь это графы пользовательских траекторий то есть связи о и точками на основании того как там пользователя между ними ходят то есть у нас уже сейчас сделано это на ул его не 1 уровня клиентского логирования сейчас вот мы переходим к сшивки этой информацией и пробросов ткань чтобы можно было пользовательский опыт уже оттранслировать в ой точки оттуда в in the point и оттуда в сервисы просто понять но условно говоря чем у нас пользователи честь чаще всего пользуются и почему вот этот маленький сервис который вроде как не очень нагружена не очень важный на самом деле так эффекте с пользовательский опыт так сказать дисклеймер потому что этот сервис вызов его находится на важной пользовательской траектории вот собственно я практически все что хотел рассказал давайте к вопросам так поднимайте ручки потом вставайте и задавайте спасибо за доклад ближе микрофон приводит спасибо большой так вот два маленьких вопроса первый кто наполняет эту ткань то есть это люди которые меняют конфигурацию как вот процесс изменения происходят каждый сунула тут эта иллюстрация она хорошо подсказывает то есть каждый слой наполняется отдельно то есть каждый слой практически наполняется из некоторого а api некоторого источника ну то есть в эту услугу или сервис и вот просто там запрос к атласу охлаждающие сервисы и там есть интерфейс который позволяет то вводить потому что все в атласе не всех баз в 1-ой сильнее всех сотрудников почему-то в вал api dapi нет всех команд вот то есть результат автоматически от штата рука автоматика на участие до часть продлевается руками потому что вот это позволяет понять что у нас вот автоматика то там не все есть а второй вопрос не кажется ли вам что это очень сильно похоже на то что десятками лет делается в semantic web собственно link td это где есть связи собственно семантика и описывается как что с чем связано кажется что это ну смотрите вообще идея графа знания она на самом деле много используется привет поисковым движками прочим ну да это все это все примерно про это вопрос как именно жить и этим как это быстро поднять у себя в конторе то есть это все делается руками без покупки всяких с авто и прочее довольно быстро спасибо за доклад значит у меня вопрос такой вот мы учитываем информацию пройти мы я здесь сейчас ну а вижу да значит мы учитываем информацию про этим и откуда они взялись куда они залились где за кашира волеси тогда в рамках этой концепции как еда рассматривается история о том что этим может быть за кашира ванне весь и какая часть его за кашира но просто предупреждаю я понимаю да что каждый элемент каждая их база она кэширует только фрагмент нужных им полей мы сейчас вот вот этот нюанс не капает потому что вам нафиг не надо ну потому что мы как бы мы слона начинаем есть да чё то тянулись тебя съели если это будет актуально это тоже добавим то есть тут суть этой ткани заключается в том что оно может разрастаться если нужно то есть возможно мы обогатим каналы информация перетекаем их атрибутов просто сейчас это пока излишняя сложность потребностей особо не было спасибо спасибо за доклад скажите а после появления у вас такой визуализации насколько ваша монолиту уменьшился вообще-то помогает уменьшать монолит которые стоится так ну во первых мне монолиту вот это вязать скажем так нет она не от этого не уменьшился вообще просто с ним стало проще жить то есть смотрите кейс про уменьшение монолита про обоснование монолита это вообще находится в другой зоне этим тоже кстати я занимаюсь во многих во многом в рамках доменного моделирования но это происходит не на техническом уровне это происходит на уровне продажи бизнес-идеи бизнесу продажи числовых метрик что распил монолита позволит нам вот здесь заработать больше а здесь терять меньше это вообще в другой зоне но визуализация позволяет а на презентациях пусть бы позволяет испугать какой страшной картинкой бонус еще есть вопрос можно и так так ага да добрый день спасибо за доклад собственно есть такие системы я уже говорил сентябре есть гендерное решение от объёма . например и от beyonce тоже есть пропиши вас перебью 7 тебе мы знаем он а вот информация серверах хостах от железах вот оттуда вот смотрите там есть спирали discovering знаете нам не расслышал есть такая штука как спиральный discovering это зонт автоматически начинают искать папу лай и адресов выделять сервис и находить на них марты и потом ну соответственно discovered вплоть до and user experience если есть интеграции там систему мониторинга и так далее вот ну почему вы решили использовать что-то новое а не используйте что-то старые что уже десятками лет работает и во многих но многие компании используют гендерные решения которые хорошо себя показали то есть есть скажем так сентябре который также отражает графы там все это можно найти можно найти связь от бизнес сервиса его owner а это вообще эта тема называется эти сервис-менеджмент от оттуда пошло того как бы войти у есть отдельная тема как управлять тишка это типа itsm и войти сэм есть отдельная ветка 7 db есть то же самое это ну как бы почему вот это не было взято хорошая старая да что-то новое и эта история ответ на эту историю стоит на это вопрос состоит из двух частей потому что во-первых мы не очень любим гендерное решение мы не любим платить вендорам кроме тех случаев когда не делать что-то прорывное но в данном случае у нас был ряд заходов когда к нам приходили крупные векторы тут например вот последний к нам приходил это оп оп detect динамик от не мой ноут у меня просто на ноуте так классно наклейка от них есть они тоже это делают из свежей компания вот про это же то есть который это делает вот лучше чем вот те ребят что вы описали ну и пройти как бы они теоретически это делают но дальше вон за им практически вопрос вот типа вот наши жизни вот у нас монолит вот у нас тут по хп тут у нас год тут у нас губерний тут все поднимается вот тут у нас свой суп описанный атлас тут у нас базы который развертывается ну типа давайте нет ну то есть короче и понятно чтоб через через пару лет внедрение кучу денег конечно да ну то есть речь идет о том что это очень простая вещь и вам для этого метр на решение и нужно они дали кому-то может нравиться работать вендорами ну смотрите а вот как вы собираете встретить юзер экспириенс они не смотрите мы в бою мы сейчас на уровне логированием и у нас вот во первых все пользовательские действия не лаги руется через шину в нашу аналитику и у нас каждое событие которое пользователь делает мы сейчас его обогатили и там появилась ссылка назад то есть ссылка то есть вот человека вот он делает действие точно же открывает ой там и мы знаем что такое забыть у нее было предыдущие и каждое событие она как бы лакируется ссылкой и мы вот эти события ссылками мы специальным накопителям на потоке мы выстраиваем в траектории это называется к разблокирование пользуетесь которых трое то что у меня на последнем слайде и там мы как раз можем отличить траектории которые частые насыщены а то те которые случайно вот таким образом это дело это уже реализовали или блокирование реализовано сшив к в процессе аналитика частично используется то есть логирование да все уже ссылками назад лакируется причем вступаете в чем проблема тут и ос android the same десктопном avi там все везде есть небольшие специфики потому что это чисто на фронте на отдел чтоб по-честному но это ездил спасибо вот я кстати не принуждая использовать винтер на и решение груш то есть технология 7 т.п. есть конечно магически discovering ну как бы технология старая неважно кто и внедряет vanderley open source конкретный вендор без разницы ну да такое тоже то же самое округа это тоже атлас вы посмотрите все же знают а то снова посмотрели как выглядит атлас у нас после того как мы его доработали то есть возможности подкрутить под себя она хороша всегда идет спасибо в центр сюда теперь вы т.д. спасибо за доклад на наболевшем у вопрос следующий на самом деле понять более факту по наболевшем понятно как идет связь сервисов между сервисами когда у нас идет синхронный запрос по синхронным когда мы пациенту это запрос шину вы пытаетесь тоже идти от сервиса к сервису они по сообщениям которые в его посылаются потому что я наряду со списком да по подпискам все-таки по типу сообщений да у нас тип сообщений четко вот он в кайф терминология кафка он считается как бы топиком и там четко происходит вот сервис вам публикует определенные топики и мы знаем что кто на какой топик подписан все общаешься позволяет если чего кому не надо закрытию возможности выйти на определенный топик чтобы там чем чё попало не слушали спасибо чуть не раскрыто был здрасте геннадий московская биржа спасибо за интересный возврат хайповые слова надеюсь пригодится тема точно актуальное и спасибо за доклад есть пара вопросов 1 вы упоминали в дата фабрики вот в этом своем pierce the фабрики на самом нижнем уровне entity вы их как-то автоматически отслеживается я не раз слышал ни одна предыдущих раз попридержать конечно вы на пирсе стэн фабрики на самом нижнем уровне на 2 entity на ткани на ткани на этого сказать не шутка меня же рассказывали ребята из беды что у них провал проекта дайте побрит был что они перевели как фабрика данных построили фабрику а вообще-то в оригинале имелось дудка не вот и поэтому сегодня получилось поэтому важно это не фабрика у меня департамент как раз фабрика дана понимаю так вот сущности вы как-то отдельно отслеживаете автоматически наполняете или это ручная работа срочности список сущности да и их связи если они есть и либо связи и не следить а ну это вот мои доклады про анкор modeling по х аналитическое хранилище то есть у нас фактически хранилище она автоматически генерируется по описанию сущности связи атрибуты и вот эта информация прямо напрямую льется там хитрец источниками потому что истоки что вот эта сущность она поступает от туда еще оттуда оттуда там часто еще из-за промежуточных буферов вот этого путь как каждая сущность прилетает не всегда допускает автоматический парсинг то есть сущности автоматически пути заливки сущностей полу ручном режиме а вы справляетесь со сложностью когда сущности из одной в другую сильно пересекаю трансформируются там в одной грубо говоря условно таблички лежит целая их пачка разделенные по каким-то признакам то есть со сложными читайте про анкор modeling там такого не бывает то есть мы у нас сущность это бизнес сущность а не то что система там какую-то сущность назвала x там изгибе юзер как кончита к у нас есть пользователь пойти пользователи этот чек из мяса то есть вот он тоже разная система могут его по-разному представляет но это одна сущность но это это это вот про другое завод из другой немножко областью ну окей хорошо спасибо большое спасибо большое николай вот это вам ура и скажите кому из-за давших вопросы книга книга вот кто просил baby рассказывал там было интересно вот он ушел не ушел он та