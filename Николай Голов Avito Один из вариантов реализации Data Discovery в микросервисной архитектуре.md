![](https://habrastorage.org/webt/dx/v7/rl/dxv7rla5yqukjac9fnpiibr4ntg.png)

Здесь переводится видео в статью Николая Голова из Avito "Один из вариантов реализации Data Discovery в микросервисной архитектуре"**

Ссылка на видеодоклад https://www.youtube.com/watch?v=yYp6Nqf-SME

**Текст сделан из субтитров. Буду очень благодарен в форматировании текста, его вычитки. Для этого достаточно знать русский язык.
Присылайте свои pull request или присылайте текст на почту patsev.anton[собака]gmail.com**

![](https://habrastorage.org/webt/vu/or/jd/vuorjdjvigwgooxhgsot_chxeqi.png)

Всем привет. Я Николай Голов. Расскажу вам про микросервисы, но с другой точки зрения. Авито большая компания. Мы недавно стали вторым classified на планете. Небольшая приписка: "А еще мы монолит пилим".

![](https://habrastorage.org/webt/_b/m4/gb/_bm4gbxrpfogi6f97mzwnxuoyyo.png)

Я считаю себя разработчиком баз данных (далее просто база). И тут внезапно вопрос: при чем тут микросервисы? Я возглавляю в Авито Data платформу. Мы занимаемся всеми базами, которые используются в Авито: Vertica, PostgreSQL, Redis, MongoDB, Tarantool, VoltDB. Cамая наша любимая база SQLite. Баз много. У нас сотни сервисов. У нас сотни баз данных. 

![](https://habrastorage.org/webt/gv/nx/yv/gvnxyvfq0nunhm9l7tvgf6yk5lo.png)

В принципе, как должна быть построена микросервисной архитектура с точки зрения баз все неплохо знают. Вот паттерн с которого все обычно начинают. Есть общая база  между сервисами. На слайде оранжевые это сервисы и между ними общая база.

Так жить нельзя, потому что вы не можете изолировано тестировать сервис, когда между ними помимо прямой связи есть еще вот такая связь через базу. Один сервис запросами может замедлять другой сервис. Это плохо.

![](https://habrastorage.org/webt/ky/l6/l2/kyl6l26ccxq-azh8cu2ahqlefig.png)

С точки зрения работы с базами для микросервисной архитектуры должен применятся паттерн DataBase-per-Service - у каждого сервиса своя база. Если в базе много шардов, то база должна быть общая чтобы они синхронизировались.

![](https://habrastorage.org/webt/vk/a1/td/vka1tdxspxdrkwy2rllv3p6drwa.png)

Это теория, а в реальности все не так. В реальности когда мы смотрим реальную компанию. В ей используют не только микросервисы, но и монолит. Есть сервисы написанные правильно. Есть старые сервисы, которые до сих пор используют паттерн с общей базой.

Вадим Мэдисон на своей презентации показывал эту картину со связанностью. Только он не показывал без одного компонента и была равномерна сеть. В этой сети в центре есть точка, которая связана с многими точками (микросервисами). Это монолит. Он небольшой на схеме. На самом деле монолит большой. Когда мы говорим про реальную компанию, то вам нужно понимать нюансы сосуществования микросервисный возрождающиеся и уходящий, но по прежнему важной монолитической аналитической архитектуры.

С чего начинается переписывание монолита на микросервисную архитектуру на уровне планирования? Правильно доменное моделирование. Во всех пунктах написано что нужно сделать доменное моделирование. Мы в Авито несколько лет создавали микросервисы без доменного моделирования.

Вопрос: кто в компании может сделал доменное моделирование? Ответ из зала: Никто. Это тоже правильный ответ, но есть люди, которые могут попробовать. У нас в Авито так получилось архитекторы, которые занимаются микросервисами. Но доменное моделирование делал я и разработчики баз данных. Мы имеем представление о полных потоков данных. Эти знания неплохо помогают спроектировать доменную модель.

![](https://habrastorage.org/webt/g2/71/wr/g271wrphe2gn_lvppcmna3_ec3o.png)

У Data discovery есть классическая интерпретация - это как работать с данными разбросанными по разным хранилищам чтобы приводить к совокупным выводам и делать какие-нибудь правильные выводы. На самом деле это все маркетинговый bullshit. Эти определения про то как все данные с микросервисов загрузить хранилище. Про это у меня были доклады несколько лет назад.

Я вам расскажу про другой процесс, который ближе к процессу перехода на микросервисы. Хочу показать способ как вы можете осознать сложность непрерывно эволюционирующей системы с точки зрения данных, с точки зрения микросервисов. Сотни микросервисов сотни баз. Где посмотреть цельную картину сотен сервисов, баз, команд, людей?

![](https://habrastorage.org/webt/bl/r8/lk/blr8lkehwkfekwub2ndo7yhugg0.png)

Фактически этот вопрос является основной идеей доклада. Чтобы в этой микросервисной архитектуре не умереть, вам нужен digital twin. Ваша компания это совокупность всего что обеспечивает технологическую инфраструктуру. Вам нужно создать адекватный образ всех этих сложностей, на основе которых можно быстро решать задачи. И это не аналитическое хранилище. 

![](https://habrastorage.org/webt/yq/dz/ve/yqdzvefohzr5odnb3oyq2ethzwu.png)

Кстати digital один из главных хайпов. Это уже круче blockchain. Какие задачи мы можем поставить такому digital twin. Все начиналось с data discovery. Вопросы: В каких сервисах хранятся важные данные? В каких не хранятся персональные данные? У вас сотни баз. В каких персональные данные есть? А в каких нет? Как важные данные ходят между сервисом? Например, в сервисе не было персональных данных, а потом он начал слушать шину и они появились. Куда данные копируются когда стираются? Кто с какими данными может работать? Кто может получить доступ напрямую через сервис, кто через базу, кто через шину слышать? Кто через другой сервис может дернуть API ручку (запрос) и себе что-то скачать?

![](https://habrastorage.org/webt/hf/3h/x2/hf3hx2pwjuk6jyarnj6cozb9xww.png)

Ответ на эти вопросы практически всегда является граф элементов, граф связей. Этот граф нужно наполнить, актуализировать и поддерживать свежими данными. Мы этот граф решили назвать Persistent Fabric (в переводе помнящая ткань). 

Точки интерфейса. Начнем с пользователей. Это элементы пользовательского взаимодействия с графическим интерфейсом. На одной странице может быть несколько UI points. Это пользовательские ключевые действия, которые пользователь делает.

UI points дергают Endpoint. В русской традиции это называется ручками. Ручки сервисов.

Endpoint дергают сервисы. Сервисов сотни. Сервисы связаны друг с другом. Мы понимаем какой сервис может дернуть сервис. Мы понимаем какой вызов UI points может вызвать какие сервисы по цепочке. Это только начало.

Базы в логическом смысле. База как термин хранилища плохо звучит потому что под этим термином понимается аналитическое что то. Сейчас мы рассматривает базу как storage. Например, redis, postgresql, tarantool. Если сервис использует базу, то обычно использует несколько баз. 

- Для долговременного хранения данных, например PostgreSQL.
- Redis использется как кэш. 
- Tarantool, который может что-то быстро посчитать в потоке данных.

У базы есть развертывание на хосты. Одна база, один redis на самом деле может жить на 16 машинах (мастер кольцо) и еще на 16 живут slave. Это даст понимание того к каким серверам нужно ограничивать доступ чтобы какие-то важные данные не утекли.

В базах хранятся сущности. Примеры сущностей: пользователь, объявление, платеж. Сущности могут храниться в нескольких базах. И тут важно не просто знать что эта сущность там есть. Важно знать что у этой сущности 1 storage является golden source. Golden Source это база где сущность создается и редактируется. Все остальные базы являются функциональными кешами. Важный момент. Если не дай бог у какой-то сущности 2 golden source, то необходимо трудоемкое согласование разделенных источников.

Нужно сущностям лежащими в базе выдать доступ для сервиса, если хотим этом сервис обогатить новой функциональностью. Тогда у нас появляется команды, которые владеют сервисами. Сервис, который не принадлежит командам, это плохой сервис. непонятно где найти отвещяющего за сервис.

Сейчас буду сильно коррелировать с докладом Вадима Мэдисона, потому что он упоминал что в сервисах отражается человек, который туда последним делал commit. Это неплохо как отправная точка для того, чтобы для сервиса найти крайне unit. В долгосрочной перспективе это плохо, потому что человек который последний туда делал commit может уволиться. 

Поэтому нужно знать команды, людей в командах, с их ролями. У нас получился такой простенький граф, где на каждом слое несколько 100 элементов. Знаете ли вы систему где вот это все можно хранить?

Ключевой момент. чтобы этой Persistent Fabric жил, он должна не просто один раз наполнится. Можно всех разработчиков попросить заполнить, но Persistent Fabric должен жить. Так как сервисы создаются, умирают, storage выделяются, перемещаются по серверам, команды создаются, разбиваются, люди переходят в другие команды. Сущности появляются новые, добавляются в новые сервисы, удаляются. Endpoint создаются, регистрируется, пользовательские траектории с точки зрения GUI тоже переделываются.

Cамое важное не то что где-то это технически хранить. Самое важное сделать так чтобы каждый слой Persistent Fabric был свежим и актуальным. Чтобы он актуализировался. 

Предлагаю пройтись по слоям. Проиллюстрирую как делается у нас. Покажу как это можно сделать на уровне отдельных слоев.

![](https://habrastorage.org/webt/q3/dq/m8/q3dqm8borddqebpb3hipn90-p3u.png)

Информацию про команды можно взять из организационной структуры 1С. Для заполнения Persistent Fabric не нужно заполнять весь гигантский граф. Нужно чтобы каждый слой правильно заполнялся.

![](https://habrastorage.org/webt/z2/61/qc/z261qcwrvjckkahih7resrerido.png)

Информацию про людей можно взять из LDAP. Один человек может в разных командах занимать разные роли. Это абсолютно нормально. Сейчас мы хотим сделать новую систему авито people и из нее брать людей, но это не так важно. Самое главное чтобы такие простейшие данные шли, чтобы хотя бы сохраняли ссылки на концы. Чтобы названия команд соответствовали командам из организационной структуры 1С.

![](https://habrastorage.org/webt/2o/dc/qn/2odcqnp85pbxn4adiss_lilwvy8.png)

Сервисы. Для сервиса нужно получить название и команду, которая им владеет. Источник это service discovery. Service discovery это та система, которую упоминал Вадин Мэдисон под названием Atlas. Atlas - общий реестр сервисов.

Важный нюанс. В Atlas сервисы есть не все. Почти все такие системы типа Atlas хранят информацию про 95% сервисов. 5% сервисов в таких системах отстутвуют, т.к. сервисы старые, созданные без регистрации в Atlas. Когда вы начинаете с этой схемой работать, вы чувствуете, чего вам не хватает.

![](https://habrastorage.org/webt/yb/5t/sk/yb5tskjhylnipret3rbi9syyvry.png)

Storages это обобщенные хранилища. Это может быть postgresql, mongodb, memcache, vertica. Есть несколько источников storage discovery. Для nosql баз используется своя половинка атласа. Для информации о postgresql базах применяется парсинг yaml. Но они хотят сделать свой storage discovery более правильным. 

![](https://habrastorage.org/webt/65/ae/iy/65aeiywykl3mflc-c6mcg9rizj4.png)

Итак, storages и информация о том, что сервис использует, ну или владеет (это разные типы) владеет storage. Смотрите, все, что я описал, в принципе, довольно просто.

![](https://habrastorage.org/webt/fd/o2/54/fdo25465o91fdzbgffgfppzsenu.png)

Что с этим можно делать? Давайте представим, что это граф. Как работать с графом? Добавить его в графовую базу. Например, в neo4j. Это уже примеры реальных запросов и примеры результатов этих запросов. 

Первый сценарий. Нам нужно выдавать права на базу. База должна быть строго в сервисе. Туда должен входить только этот сервис и только члены команды, которая сервисом владеет. Но мы живем в реальном мире. Довольно часто другим командам полезно сходить в базу другого сервиса. Это вопрос: у кого спросить о выдаче прав. Реально большая проблема для сотни баз понять кто там главный. Кто ее создавал, давно уволился или перешёл на другую должность, вообще не помнит, кто с ней работает.

И тут простейший графовый запрос (neo4j). Вам нужен доступ к storage. Вы от storage идете к сервису, который владеет им. Переходите в команду, которая владеет сервисом. Дальше для сервиса вы узнаете, кто у этой команды TechLead. В Авито у продуктовых команд есть технический руководитель и продуктовый руководитель, который не сможет помочь по базам. Это на самом деле только половина запроса. Доступ к storage это не атомарная операция. Чтобы получить доступ к storage нужно получить доступ к серверам, на которых он установлен.

![](https://habrastorage.org/webt/og/ms/f5/ogmsf5wqryeawnzjo5egptkyww8.png)

Это довольно интересная отдельная задача. Для этого мы добавляем новую сущность. Это  инсталляция. Здесь терминологическая проблема. Есть storage, например redis база (redis:address). Есть host - это может быть физическая машина, lxc-контейнер, kubernetes. Установка storage на хост мы называем Intance.

В примере показана 4 установки 1 storage (redis) на 3 хоста. Storage для production разумно установить на отдельные физические машины для увеличения performance. Для development среды вам достаточно установить на 1 хост и назначить redis разные порты.

![](https://habrastorage.org/webt/kq/bh/28/kqbh28i2h6skeusphq86widagie.png)

Вторая часть запроса. Первый запрос по выдаче прав на базу ушел к руководителю. Руководитель подтвердил что права можно выдать. Второй запрос от storage идет к instance и host. Этот запрос рассматривает все инсталляции для соответствующего окружения. На слайде пример для production окружения. Исходя из этого выдаются уже права на подключение к конкретным хостам, конкретным портам. Это пример запроса по выдаче прав для сотрудника не из команды.

![](https://habrastorage.org/webt/3v/mq/27/3vmq27vydcext6rxcir-wp9xv_4.png)

Рассмотрим пример, когда в команду нужно взять нового сотруднику. Новому сотруднику нужно выдать доступ readonly на испытательный срок ко всем сервисам, ко всем storage этой команды. На слайде реальная команда с неполной выборкой. Зеленые круги это руководители команд. Красные это команды. Желтые это сервисы. У ряда жёлтых сервисов есть storage. Синие это storage. Серые это хосты. Фиолетовые это инсталляции storage на хосты. Это пример небольшого unit. Есть много юнитов, у которых сервисов не 7, а 27. Для таких юнитов картинка будет большая. Если использовать есть Persistent Fabric, вы можете сделать запросы в ней и получить ответы списком.

![](https://habrastorage.org/webt/kt/5y/7m/kt5y7mabyvyxpwez0z1bpemntl8.png)

Сущности в Авито это объявления, пользователи, платежи и так далее. Из моих докладах про хранилища данных вы знаете что в Авито этих сущностей сотни. На самом деле все их логировать не обязательно. Откуда можно получить список сущностей? Из аналитического хранилища. Из аналитического хранилища можно выгрузить информацию о том, откуда они эту сущность берут. На первом этапе этого достаточно.

Для каждой сущности необходимо составить список хранилищ где это сущность хранится. Там же указываем что storage же хранит сущность как кэш или storage хранит сущность как golden source - является ее первоисточником.

![](https://habrastorage.org/webt/tb/lm/4a/tblm4aoo5umutn_trjmr85fgbfk.png)

Когда вы заполняете этот граф, у вас появляется возможность делать запросы к этому графу. У вас есть некоторая сущность и вам нужно понять: в каких сервисах живет сущность, в каких storage, на какие хосты инсталлированна?

При обработке персональных данных нужно уничтожать лог-файлы. Для этого очень важно понять на каких физических машинах могут остаться лог-файлы.

![](https://habrastorage.org/webt/cw/a_/s2/cwa_s2ugh3jrxmhmqroa_phruri.png)

На слайде иллюстрация простого запроса для воображаемой сущности. Количество storage уменьшено чтобы граф влез на слайд. Красные круги это сущности. Синие круги это базы где эта сущность находится. Остальные как на предыдущих слайдах. Серые круги это хосты.  Фиолетовые это инсталляции storage на хосты.

Если вы хотите пройти PCI DSS вам нужно ограничить доступ к определенным сущностям. Для этого вам нужно ограничить доступ к серым кругам. Если нужно имеется ввиду real-time доступ, то закрываем доступ к фиолетовым кругам. Эта статическая информация. 

![](https://habrastorage.org/webt/pw/hl/v4/pwhlv48yfmtqfxwxwv2xa1esbrw.png)

Когда мы говорим про микросервисную архитектуру самое важное это то что она меняется. Важно иметь не просто иерархическую связь между сущностями, но и одноуровневые связи. Вот пример одноуровневых связей, которую мы неплохо прокачали и используем это связка сервисов. Связка вида сервис вызывает сервис. Тут есть информация о прямых вызовах - сервис вызывает API другово сервиса.

Здесь также должна быть информация о связи вида: сервис один отправляет в ширу (очередь) события, a service № 2 на это событие подписан. Это похоже на асинхронную медленную связь проходящую через шину. Такая связь важна с точки зрения движения данных. С помощью подобных связей можно проверить работу сервисов, если поменялась версия сервиса, на которые они подписаны.

![](https://habrastorage.org/webt/1p/fa/8a/1pfa8ad2p1bzr1jj8cbjj2qojyc.png)

Есть сущность и мы знаем что она хранится в определенных storage. Если мы рассматриваем задачу поиска точек использования сущности, то очевидный запрос, который у нас возникает это проверка периметра. Storage принадлежат некоторым сервисам. Куда эта сущность может быть утечь (скопирована) из периметра. Она может утечь через вызовы сервисов. Сервис обратился, получил и сохранил у себя пользователя. Она может утечь через шины. Шины могут вас могут быть соединены друг с другом используя RabbitMQ, Londiste. На слайде landis мы еще не подгрузили. А вот вызовы уже подгрузили.

Вот пример реального запроса: объявление, две базы где оно хранится, два сервиса, которые владеют этими базами. После трех колонок идут сервисы, которые работают с сервисами, владеющими этой сущностью. Это потенциальные утечки, которые стоит добавить.

![](https://habrastorage.org/webt/hp/em/hx/hpemhx6tvmyvp0aeti-snjvixri.png)

Еndpoints. Вадим упоминал, что для построения реестра еndpoints сервисов можно использовать документацию. Можно сделать запрос в атлас. Эту информацию также можно получить из мониторинга. Если Еndpoint важный, то сами разработчики добавят его в мониторинг.  Если Еndpoint не мониторится, то он нам не нужен.

![](https://habrastorage.org/webt/zf/sn/bh/zfsnbh8uiaiv3o0sruq8_bmnovw.png)

Из мониторинга можно получить метрики. Привязка метрик к storage, к сервисам, к хостам, к instance (шардам баз) и endpoint. 

![](https://habrastorage.org/webt/__/7y/yp/__7yyperigc964pnzgt29bxhlcm.png)

Когда у вас возникает сбой, например endpoint выдает HTTP код 500, то чтобы отследить корень проблемы необходимо сделать запрос по этому endpoint. От endpoint переходите к сервису, переходите к сервисам, которые этот сервис вызывает, от сервисов идете к storage, от storage идете instance и хостам. 

![](https://habrastorage.org/webt/cc/v7/5o/ccv75osool_2qblerm6cujcg4gw.png)

Дальше найдя этот граф вниз, вы можете взять и получить по ним мониторинги. Вы можете посмотреть для этого endpoint всю цепочку вниз что можно вызвать сбой. В микросервисной архитектуре сбой на endpoint, может быть вызван сбоем сети на каком-то сервере, на котором развернут 1 шард базы. В мониторинге это видно, но при большой структуре сервисов, проверять все сервисы в мониторинге очень трудозатратно.

![](https://habrastorage.org/webt/yt/es/rr/ytesrrfbryu3z-udytw2h-rxwi0.png)

Тестирование. Вадим описывал кейс как можно тестировать микросервисы в изоляции. Но на самом деле чтобы адекватно тестировать, вам нужно тестировать сервис с другими сервисами, которые нужны ему для работы. Вам нужно поднять в вашем тестовом окружении сервисы, которые он вызывает. А для вызываемых сервисов поднять все базы. Получается связанный подграф. В этом графе не все связи нужны. Некоторыми связами можно пренебречь. Этот подграф изолировано нагрузочно тестировать как полностью замкнутую систему.

Изолированные подграфы микросервисов, которые можно поднять независимо, можно тестировать и выкатывать в production. На самом деле, оказалось что подграф вызовов у любого микросервиса входит и выходит из монолита. Монолит он ходит, выходит практически везде. Если разрабатывать микросервисы и не думаю о таких последствиях, то в итоге микросервисная архитектура все равно не будет работать без монолита и изолированное тестирование не позволяет.

Подобное графовое представление позволяет вам найти кандидаты изолированых подграфов.

Это подмножество сервисов почти готово для того чтобы работать изолировано. Необходимо отрефакторить пару вызовов. Это позволяет приоритизировать разбиение монолита в части  уменьшения связанности архитектуры.

![](https://habrastorage.org/webt/4k/lv/hi/4klvhib-3efj5j8ubia7s76u5ie.png)

Поддержка и наполнение. Это довольно просто сделать при соблюдении небольших правил.

правило первое. Нельзя заставлять людей заполнять все зависимости понимает структур. Нужно чтобы каждый источник заполнял только свой маленький кусок информации. Из одного источника при чем тут возможно доливка и из ручных инструментов воды и запер ручек заполняли сервисы. Из второго storage. Из 3 сервера. Из 4 endpoint. Из отдельных источников заполнялись ссылки что сервис принадлежит команде, что человек стал TechLead команды. это первый момент туда просто должна лица от максимально быстро просто то есть в режиме стерли пересолили информации из них отстой внешних источников

дальше вот эта информация о графе она с соблюдением историчности должна загружаться в базу. тут очень важная историчность потому что реальная микросервисная архитектура постоянно дышит и меняется. Сервисы возникают, умирают, появляются новые базы, появляется connection, старый connection отменяют, люди ходят между командами. Вам довольно часто в контексте того же расследования там какая сущность как ходит вам бывает полезно понять что эта сущность вот она сейчас живет у этого сервиса, а она была жила в том сервисе туда за и за ней все ходили туда. то есть нужно что все связи были историчны как делать граф исторических связей этого мои доклады про анкор modeling.

большинству людей для того чтобы работать с данными им историчность не нужна. Им нужен актуальный срез на сейчас. На основании исторического графа должна строится витрина графа на состояние сейчас, которую уже можно в принципе можно с ней конечно их подгрести работать но намного проще и поднять во всякие инструменты вот он у нас например него Neo4j базы поднять, которому ее можно использовать для визуализации и туда же через api и ручки можно вот эти запросы которые сказал кидать. Запрос простейший пример вот база кто владелец базы графов и запрос ответ вот база доступа к ней какие сервера какие хасты в production окружений это просто запрос в api ответа , опять же вот сервис история просто цепочка метрика от этого сервиса вот да например его стороны и его хранилищ.

Чем больше такой штукой люди используют тем больше у них появляются стимул самим следить за ее актуальностью потому что каждый каждый слой этого ткани его может наполнять отдельная команда то есть ой точки наполняют например front in der и сервисы заполняют backend разработчики стороны заполняют тебе сервера заполняют devops сущности заполняют аналитики то есть все чем-то заняты и они должны чувствовать что их заполнение она живет и приносит пользу.

![](https://habrastorage.org/webt/f5/om/mo/f5ommocowd1pnhu0e64r0rpq-ag.png)

вот ну и напоследок пара вещей то есть вот практически то что я вам все показал она уже такое живое там не везде пока 100-процентная заполненность но вот по некоторым вещам у нас шанс продолжается активная работа мы его сейчас хотим у нас сейчас есть информация уже залитая а вызовы сервисом сервисов мы хотим добавить еще информацию о потоке данных через жену вот шотландии стрижек и прочие этом те самые ребята и прочее и последняя вещь которую мы пытаемся добавить час это вот самая горячая вещь это графы пользовательских траекторий то есть связи о и точками на основании того как там пользователя между ними ходят то есть у нас уже сейчас сделано это на ул его не 1 уровня клиентского логирования сейчас вот мы переходим к сшивки этой информацией и пробросов ткань чтобы можно было пользовательский опыт уже оттранслировать в ой точки оттуда в in the point и оттуда в сервисы просто понять но условно говоря чем у нас пользователи честь чаще всего пользуются и почему вот этот маленький сервис который вроде как не очень нагружена не очень важный на самом деле так эффекте с пользовательский опыт так сказать дисклеймер потому что этот сервис вызов его находится на важной пользовательской траектории

![](https://habrastorage.org/webt/5q/p0/ei/5qp0eit7k2xm7c6_-et3wfquclc.png)

Это все, что я хотел вам рассказать. Давайте перейдем к вопросам.

Вопрос: Кто наполняет эту ткань? Ее заполняют люди, которые меняют конфигурацию? Как происходит процесс изменения? 

Ответ: Каждый слой наполняется отдельно. Каждый слой наполняется из API некоторого источника. Выполняется запрос к атласу с целью получения списка всех сервисов. Атлас выдает список всех сервисов. Есть интерфейс, который позволяет дополнительно вводить сервисы. В атласе не всех баз. В 1С нет всех сотрудников. В LDAP нет всех команд. Часть данных получается автоматически, недостающая информация добавляется руками.

Вопрос: Не кажется ли вам что это похоже на то что десятками лет делается в semantic web? Например, link data. В link data есть связи, семантика. В link data описывается как что с чем связано.

Ответ: Идея графа знаний много где используется. Например, в поисковых движках. Это все примерно про это. Вопрос как именно жить с этим, как это быстро поднять у себя в конторе. Это все делается руками без покупки стороннего программного обеспечения.

Вопрос: Мы учитываем информацию про item, откуда они взялись, куда залились (где они создались), где закешировались. В рамках этой концепции как-нибудь рассматривается история о том что item может быть закеширован не весь? И какая часть его закеширована? 

Ответ: Я понимаю. Каждый элемент, каждая их база она кэширует только фрагмент нужных им полей. Мы сейчас вот этот нюанс не копаем.

Вопрос: потому что вам не надо?

Ответ: потому что мы как бы мы слона начинаем есть да чё то тянулись тебя съели. Если это будет актуально, это тоже добавим. Cуть этой ткани заключается в том, что она может разрастаться если нужно. Возможно мы обогатим каналы информацией о перетекаемых атрибутах. Сейчас это пока излишняя сложность. Потребностей особо не было.

Вопрос: После появления у вас такой визуализации насколько ваш монолит уменьшился? Помогает ли это уменьшать монолит?

Ответ: Нет, он от этого не уменьшился. Просто с ним стало проще жить. Кейс про уменьшение монолита, про обоснование монолита - это вообще находится в другой зоне. Этим тоже я занимаюсь во многом в рамках доменного моделирования. Это происходит не на техническом уровне. Это происходит на уровне продажи идеи бизнесу, продажи числовых метрик, в которых видно что распил монолита позволит нам вот здесь заработать больше, а здесь терять меньше. 

Вопрос: Есть такие системы SMDB. Есть вендерные решения от IBM, HP, BMC.

Ответ: CMDB мы знаем. Вот информация серверах, хостах, железах из CMDB.

Вопрос: В CMDB есть Spiral discovering. Spiral discovering начинает искать по address pool сервисы и выделять их, находить на них порты. Spiral discovering выполняет поиск вплоть до user experience, если есть интеграция в систему мониторинга. Почему вы решили использовать что-то новое, а не используйте что-то старое что уже десятками лет работает? Многие компании используют вендерные решения, которые хорошо себя показали. CMDB отображает графы. В этих графах можно найти связь от бизнес сервиса до его owner. Эта тема называется ITSM (IT Service Management, управление ИТ-услугами). В ITIL есть отдельная тема как управлять IT. Это ITSM. В ITSM есть отдельная ветка CMDB.

Ответ: ответ на этот вопрос состоит из двух частей. Во-первых, мы не очень любим вендерные решения. Мы не любим платить вендорам, кроме тех случаев, когда они делают что-то прорывное. В данном случае у нас был ряд заходов. К нам приходили крупные векторы. Последний кто к нам приходил это app dynamics. Они тоже это делают. Эта компания делает лучше, чем те компании что вы описали. Мы задаем практический вопрос: у нас монолит, у нас тут PHP, тут у нас Golang, тут у нас Kubernetes, тут у нас свой самописанный атлас. понятно что через пару лет внедрения и кучу денег конечно да. Речь идет о том, что это простая вещь и вам для этого вендорное решение не нужно.

Вопрос: Как вы собираете делать tracing user experience?

Ответ: Все пользовательские действия логируются через шину в нашу аналитику. Каждое событие, которое пользователь делает мы сейчас его обогатили и там появилась ссылка назад. Человека делает действие точно же открывает ой там и мы знаем что такое забыть у нее было предыдущие. 

каждое событие логируются ссылкой и мы вот эти события ссылками мы специальным накопителям на потоке мы выстраиваем в траектории. это называется к разблокирование пользуетесь которых трое то что у меня на последнем слайде и там мы как раз можем отличить траектории которые частые насыщены а то те которые случайно.

Вопрос: это уже реализовали или в процессе?

Ответ: логируются реализовано сшив к в процессе аналитика частично используется. то есть логирование да все уже ссылками назад лакируется причем вступаете в чем проблема тут и ос android the same десктопном avi там все везде есть небольшие специфики потому что это чисто на фронте на отдел чтоб по-честному но это ездил спасибо вот

я кстати не принуждая использовать винтер на и решение груш то есть технология 7 т.п. есть конечно магически discovering ну как бы технология старая неважно кто и внедряет vanderley open source конкретный вендор без разницы ну да такое тоже то же самое округа это тоже атлас вы посмотрите все же знают а то снова посмотрели как выглядит атлас у нас после того как мы его доработали то есть возможности подкрутить под себя она хороша всегда идет спасибо в центр сюда теперь вы т.д. 

спасибо за доклад на наболевшем у вопрос следующий на самом деле понять более факту по наболевшем понятно как идет связь сервисов между сервисами когда у нас идет синхронный запрос по синхронным когда мы пациенту это запрос шину вы пытаетесь тоже идти от сервиса к сервису они по сообщениям которые в его посылаются потому что я наряду со списком да по подпискам все-таки по типу сообщений да у нас тип сообщений четко вот он в кайф терминология кафка он считается как бы топиком и там четко происходит вот сервис вам публикует определенные топики и мы знаем что кто на какой топик подписан все общаешься позволяет если чего кому не надо закрытию возможности выйти на определенный топик чтобы там чем чё попало не слушали спасибо чуть не раскрыто был 

здрасте геннадий московская биржа спасибо за интересный возврат хайповые слова надеюсь пригодится тема точно актуальное и спасибо за доклад есть пара вопросов 1 вы упоминали в дата фабрики вот в этом своем pierce the фабрики на самом нижнем уровне entity вы их как-то автоматически отслеживается я не раз слышал ни одна предыдущих раз попридержать конечно вы на пирсе стэн фабрики на самом нижнем уровне на 2 entity на ткани на ткани на этого сказать не шутка меня же рассказывали ребята из беды что у них провал проекта дайте побрит был что они перевели как фабрика данных построили фабрику а вообще-то в оригинале имелось дудка не вот и поэтому сегодня получилось поэтому важно это не фабрика у меня департамент как раз фабрика дана понимаю так вот сущности вы как-то отдельно отслеживаете автоматически наполняете или это ручная работа срочности список сущности да и их связи если они есть и либо связи и не следить а ну это вот мои доклады про анкор modeling по х аналитическое хранилище то есть у нас фактически хранилище она автоматически генерируется по описанию сущности связи атрибуты и вот эта информация прямо напрямую льется там хитрец источниками потому что истоки что вот эта сущность она поступает от туда еще оттуда оттуда там часто еще из-за промежуточных буферов вот этого путь как каждая сущность прилетает не всегда допускает автоматический парсинг то есть сущности автоматически пути заливки сущностей полу ручном режиме а вы справляетесь со сложностью когда сущности из одной в другую сильно пересекаю трансформируются там в одной грубо говоря условно таблички лежит целая их пачка разделенные по каким-то признакам то есть со сложными читайте про анкор modeling там такого не бывает то есть мы у нас сущность это бизнес сущность а не то что система там какую-то сущность назвала x там изгибе юзер как кончита к у нас есть пользователь пойти пользователи этот чек из мяса то есть вот он тоже разная система могут его по-разному представляет но это одна сущность но это это это вот про другое завод из другой немножко областью ну окей хорошо спасибо большое спасибо большое николай вот это вам ура и скажите кому из-за давших вопросы книга книга вот кто просил baby рассказывал там было интересно вот он ушел не ушел он та