**Здесь переводится видео в статью Николая Голова из Avito "Один из вариантов реализации Data Discovery в микросервисной архитектуре"**

Ссылка на видеодоклад https://www.youtube.com/watch?v=yYp6Nqf-SME

**Текст сделан из субтитров. Буду очень благодарен в форматировании текста, его вычитки. Для этого достаточно знать русский язык.
Присылайте свои pull request или присылайте текст на почту patsev.anton[собака]gmail.com**

Всем привет. Я Николай Голов. Расскажу вам про микросервисы, но с другой точки зрения. Авито большая компания. Мы недавно стали вторым classified на планете. Небольшая приписка: "А еще мы монолит пилим".

Я считаю себя разработчиком баз данных (далее просто база). И тут внезапно вопрос: при чем тут микросервисы? Я возглавляю в Авито Data платформу. Мы занимаемся всеми базами, которые используются в Авито: Vertica, PostgreSQL, Redis, MongoDB, Tarantool, VoltDB, самая наша любимая база SQLite. Баз много. У нас сотни сервисов, у нас сотни баз данных. 

В принципе, как должна быть построена микросервисной архитектура с точки зрения баз все неплохо знают. Вот паттерн с которого все обычно начинают. Есть общая база  между сервисами. На слайде оранжевые это сервисы и между ними общая база.

Так жить нельзя, потому что вы не можете изолировано тестировать сервис, когда между ними помимо прямой связи есть еще вот такая связь через базу.

Один сервис запросами может замедлять другой сервис. Это плохо.

С точки зрения работы с базами для микросервисной архитектуры должен применятся паттерн DataBase-per-Service - у каждого сервиса своя база. Если в базе много шардов, то база должна быть общая чтобы они синхронизировались.

Это теория, а в реальности все не так. В реальности когда мы смотрим реальную компанию. В ей используют не только микросервисы, но и монолит. Есть сервисы написанные правильно. Есть старые сервисы, которые до сих пор используют паттерн с общей базой.

Вадим Мэдисон на своей презентации показывал эту картину со связанностью. Только он не показывал без одного компонента и была равномерна сеть. В этой сети в центре есть точка, которая связана с многими точками (микросервисами). Это монолит. Он небольшой на схеме. На самом деле монолит большой. Когда мы говорим про реальную компанию, то вам нужно понимать нюансы сосуществования микросервисный возрождающиеся и уходящий, но по прежнему важной монолитической аналитической архитектуры.

С чего начинается переписывание монолита на микросервисную архитектуру на уровне планирования? Правильно доменное моделирование. Во всех пунктах написано что нужно сделать доменное моделирование. Мы в Авито несколько лет создавали микросервисы без доменного моделирования.

Вопрос: кто в компании может сделал доменное моделирование? Ответ из зала: Никто. Это тоже правильный ответ, но есть люди, которые могут попробовать. У нас в Авито так получилось архитекторы, которые занимаются микросервисами. Но доменное моделирование делал я и разработчики баз данных. Мы имеем представление о полных потоков данных. Эти знания неплохо помогают спроектировать доменную модель.

У Data discovery есть классическая интерпретация - это как работать с данными разбросанными по разным хранилищам чтобы приводить к совокупным выводам и делать какие-нибудь правильные выводы. На самом деле это все маркетинговый bullshit. Эти определения про то как все данные с микросервисов загрузить хранилище. Про это у меня были доклады несколько лет назад. Ничего подобного рассказывать не буду.

Я вам расскажу про другой процесс, который ближе к процессу перехода на микросервисы. Хочу показать способ как вы можете осознать сложность непрерывно эволюционирующей системы с точки зрения данных, с точки зрения микросервисов. В чем суть идеи. Сотни микросервисов сотни баз. Где посмотреть цельную картину сотен сервисов, баз, команд, людей?

Фактически этот вопрос является основной идеей доклада. Чтобы в этой микросервисной архитектуре не умереть, вам нужен digital twin. Ваша компания это совокупность всего что обеспечивает технологическую инфраструктуру. Вам нужно создать адекватный образ всех этих сложностей, на основе которых можно быстро решать задачи. И это не аналитическое хранилище. 

Кстати digital один из главных хайпов. Это уже круче blockchain. Какие задачи мы можем поставить такому digital twin. Все начиналось с data discovery. Вопросы: В каких сервисах хранятся важные данные? В каких не хранятся персональные данные? У вас сотни баз. В каких персональные данные есть? А в каких нет? Как важные данные ходят между сервисом? Например, в сервисе не было персональных данных, а потом он начал слушать шину и они появились. Куда данные копируются когда стираются? Кто с какими данными может работать? Кто может получить доступ напрямую через сервис, кто через базу, кто через шину слышать? Кто через другой сервис может дернуть API ручку (запрос) и себе что-то скачать?

Ответ на эти вопросы практически всегда является граф элементов, граф связей. Этот граф нужно наполнить, актуализировать и поддерживать свежими данными. Мы этот граф решили назвать Persistent Fabric (в переводе помнящая ткань). 

Точки интерфейса. Начнем с пользователей. Это элементы пользовательского взаимодействия с графическим интерфейсом. На одной странице может быть несколько UI points. Это пользовательские ключевые действия, которые пользователь делает.

UI points дергают Endpoint. В русской традиции это называется ручками. Ручки сервисов.

Endpoint дергают сервисы. Сервисов сотни. Сервисы связаны друг с другом. Мы понимаем какой сервис может дернуть сервис. Мы понимаем какой вызов UI points может вызвать какие сервисы по цепочке. Это только начало.

Базы в логическом смысле. База как термин хранилища плохо звучит потому что под этим термином понимается аналитическое что то. Сейчас мы рассматривает базу как storage. Например, redis, postgresql, tarantool. Если сервис использует базу, то обычно использует несколько баз. 

- Для долговременного хранения данных, например PostgreSQL.
- Redis использется как кэш. 
- Tarantool, который может что-то быстро посчитать в потоке данных.

У базы есть развертывание на хосты. Одна база, один redis на самом деле может жить на 16 машинах (мастер кольцо) и еще на 16 живут slave. Это даст понимание того к каким серверам нужно ограничивать доступ чтобы какие-то важные данные не утекли.

В базах хранятся сущности. Примеры сущностей: пользователь, объявление, платеж. Сущности могут храниться в нескольких базах. И тут важно не просто знать что эта сущность там есть. Важно знать что у этой сущности 1 storage является golden source. Golden Source это база где сущность создается и редактируется. Все остальные базы являются функциональными кешами. Важный момент. Если не дай бог у какой-то сущности 2 golden source, то необходимо трудоемкое согласование разделенных источников.

Нужно сущностям лежащими в базе выдать доступ для сервиса, если хотим этом сервис обогатить новой функциональностью. Тогда у нас появляется команды, которые владеют сервисами. Сервис, который не принадлежит командам, это плохой сервис. непонятно где найти отвещяющего за сервис.

Сейчас буду сильно коррелировать с докладом Вадима Мэдисона, потому что он упоминал что в сервисах отражается человек, который туда последним делал commit. Это неплохо как отправная точка для того, чтобы для сервиса найти крайне unit. В долгосрочной перспективе это плохо, потому что человек который последний туда делал commit может уволиться. 

Поэтому нужно знать команды, людей в командах, с их ролями. У нас получился такой простенький граф, где на каждом слое несколько 100 элементов. Знаете ли вы систему где вот это все можно хранить?

Ключевой момент. чтобы этой Persistent Fabric жил, он должна не просто один раз наполнится. Можно всех разработчиков попросить заполнить, но Persistent Fabric должен жить. Так как сервисы создаются, умирают, storage выделяются, перемещаются по серверам, команды создаются, разбиваются, люди переходят в другие команды. Сущности появляются новые, добавляются в новые сервисы, удаляются. Endpoint создаются, регистрируется, пользовательские траектории с точки зрения GUI тоже переделываются.

Cамое важное не то что где-то это технически хранить. Самое важное сделать так чтобы каждый слой Persistent Fabric был свежим и актуальным. Чтобы он актуализировался. 

Предлагаю пройтись по слоям. Проиллюстрирую как делается у нас. Покажу как это можно сделать на уровне отдельных слоев.

Информацию про команды можно взять из организационной структуры 1С. Для заполнения Persistent Fabric не нужно заполнять весь гигантский граф. Нужно чтобы каждый слой правильно заполнялся.

Информацию про людей можно взять из LDAP. Один человек может в разных командах занимать разные роли. Это абсолютно нормально. Сейчас мы хотим сделать новую систему авито people и из нее брать людей, но это не так важно. Самое главное чтобы такие простейшие данные шли, чтобы хотя бы сохраняли ссылки на концы. Чтобы названия команд соответствовали командам из организационной структуры 1С.

Сервисы. Для сервиса нужно получить название и команду, которая им владеет. Источник это service discovery. Service discovery это та система, которую упоминал Вадин Мэдисон под названием Atlas. Atlas - общий реестр сервисов.

Важный нюанс. В Atlas сервисы есть не все. Почти все такие системы типа Atlas хранят информацию про 95% сервисов. 5% сервисов в таких системах отстутвуют, т.к. сервисы старые, созданные без регистрации в Atlas. Когда вы начинаете с этой схемой работать, вы чувствуете, чего вам не хватает.

Storages это обобщенные хранилища. Это может быть postgresql, mongodb, memcache, vertica. Есть несколько источников storage discovery. Для nosql баз используется своя половинка атласа. Для информации о postgresql базах применяется парсинг yaml. Но они хотят сделать свой storage discovery более правильным. И так, storages и информация о том, что сервис использует, ну или владеет (это разные типы) владеет storage. Смотрите, все, что я описал, в принципе, довольно просто.

Что с этим можно делать? Давайте представим, что это граф. Как работать с графом? Добавить его в графовую базу. Например, в neo4j. Это уже примеры реальных запросов и примеры результатов этих запросов. 

Первый сценарий. Нам нужно выдавать права на базу. База должна быть строго в сервисе. Туда должен входить только этот сервис и только члены команды, которая сервисом владеет. Но мы живем в реальном мире. Довольно часто другим командам полезно сходить в базу другого сервиса. Это вопрос: у кого спросить о выдаче прав. Реально большая проблема для сотни баз понять кто там главный. Кто ее создавал, давно уволился или перешёл на другую должность, вообще не помнит, кто с ней работает.

И тут простейший графовый запрос (neo4j). Вам нужен доступ к storage. Вы от storage идете к сервису, который владеет им. Переходите в команду, которая владеет сервисом. Дальше для сервиса вы узнаете, кто у этой команды TechLead. В Авито у продуктовых команд есть технический руководитель и продуктовый руководитель, который не сможет помочь по базам. Это на самом деле только половина запроса. Доступ к storage это не атомарная операция. Чтобы получить доступ к storage нужно получить доступ к серверам, на которых он установлен.

Это довольно интересная отдельная задача. Для этого мы добавляем новую сущность. Это  инсталляция. Здесь терминологическая проблема. Есть storage, например redis база (redis:address). Есть host - это может быть физическая машина, lxc-контейнер, kuberntes. найти быть host card виртуальные так далее и установка 100 раджа на хост мы называем это инстанциям то есть условно говоря о 1 100 раджива 1 редиса он может быть установлен на у него может быть 4 установки но 3-х 100 вот как в примере показано потому что смотрите когда вы делаете установку сто раджа для продакшен нагрузки вам разумно их поселить на отдельные физические машины чтобы там performance был повыше вот соответственно выселить их на отдельные машины для дев среды вам достаточно и сунуть в одной машины просто импорт и разной дать ну и поделить ресурсу внутри вот то есть инсталляция она такая специфическая вот и вот мы приходим уже ко второй части запроса по выдаче прав на базу когда нам то есть вот смотрите македа кинули первый запрос он ушел к руководителю руководитель подтвердил что права можно давать а дальше идет второй запрос который от стороже идет к инстанции хасты то есть рассматривает все инсталляции инсталляции для соответствующей среды точно для для prado например для про то кружения и исходя из этого выдается уже право на к на подключение к конкретным хастам конкретно портом вот это когда человек со стороны просит а теперь давайте посмотрим что происходит как эта штука может работать когда в команду нужно взять нового человека кстати выходит новый чек команду и этому новому человеку нужно дать по-хорошему доступ можно там readonly по первости но хотя бы такой к ко всем сервисам ко всем 100 раджим соответственно этой команды и вот происходит вот так реальная команда неполная выборка вот надписи видно как нибудь я думаю не очень ну давайте вам по цветам расскажу о принципе предполагал поверьте там еще дальше веселее будет то есть зеленый это руководитель команды красная . это команда в данном случае там man this action праздник все название сервисов и что воображаемые желтенькие то сервиса как видите у ряда жёлтеньких сервисов есть с тораджи и эти 100 раджа то есть синее это 100 раджа серы это хасты фиолетовый это инсталляции стороны хасты вот the city в принципе простой задачи причем довольно маленький июня то есть у нас есть куча юнитов у которых на самом деле сервисов не 727 и для них вот эта картинка она совсем не приятно выглядит вот но когда у вас есть помнящая ткань вы можете у нее кинуть получить ответы выдать просто под списком давайте продолжим нашу у меня ткани расширять значит у нас сущность под названием сущность сущность то есть это бизнес сущность бизнес сущность логическая вашего бизнеса вот например когда мы говорим про авито наша сущность это объявление дома этого это пользователи это платежи но и так далее в принципе это вот на моих докладах про хранилище данных там вы знаете что у нас эти сущности сотни но на самом деле прямо все их алагира вать не обязательно откуда можно получить список сущностей вот это вот тут как раз аналитическое хранилище поможет тогда они как раз его могут выгрузить и они могут выгрузить информацию о том откуда они эту сущность берут это хотя бы стартовой точки где сущность хранится и эту информацию уже дальнейшем можно развивать в контексте вот того что я упоминал что для каждой сущности адекватно составить список хранилищ где это сущность хранит с указанием того что сущность хранит то есть что сторож же хранит сущность как кэш или сторож хранит сущность как golden source то есть является ее первоисточником и когда вы заполняете вот такую штуку у вас появляется возможность делать вот такие вещи то есть у вас есть некоторая сущность и вам нужно понять где она живет где она живет где она отражается в каких сервисов в каких 100 раджаф инсталлированных на каких аст и потому что когда дело касается например персональных данных там же там там довольно частые нужные локи стирать и для этого очень важно понять физические машины где эти логе могут остаться а вот у меня endpoint а тут вот они так вот таким цветом нарисованы потому что and point и мы пока не пролили от от остальные вещи не более менее пролиты вот иллюстрация простенького запроса для так сказать воображаемых воображаемой сущности тут я честно говоря количество 100 раджей pm порезал чтобы было чтобы ему влезла на экран вот то есть красная эту сущность данном случае это вот объявление по он был синяя это базы где эта сущность есть ну а дальше все как в предыдущих сериях то есть серые the host и а фиолетово-это инсталляции стороны на хаст и соответственно если вы хотите для соображений комплаенс прикрыть определенную сочную сущность безопа ну ограничение без безопасности вам нужно прикрывать условно говоря все серые точки все фиолетовые точки если вы имеете ввиду реке real-time доступ вот причем это я не не все стороны вывел именно чтобы она в экран влезла вот но эта информация эта информация о состоянии на сейчас это вот такая статическая информация а когда мы говорим про микро серво на архитектуру самое важное в не это то что на меняется она меняется и данные по ней ходят и очень важно нам иметь не просто иерархическую связь между сущностями но и одноуровневые связи то есть вот пример одноуровневые связи которую мы более-менее неплохо уже прокачали используем это связка сервисов то есть связка вида сервис вызывает сервис при чем тут смотрите вот у меня есть несколько источников перечислено то вот это вот парсинг клиентов бо subscription стиву ну вот этот сервис маршрут эти модные слова я человек из другого мира но наслушался от вадима всех историй тут есть информация о вызовах прямых то есть когда сервис дергает другому api ручку но тут же должна быть информация о связи вида что сервис один кидает в шину события a service 2 на это событие подписан то есть это такая асинхронная медленная связь просто направь проходит через шину она точно также важно с точки зрения движения данных ну вот частности и по этой штуки можно еще смотреть какой сервис если что когда мы его переделываем кому нужно ходить чтобы кому нужно ходить чтобы проверить что они могут смогут работать с новой версии сущность этого прям реальный с новые версии сервиса это вот реальный пример кстати с сервисом словарей и так значит если мы рассматриваем задачу поиска точек использования сущности то очевидный запрос который нам возникает вот у нас сущность и мы знаем что она хранится в определенных старриджа и прямо классический запрос это проверка периметра то есть вот 100 раджа они при по они принадлежат некоторым сервисом и куда эта сущность может у течь из вот этого периметра она может у течь через вызовы сервисов то есть ну как бы сервис обратился получил например пользователя у тебя сохранил и она может учим течь через шины потому что как вы знаете столь же они могут вас могут быть соединены друг с другом там ребятами там landis томи всякими поджиг и всякими такими штуками вот у меня вот они сейчас стороны нарисована таким цветом потому что мы вот landis ты еще не подгрузили сейчас процессе а вот вызову уже подгрузили и вот пример реального запроса то есть смотрите тут прямо если по слоям 100 смотреть вот объявление вот две базы где он хранится вот два сервиса которые владеют этими базами а то есть вот это вот при колоночки все что после это сервисы которые работают сервисами которые владеют эту сущность то есть это потенциальные точки утечки которые возможно стоит добавить вот помимо этого что тут уже кейс более без таких рыб про практических примеров and point и источники от вадим упоминал что для построения реестра in point of сервисов можно использовать вот эти ну документацию это можно сделать в атласе можно но обычно всей лестнице это делать с другой стороны эту информацию можно получить и за мониторинг а потому что если and point важный сами разработчики вы добавить мониторинг и проще взять и посмотреть в этого сервиса какие and point и мониторить но ведь неплохо и скрытным поднимание талица значит на но он не нужны но это один вариант соответственно из мониторинга можно получить метрики привязка у метрик к сторожем к сервисам кастом ка ну ка инстанциям ну как бы шар донбасс и контентом а дальше когда у вас возникает какой-то сбой то есть сок говоря контент point вам начинает 500 в принципе очевидно кейс это взять поэтому in the point у и кинуть запрос с целью отследить корни проблемы то есть вы кида вы in the point от него переходите к сервису переходите к сервисам которые этот сервис вызывает от сервиса идете к сторожем от 100 руб же идете констанца мудха стом и дальше просто на уровне то есть а от всех них то есть найдя вот этот граф вниз вы уже можете взять и получить по ним мониторинге и посмотреть вот именно для этого in the point а вот эта цепочка вниз среди мониторингов не было ли чего-нибудь что вызвало косяк потому что вы понимаете да когда мы говорим про ролями красивую архитектуру там сбой на and panty а он может быть вызван например что там на каком-то серые на каком-то сервере на котором развернуто 1 шард база там чихнула сеть мониторинга это видно но когда у нас структура сервисов и стороны и даже вот такая то проверяйте вспоминать все мониторинге это можно застрелить снави особенно когда они еще новые постами появляются вот помимо этого сценария есть еще очень интересный сценарий что сценарий связанные с тестированием потому что смотрите на самом деле чтобы адекватно тестировать с учетом всех нюансов то есть вадим описывал кейса как можно все микро сервиса тестировать как бы в изоляции но на самом деле чтобы адекватно тестировать вам нужно сервис брать и все те кто нужен ему для работы то есть вам нужно поднять вашим тестовом окружения сервисы сервиса который он вызывает и для них все базы то есть технически вам вашим вашей ткани нужно поднять как бы связанный под графа там вам не все связи нужно этом некоторые можно подчеркнуть но можно нужно поднять под граф и уже этот под граф уже изолирована нагрузочном тесте как полностью такую замкнутую систему это прямо цель вот я сразу скажу тут бы конечно было бы круто крисом я показал как вот у нас граф сущности авито ну да вот это вот на тему все микро сервисной архитектуры был бы круто мне сейчас показать что сейчас граца граф сервисов авито и вот тут изолированные под гриф и которые можно изолированной под графа микро сервисов которые можно поднять независимо тестировать и катись продакшен типа быстро красиво но на самом деле когда вот так мы попытались сделать практически оказалось что любой микро сервис у него вот этот под граф вызовов он на входит и выходит из монолита а монолит он ходит выходит практически везде и то есть как бы это просто иллюстрация того что если микро сервису и пилить не думаю о таких вещах в итоге будет микро сервисной архитектура которая все равно без монолит не работает изолированно и тестирование не позволяет но вот подобное графова и представление позволяет вам найти кандидаты под графа изолированы то есть узнать вот это подмножество сервис оно почти готово для того чтобы работать изолирована нужно вот только парочку вызовов отсечь ну или там вот там как-нибудь из-за изолироваться и уже будет хорошо то есть это позволяет приоритизировать разбил монолита именно чтобы увеличить уменьшить связанность архитектуры вот ну и собственно на последок про то как всю эту штуку можно поддерживать и наполнять потому что я не знаю может кажется что этот все не знаю как это воспринимается снаружи на сложно не сложно просто я про это долго думал с этим долго работал нет кажется довольно простым 10 довольно просто сделать все при соблюдении маленьких правил то есть правило первое что с эта штука нельзя работать заставляя людей заполнять все зависимости понимает структур нужно чтобы каждый источник заполнял только свой маленький кусок информации то есть из одного источника при чем тут возможно доливка и из ручных инструментов воды и запер ручек заполняли сервисы из другого стороже из 3 сервера из 4 in the point и и просто а из отдельных источников заполнялись ссылки что сервис принадлежит команде что человек стал тех рядом команды это первый момент туда просто должна лица от максимально быстро просто то есть в режиме стерли пересолили информации из них отстой внешних источников дальше вот эта информация аграфия она соблюдением историчности должна загружаться в базу тут очень важная историчность потому что понимаете реальной микро cyrus на верхотуру она постоянно дышит и меняется то есть сервис ип и возникают умирают появляются новые базы появляется connection и старый connection отменяют люди ходят между командами и вам довольно часто в контексте того же расследования там какая сущность как ходит вам бывает полезно понять что эта сущность вот она сейчас живет у этого сервиса а она была жила в том сервисе туда за и за ней все ходили туда вот то есть нужно что все связи были историчной как делать граф исторических связей этого там мои дак доклады пранкер modeling отдельная история но историчности статичностью большинству людей для того чтобы работать с данными им историчность не нужно им нужен актуальный срез на сейчас и на основании исторического графу должна уже строится витрина графа на состояние сейчас которую уже можно в принципе можно с ней конечно их подгрести работать но намного проще и поднять во всякие инструменты вот он у нас например него fuji базы поднята еще один сторож которому ее можно использовать для визуализации и туда же через api и ручки можно вот эти запросы которые сказал кидать то есть вот этот запрос простейший пример вот база кто владелец базы графов и запрос ответ вот база доступа к ней какие сервера какие хасты в продакшен окружений это просто запрос в api ответа , опять же вот сервис история просто цепочка метрика от этого сервиса вот да например его стороны и его хранилищ и так далее вот то есть и чем чаще чем больше такой штукой люди используют тем больше у них появляются стимула самим следить за ее актуальностью потому что каждый каждый слой этого ткани его может наполнять отдельная команда то есть ой точки наполняют например front in der и сервисы заполняют backend разработчики стороны заполняют тебе сервера заполняют devops сущности заполняют аналитики то есть все чем-то заняты и они должны чувствовать что их заполнение она живет и приносит пользу вот ну и напоследок пара вещей то есть вот практически то что я вам все показал она уже такое живое там не везде пока 100-процентная заполненность но вот по некоторым вещам у нас шанс продолжается активная работа мы его сейчас хотим у нас сейчас есть информация уже залитая а вызовы сервисом сервисов мы хотим добавить еще информацию о потоке данных через жену вот шотландии стрижек и прочие этом те самые ребята и прочее и последняя вещь которую мы пытаемся добавить час это вот самая горячая вещь это графы пользовательских траекторий то есть связи о и точками на основании того как там пользователя между ними ходят то есть у нас уже сейчас сделано это на ул его не 1 уровня клиентского логирования сейчас вот мы переходим к сшивки этой информацией и пробросов ткань чтобы можно было пользовательский опыт уже оттранслировать в ой точки оттуда в in the point и оттуда в сервисы просто понять но условно говоря чем у нас пользователи честь чаще всего пользуются и почему вот этот маленький сервис который вроде как не очень нагружена не очень важный на самом деле так эффекте с пользовательский опыт так сказать дисклеймер потому что этот сервис вызов его находится на важной пользовательской траектории вот собственно я практически все что хотел рассказал давайте к вопросам так поднимайте ручки потом вставайте и задавайте спасибо за доклад ближе микрофон приводит спасибо большой так вот два маленьких вопроса первый кто наполняет эту ткань то есть это люди которые меняют конфигурацию как вот процесс изменения происходят каждый сунула тут эта иллюстрация она хорошо подсказывает то есть каждый слой наполняется отдельно то есть каждый слой практически наполняется из некоторого а api некоторого источника ну то есть в эту услугу или сервис и вот просто там запрос к атласу охлаждающие сервисы и там есть интерфейс который позволяет то вводить потому что все в атласе не всех баз в 1-ой сильнее всех сотрудников почему-то в вал api dapi нет всех команд вот то есть результат автоматически от штата рука автоматика на участие до часть продлевается руками потому что вот это позволяет понять что у нас вот автоматика то там не все есть а второй вопрос не кажется ли вам что это очень сильно похоже на то что десятками лет делается в semantic web собственно link td это где есть связи собственно семантика и описывается как что с чем связано кажется что это ну смотрите вообще идея графа знания она на самом деле много используется привет поисковым движками прочим ну да это все это все примерно про это вопрос как именно жить и этим как это быстро поднять у себя в конторе то есть это все делается руками без покупки всяких с авто и прочее довольно быстро спасибо за доклад значит у меня вопрос такой вот мы учитываем информацию пройти мы я здесь сейчас ну а вижу да значит мы учитываем информацию про этим и откуда они взялись куда они залились где за кашира волеси тогда в рамках этой концепции как еда рассматривается история о том что этим может быть за кашира ванне весь и какая часть его за кашира но просто предупреждаю я понимаю да что каждый элемент каждая их база она кэширует только фрагмент нужных им полей мы сейчас вот вот этот нюанс не капает потому что вам нафиг не надо ну потому что мы как бы мы слона начинаем есть да чё то тянулись тебя съели если это будет актуально это тоже добавим то есть тут суть этой ткани заключается в том что оно может разрастаться если нужно то есть возможно мы обогатим каналы информация перетекаем их атрибутов просто сейчас это пока излишняя сложность потребностей особо не было спасибо спасибо за доклад скажите а после появления у вас такой визуализации насколько ваша монолиту уменьшился вообще-то помогает уменьшать монолит которые стоится так ну во первых мне монолиту вот это вязать скажем так нет она не от этого не уменьшился вообще просто с ним стало проще жить то есть смотрите кейс про уменьшение монолита про обоснование монолита это вообще находится в другой зоне этим тоже кстати я занимаюсь во многих во многом в рамках доменного моделирования но это происходит не на техническом уровне это происходит на уровне продажи бизнес-идеи бизнесу продажи числовых метрик что распил монолита позволит нам вот здесь заработать больше а здесь терять меньше это вообще в другой зоне но визуализация позволяет а на презентациях пусть бы позволяет испугать какой страшной картинкой бонус еще есть вопрос можно и так так ага да добрый день спасибо за доклад собственно есть такие системы я уже говорил сентябре есть гендерное решение от объёма . например и от beyonce тоже есть пропиши вас перебью 7 тебе мы знаем он а вот информация серверах хостах от железах вот оттуда вот смотрите там есть спирали discovering знаете нам не расслышал есть такая штука как спиральный discovering это зонт автоматически начинают искать папу лай и адресов выделять сервис и находить на них марты и потом ну соответственно discovered вплоть до and user experience если есть интеграции там систему мониторинга и так далее вот ну почему вы решили использовать что-то новое а не используйте что-то старые что уже десятками лет работает и во многих но многие компании используют гендерные решения которые хорошо себя показали то есть есть скажем так сентябре который также отражает графы там все это можно найти можно найти связь от бизнес сервиса его owner а это вообще эта тема называется эти сервис-менеджмент от оттуда пошло того как бы войти у есть отдельная тема как управлять тишка это типа itsm и войти сэм есть отдельная ветка 7 db есть то же самое это ну как бы почему вот это не было взято хорошая старая да что-то новое и эта история ответ на эту историю стоит на это вопрос состоит из двух частей потому что во-первых мы не очень любим гендерное решение мы не любим платить вендорам кроме тех случаев когда не делать что-то прорывное но в данном случае у нас был ряд заходов когда к нам приходили крупные векторы тут например вот последний к нам приходил это оп оп detect динамик от не мой ноут у меня просто на ноуте так классно наклейка от них есть они тоже это делают из свежей компания вот про это же то есть который это делает вот лучше чем вот те ребят что вы описали ну и пройти как бы они теоретически это делают но дальше вон за им практически вопрос вот типа вот наши жизни вот у нас монолит вот у нас тут по хп тут у нас год тут у нас губерний тут все поднимается вот тут у нас свой суп описанный атлас тут у нас базы который развертывается ну типа давайте нет ну то есть короче и понятно чтоб через через пару лет внедрение кучу денег конечно да ну то есть речь идет о том что это очень простая вещь и вам для этого метр на решение и нужно они дали кому-то может нравиться работать вендорами ну смотрите а вот как вы собираете встретить юзер экспириенс они не смотрите мы в бою мы сейчас на уровне логированием и у нас вот во первых все пользовательские действия не лаги руется через шину в нашу аналитику и у нас каждое событие которое пользователь делает мы сейчас его обогатили и там появилась ссылка назад то есть ссылка то есть вот человека вот он делает действие точно же открывает ой там и мы знаем что такое забыть у нее было предыдущие и каждое событие она как бы лакируется ссылкой и мы вот эти события ссылками мы специальным накопителям на потоке мы выстраиваем в траектории это называется к разблокирование пользуетесь которых трое то что у меня на последнем слайде и там мы как раз можем отличить траектории которые частые насыщены а то те которые случайно вот таким образом это дело это уже реализовали или блокирование реализовано сшив к в процессе аналитика частично используется то есть логирование да все уже ссылками назад лакируется причем вступаете в чем проблема тут и ос android the same десктопном avi там все везде есть небольшие специфики потому что это чисто на фронте на отдел чтоб по-честному но это ездил спасибо вот я кстати не принуждая использовать винтер на и решение груш то есть технология 7 т.п. есть конечно магически discovering ну как бы технология старая неважно кто и внедряет vanderley open source конкретный вендор без разницы ну да такое тоже то же самое округа это тоже атлас вы посмотрите все же знают а то снова посмотрели как выглядит атлас у нас после того как мы его доработали то есть возможности подкрутить под себя она хороша всегда идет спасибо в центр сюда теперь вы т.д. спасибо за доклад на наболевшем у вопрос следующий на самом деле понять более факту по наболевшем понятно как идет связь сервисов между сервисами когда у нас идет синхронный запрос по синхронным когда мы пациенту это запрос шину вы пытаетесь тоже идти от сервиса к сервису они по сообщениям которые в его посылаются потому что я наряду со списком да по подпискам все-таки по типу сообщений да у нас тип сообщений четко вот он в кайф терминология кафка он считается как бы топиком и там четко происходит вот сервис вам публикует определенные топики и мы знаем что кто на какой топик подписан все общаешься позволяет если чего кому не надо закрытию возможности выйти на определенный топик чтобы там чем чё попало не слушали спасибо чуть не раскрыто был здрасте геннадий московская биржа спасибо за интересный возврат хайповые слова надеюсь пригодится тема точно актуальное и спасибо за доклад есть пара вопросов 1 вы упоминали в дата фабрики вот в этом своем pierce the фабрики на самом нижнем уровне entity вы их как-то автоматически отслеживается я не раз слышал ни одна предыдущих раз попридержать конечно вы на пирсе стэн фабрики на самом нижнем уровне на 2 entity на ткани на ткани на этого сказать не шутка меня же рассказывали ребята из беды что у них провал проекта дайте побрит был что они перевели как фабрика данных построили фабрику а вообще-то в оригинале имелось дудка не вот и поэтому сегодня получилось поэтому важно это не фабрика у меня департамент как раз фабрика дана понимаю так вот сущности вы как-то отдельно отслеживаете автоматически наполняете или это ручная работа срочности список сущности да и их связи если они есть и либо связи и не следить а ну это вот мои доклады про анкор modeling по х аналитическое хранилище то есть у нас фактически хранилище она автоматически генерируется по описанию сущности связи атрибуты и вот эта информация прямо напрямую льется там хитрец источниками потому что истоки что вот эта сущность она поступает от туда еще оттуда оттуда там часто еще из-за промежуточных буферов вот этого путь как каждая сущность прилетает не всегда допускает автоматический парсинг то есть сущности автоматически пути заливки сущностей полу ручном режиме а вы справляетесь со сложностью когда сущности из одной в другую сильно пересекаю трансформируются там в одной грубо говоря условно таблички лежит целая их пачка разделенные по каким-то признакам то есть со сложными читайте про анкор modeling там такого не бывает то есть мы у нас сущность это бизнес сущность а не то что система там какую-то сущность назвала x там изгибе юзер как кончита к у нас есть пользователь пойти пользователи этот чек из мяса то есть вот он тоже разная система могут его по-разному представляет но это одна сущность но это это это вот про другое завод из другой немножко областью ну окей хорошо спасибо большое спасибо большое николай вот это вам ура и скажите кому из-за давших вопросы книга книга вот кто просил baby рассказывал там было интересно вот он ушел не ушел он та